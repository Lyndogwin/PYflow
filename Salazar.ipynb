{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "\"\"\"\n",
    "Numpy: matrix manipulation and math\n",
    "Pandas: csv parsing and various data structure tasks\n",
    "Mathpltlib.pyplot: data visualization\n",
    "set_trace: debug breaks\n",
    "keras: a machine learning library that is intuitive to read\n",
    "tensorflow: backend for keras... also the most widely used machine learning library\n",
    "re: regular expressions\n",
    "\"\"\"\n",
    "from copy import deepcopy as copy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sci\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "\n",
    "tf.config.optimizer.set_jit(True) # optimizes cpu usage\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "concat_files\n",
    "----------\n",
    "\n",
    "Concatenate text files in a directory as a string\n",
    "\n",
    "dependent on 'os' Python module\n",
    "\n",
    "parameters\n",
    "----------\n",
    "directory: string; path of the target directory\n",
    "\n",
    "f_type:    tuple of strings; target file extentsions Ex: ('.py', '.cpp')\n",
    "\n",
    "return\n",
    "---------\n",
    "content:   concatenated string\n",
    "\n",
    "\"\"\"\n",
    "def concat_files(directory,f_type):\n",
    "    import os\n",
    "    # List all file in the dataset directory\n",
    "    # ------------------\n",
    "    all_file = []\n",
    "    content = \"\"\n",
    "\n",
    "    # walk through every directory and open every f_type file\n",
    "    # concatenate into var string \"content\"\n",
    "    for root, dirs, files in os.walk(directory): \n",
    "        for name in files:\n",
    "            if name.endswith(f_type): # we only care about .py\n",
    "                all_file.append(name)\n",
    "                with open(os.path.join(root,name), \"r\",encoding=\"utf8\") as f:\n",
    "                    content += f.read() + \"\\n\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = concat_files(\"dataset\",('.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_ascii = \"[^\\x00-\\x7F]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "encode_string\n",
    "-----------\n",
    "Generate a dictionary representation of the characters found \n",
    "in a string keyed with integer representations\n",
    "\n",
    "Returns two dictionaries and an array. The two dictionaries are \n",
    "necessary to convert the string to integer representation\n",
    "and back again. The array is the string encoded as integer values.\n",
    "\n",
    "parameters\n",
    "----------\n",
    "content:      string; to be processed\n",
    "\n",
    "return\n",
    "----------\n",
    "vocab_to_int: dict; character to integer representation of unique characters in the string\n",
    "\n",
    "int_to_vocab: dict; integer to string representation\n",
    "\n",
    "encoded:      array; string encoded as integer values\n",
    "\"\"\"\n",
    "\n",
    "def encode_string(content):   \n",
    "    # Convert the string \"content\" into a list of intergers\n",
    "\n",
    "    ### creates a set of the individual characters\n",
    "    vocab = set(content)\n",
    "    ### attempt to clean out non-ascii characters\n",
    "    vocab_c = copy(vocab)\n",
    "    for i, char in enumerate(vocab_c):\n",
    "        if re.search(r_all_ascii,char):\n",
    "            vocab.remove(char)\n",
    "    print(vocab)\n",
    "    print(len(vocab))\n",
    "    ### use the set to sequentially generate a dictionary\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)} \n",
    "    # print(vocab_to_int)\n",
    "    ### make keys the numerical values\n",
    "    int_to_vocab = dict(enumerate(vocab)) \n",
    "    \n",
    "    ### encode the \"content\" string using dict\n",
    "    ### encoded = np.array([vocab_to_int[c] for c in content], dtype=np.int32)\n",
    "    \n",
    "    # *** Uncomment the below lines if you haven't saved the encoded array\n",
    "    # Then rerun cell\n",
    "#   -------------------------------------------------\n",
    "#     encoded = np.array([],dtype=np.int16)\n",
    "#     for c in content:\n",
    "#         if c in vocab_to_int:\n",
    "#             encoded = np.append(encoded,vocab_to_int[c])\n",
    "#   -------------------------------------------------\n",
    "    infile = \"./encoded.txt\"\n",
    "    encoded = np.loadtxt(infile, dtype=int) # comment out if above lines are uncommented\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T', 'e', 'l', 'i', 'x', '-', 'y', '0', '*', '9', '[', 'j', 'D', '7', 's', '<', '+', '!', '^', 'r', 'd', '3', 'M', 'L', 'C', '.', '/', 'O', '\\\\', 'E', 'm', '@', \"'\", 'k', 'o', '?', '(', ',', '_', 'R', 'X', '8', 'N', 'S', 'f', '$', 'Z', '1', 'K', '5', 'U', 'v', '\"', '{', 'G', '|', '&', 'F', 'I', '=', 't', 'g', '%', '~', 'a', 'p', '}', 'Q', 'V', '>', 'c', ')', 'J', 'A', '6', '#', '`', 'b', 'B', 'P', ':', '\\n', 'H', ']', ';', ' ', 'u', 'w', '4', 'W', 'Y', 'q', 'z', 'n', '2', 'h', '\\t'}\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab, encoded = encode_string(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\rightarrow$ Save encoded array to avoid heavy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tempfile import TemporaryFile as TF\n",
    "outfile = \"./encoded.txt\"\n",
    "\n",
    "np.savetxt(outfile,encoded, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'T', 1: 'e', 2: 'l', 3: 'i', 4: 'x', 5: '-', 6: 'y', 7: '0', 8: '*', 9: '9', 10: '[', 11: 'j', 12: 'D', 13: '7', 14: 's', 15: '<', 16: '+', 17: '!', 18: '^', 19: 'r', 20: 'd', 21: '3', 22: 'M', 23: 'L', 24: 'C', 25: '.', 26: '/', 27: 'O', 28: '\\\\', 29: 'E', 30: 'm', 31: '@', 32: \"'\", 33: 'k', 34: 'o', 35: '?', 36: '(', 37: ',', 38: '_', 39: 'R', 40: 'X', 41: '8', 42: 'N', 43: 'S', 44: 'f', 45: '$', 46: 'Z', 47: '1', 48: 'K', 49: '5', 50: 'U', 51: 'v', 52: '\"', 53: '{', 54: 'G', 55: '|', 56: '&', 57: 'F', 58: 'I', 59: '=', 60: 't', 61: 'g', 62: '%', 63: '~', 64: 'a', 65: 'p', 66: '}', 67: 'Q', 68: 'V', 69: '>', 70: 'c', 71: ')', 72: 'J', 73: 'A', 74: '6', 75: '#', 76: '`', 77: 'b', 78: 'B', 79: 'P', 80: ':', 81: '\\n', 82: 'H', 83: ']', 84: ';', 85: ' ', 86: 'u', 87: 'w', 88: '4', 89: 'W', 90: 'Y', 91: 'q', 92: 'z', 93: 'n', 94: '2', 95: 'h', 96: '\\t'}\n",
      "[38 56 83 ... 62 60 60]\n"
     ]
    }
   ],
   "source": [
    "#print(content)\n",
    "print(int_to_vocab)\n",
    "# this is all of the files concatenated. with each character encoded using the int_to_vocab\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape data into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  1112800\n",
      "Total unique characters:  97\n",
      "\" ing bolzano\n",
      "\n",
      "    start = a\n",
      "    end = b\n",
      "    if function(a) == 0:  # one of the a or b is a root for t \"\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(encoded)\n",
    "n_vocab = len(vocab_to_int)\n",
    "seq_len = 100 # change from 50\n",
    "datax = []\n",
    "datay = []\n",
    "\n",
    "# Loop through the encoded data and store \n",
    "# sequences in datax and datay\n",
    "for i in range(0, n_chars - seq_len, 1):\n",
    "    seq_in = encoded[i:i + seq_len] \n",
    "    seq_out = encoded[i + seq_len]\n",
    "    datax.append(seq_in)\n",
    "    datay.append(seq_out)\n",
    "n_patterns = len(datax)\n",
    "print(\"Total patterns: \", n_patterns)\n",
    "print(\"Total unique characters: \", n_vocab)\n",
    "print (\"\\\"\", ''.join([int_to_vocab[value] for value in datax[100]]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# reshape datax -- > [n_patterns, time steps, features]\n",
    "X = np.reshape(datax, (n_patterns,seq_len,1))\n",
    "X = X / float(n_vocab)\n",
    "Y = np_utils.to_categorical(datay)\n",
    "#Y = np.asarray(datay) # for sparse categorical cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "\n",
    "# optimizer = RMSprop(learning_rate=0.05)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizer,\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = \"best-weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',verbose=1, save_best_only=True, mode='min')\n",
    "callback_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.4394 - accuracy: 0.2532\n",
      "Epoch 00001: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 3.4389 - accuracy: 0.2533\n",
      "Epoch 2/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.3264 - accuracy: 0.2556\n",
      "Epoch 00002: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 3.3268 - accuracy: 0.2554\n",
      "Epoch 3/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.2724 - accuracy: 0.2510\n",
      "Epoch 00003: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 3.2726 - accuracy: 0.2509\n",
      "Epoch 4/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.2410 - accuracy: 0.2492\n",
      "Epoch 00004: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 3.2410 - accuracy: 0.2490\n",
      "Epoch 5/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.2132 - accuracy: 0.2507\n",
      "Epoch 00005: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 3.2131 - accuracy: 0.2507\n",
      "Epoch 6/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.1874 - accuracy: 0.2535\n",
      "Epoch 00006: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 3.1878 - accuracy: 0.2535\n",
      "Epoch 7/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.1583 - accuracy: 0.2555\n",
      "Epoch 00007: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 3.1580 - accuracy: 0.2556\n",
      "Epoch 8/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.1424 - accuracy: 0.2578\n",
      "Epoch 00008: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 3.1433 - accuracy: 0.2576\n",
      "Epoch 9/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.1147 - accuracy: 0.2585\n",
      "Epoch 00009: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 3.1153 - accuracy: 0.2583\n",
      "Epoch 10/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.0707 - accuracy: 0.2613\n",
      "Epoch 00010: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 3.0704 - accuracy: 0.2614\n",
      "Epoch 11/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 3.0322 - accuracy: 0.2662\n",
      "Epoch 00011: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 3.0328 - accuracy: 0.2660\n",
      "Epoch 12/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.9907 - accuracy: 0.2703\n",
      "Epoch 00012: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.9906 - accuracy: 0.2704\n",
      "Epoch 13/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.9360 - accuracy: 0.2764\n",
      "Epoch 00013: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.9355 - accuracy: 0.2765\n",
      "Epoch 14/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.8907 - accuracy: 0.2852\n",
      "Epoch 00014: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.8907 - accuracy: 0.2852\n",
      "Epoch 15/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.8421 - accuracy: 0.2945\n",
      "Epoch 00015: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 2.8419 - accuracy: 0.2945\n",
      "Epoch 16/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.8015 - accuracy: 0.2997\n",
      "Epoch 00016: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.8009 - accuracy: 0.2999\n",
      "Epoch 17/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.7593 - accuracy: 0.3081\n",
      "Epoch 00017: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.7596 - accuracy: 0.3080\n",
      "Epoch 18/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.7152 - accuracy: 0.3154\n",
      "Epoch 00018: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.7151 - accuracy: 0.3155\n",
      "Epoch 19/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.6708 - accuracy: 0.3226\n",
      "Epoch 00019: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.6720 - accuracy: 0.3224\n",
      "Epoch 20/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.6322 - accuracy: 0.3317\n",
      "Epoch 00020: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.6322 - accuracy: 0.3316\n",
      "Epoch 21/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.5943 - accuracy: 0.3390\n",
      "Epoch 00021: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.5944 - accuracy: 0.3391\n",
      "Epoch 22/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.5538 - accuracy: 0.3452\n",
      "Epoch 00022: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 2.5536 - accuracy: 0.3452\n",
      "Epoch 23/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.5332 - accuracy: 0.3501\n",
      "Epoch 00023: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.5332 - accuracy: 0.3503\n",
      "Epoch 24/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.4806 - accuracy: 0.3625\n",
      "Epoch 00024: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.4802 - accuracy: 0.3626\n",
      "Epoch 25/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.5364 - accuracy: 0.3536\n",
      "Epoch 00025: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.5372 - accuracy: 0.3536\n",
      "Epoch 26/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.4591 - accuracy: 0.3657\n",
      "Epoch 00026: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.4592 - accuracy: 0.3656\n",
      "Epoch 27/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.4056 - accuracy: 0.3776\n",
      "Epoch 00027: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.4056 - accuracy: 0.3776\n",
      "Epoch 28/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.3708 - accuracy: 0.3855\n",
      "Epoch 00028: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.3711 - accuracy: 0.3852\n",
      "Epoch 29/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.3334 - accuracy: 0.3897\n",
      "Epoch 00029: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 2.3331 - accuracy: 0.3899\n",
      "Epoch 30/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.3972\n",
      "Epoch 00030: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.3040 - accuracy: 0.3969\n",
      "Epoch 31/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.2798 - accuracy: 0.4022\n",
      "Epoch 00031: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2791 - accuracy: 0.4023\n",
      "Epoch 32/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.2484 - accuracy: 0.4084\n",
      "Epoch 00032: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2488 - accuracy: 0.4084\n",
      "Epoch 33/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.2248 - accuracy: 0.4153\n",
      "Epoch 00033: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.2243 - accuracy: 0.4154\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.2003 - accuracy: 0.4205\n",
      "Epoch 00034: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2001 - accuracy: 0.4207\n",
      "Epoch 35/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.1711 - accuracy: 0.4267\n",
      "Epoch 00035: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.1711 - accuracy: 0.4269\n",
      "Epoch 36/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.1521 - accuracy: 0.4313\n",
      "Epoch 00036: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 2.1519 - accuracy: 0.4314\n",
      "Epoch 37/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.1358 - accuracy: 0.4337\n",
      "Epoch 00037: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 2.1364 - accuracy: 0.4338\n",
      "Epoch 38/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.1144 - accuracy: 0.4400\n",
      "Epoch 00038: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.1149 - accuracy: 0.4398\n",
      "Epoch 39/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.0795 - accuracy: 0.4450\n",
      "Epoch 00039: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.0803 - accuracy: 0.4448\n",
      "Epoch 40/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.1533 - accuracy: 0.4330\n",
      "Epoch 00040: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.1529 - accuracy: 0.4329\n",
      "Epoch 41/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.0686 - accuracy: 0.4500\n",
      "Epoch 00041: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.0684 - accuracy: 0.4499\n",
      "Epoch 42/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.0333 - accuracy: 0.4590\n",
      "Epoch 00042: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.0330 - accuracy: 0.4592\n",
      "Epoch 43/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.1537 - accuracy: 0.4310\n",
      "Epoch 00043: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 2.1538 - accuracy: 0.4310\n",
      "Epoch 44/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.0098 - accuracy: 0.4623\n",
      "Epoch 00044: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.0104 - accuracy: 0.4621\n",
      "Epoch 45/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.9646 - accuracy: 0.4706\n",
      "Epoch 00045: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.9657 - accuracy: 0.4704\n",
      "Epoch 46/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.9602 - accuracy: 0.4733\n",
      "Epoch 00046: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.9596 - accuracy: 0.4733\n",
      "Epoch 47/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.9396 - accuracy: 0.4785\n",
      "Epoch 00047: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.9393 - accuracy: 0.4783\n",
      "Epoch 48/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.9209 - accuracy: 0.4808\n",
      "Epoch 00048: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.9208 - accuracy: 0.4807\n",
      "Epoch 49/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.9008 - accuracy: 0.4875\n",
      "Epoch 00049: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.9020 - accuracy: 0.4871\n",
      "Epoch 50/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.9005 - accuracy: 0.4867\n",
      "Epoch 00050: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 1.9008 - accuracy: 0.4866\n",
      "Epoch 51/100\n",
      "59400/60000 [============================>.] - ETA: 0s - loss: 1.8948 - accuracy: 0.4875\n",
      "Epoch 00051: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.8955 - accuracy: 0.4874\n",
      "Epoch 52/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8627 - accuracy: 0.4959\n",
      "Epoch 00052: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.8630 - accuracy: 0.4960\n",
      "Epoch 53/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8469 - accuracy: 0.4985\n",
      "Epoch 00053: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.8476 - accuracy: 0.4983\n",
      "Epoch 54/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8664 - accuracy: 0.4958\n",
      "Epoch 00054: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.8654 - accuracy: 0.4961\n",
      "Epoch 55/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8266 - accuracy: 0.5044\n",
      "Epoch 00055: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.8264 - accuracy: 0.5046\n",
      "Epoch 56/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8093 - accuracy: 0.5057\n",
      "Epoch 00056: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.8091 - accuracy: 0.5057\n",
      "Epoch 57/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8210 - accuracy: 0.5047\n",
      "Epoch 00057: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 1.8216 - accuracy: 0.5045\n",
      "Epoch 58/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7872 - accuracy: 0.5117\n",
      "Epoch 00058: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.7875 - accuracy: 0.5117\n",
      "Epoch 59/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7607 - accuracy: 0.5214\n",
      "Epoch 00059: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.7611 - accuracy: 0.5212\n",
      "Epoch 60/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7594 - accuracy: 0.5214\n",
      "Epoch 00060: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.7599 - accuracy: 0.5214\n",
      "Epoch 61/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7489 - accuracy: 0.5209\n",
      "Epoch 00061: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.7495 - accuracy: 0.5206\n",
      "Epoch 62/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7245 - accuracy: 0.5271\n",
      "Epoch 00062: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.7244 - accuracy: 0.5270\n",
      "Epoch 63/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7128 - accuracy: 0.5286\n",
      "Epoch 00063: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.7142 - accuracy: 0.5283\n",
      "Epoch 64/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7062 - accuracy: 0.5320\n",
      "Epoch 00064: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 1.7070 - accuracy: 0.5317\n",
      "Epoch 65/100\n",
      "59400/60000 [============================>.] - ETA: 0s - loss: 1.6991 - accuracy: 0.5326\n",
      "Epoch 00065: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.6988 - accuracy: 0.5327\n",
      "Epoch 66/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7133 - accuracy: 0.5314\n",
      "Epoch 00066: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.7134 - accuracy: 0.5314\n",
      "Epoch 67/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6667 - accuracy: 0.5405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00067: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.6670 - accuracy: 0.5405\n",
      "Epoch 68/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7025 - accuracy: 0.5329\n",
      "Epoch 00068: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.7033 - accuracy: 0.5326\n",
      "Epoch 69/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6759 - accuracy: 0.5378\n",
      "Epoch 00069: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.6766 - accuracy: 0.5376\n",
      "Epoch 70/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6483 - accuracy: 0.5466\n",
      "Epoch 00070: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.6481 - accuracy: 0.5468\n",
      "Epoch 71/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7234 - accuracy: 0.5301\n",
      "Epoch 00071: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 1.7228 - accuracy: 0.5302\n",
      "Epoch 72/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6370 - accuracy: 0.5485\n",
      "Epoch 00072: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.6377 - accuracy: 0.5482\n",
      "Epoch 73/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6160 - accuracy: 0.5527\n",
      "Epoch 00073: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.6163 - accuracy: 0.5527\n",
      "Epoch 74/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6456 - accuracy: 0.5470\n",
      "Epoch 00074: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.6461 - accuracy: 0.5469\n",
      "Epoch 75/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6317 - accuracy: 0.5490\n",
      "Epoch 00075: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.6327 - accuracy: 0.5488\n",
      "Epoch 76/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5954 - accuracy: 0.5564\n",
      "Epoch 00076: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.5956 - accuracy: 0.5562\n",
      "Epoch 77/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5799 - accuracy: 0.5611\n",
      "Epoch 00077: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.5794 - accuracy: 0.5613\n",
      "Epoch 78/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5660 - accuracy: 0.5660\n",
      "Epoch 00078: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 1.5656 - accuracy: 0.5663\n",
      "Epoch 79/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5618 - accuracy: 0.5654\n",
      "Epoch 00079: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.5620 - accuracy: 0.5653\n",
      "Epoch 80/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5669 - accuracy: 0.5643\n",
      "Epoch 00080: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.5671 - accuracy: 0.5643\n",
      "Epoch 81/100\n",
      "59400/60000 [============================>.] - ETA: 0s - loss: 1.6704 - accuracy: 0.5419\n",
      "Epoch 00081: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.6708 - accuracy: 0.5418\n",
      "Epoch 82/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5727 - accuracy: 0.5644\n",
      "Epoch 00082: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.5738 - accuracy: 0.5642\n",
      "Epoch 83/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5564 - accuracy: 0.5679\n",
      "Epoch 00083: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.5572 - accuracy: 0.5677\n",
      "Epoch 84/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5538 - accuracy: 0.5703\n",
      "Epoch 00084: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.5538 - accuracy: 0.5702\n",
      "Epoch 85/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5109 - accuracy: 0.5773\n",
      "Epoch 00085: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 1.5114 - accuracy: 0.5771\n",
      "Epoch 86/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5006 - accuracy: 0.5814\n",
      "Epoch 00086: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.5009 - accuracy: 0.5813\n",
      "Epoch 87/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.0908 - accuracy: 0.4590\n",
      "Epoch 00087: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.0898 - accuracy: 0.4591\n",
      "Epoch 88/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6627 - accuracy: 0.5446\n",
      "Epoch 00088: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.6624 - accuracy: 0.5448\n",
      "Epoch 89/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5544 - accuracy: 0.5679\n",
      "Epoch 00089: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.5543 - accuracy: 0.5680\n",
      "Epoch 90/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5070 - accuracy: 0.5785\n",
      "Epoch 00090: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.5069 - accuracy: 0.5783\n",
      "Epoch 91/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4973 - accuracy: 0.5851\n",
      "Epoch 00091: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.4975 - accuracy: 0.5851\n",
      "Epoch 92/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4860 - accuracy: 0.5852\n",
      "Epoch 00092: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 1.4859 - accuracy: 0.5851\n",
      "Epoch 93/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5766 - accuracy: 0.5646\n",
      "Epoch 00093: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.5766 - accuracy: 0.5644\n",
      "Epoch 94/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5185 - accuracy: 0.5761\n",
      "Epoch 00094: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.5194 - accuracy: 0.5757\n",
      "Epoch 95/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4779 - accuracy: 0.5878\n",
      "Epoch 00095: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.4786 - accuracy: 0.5878\n",
      "Epoch 96/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4504 - accuracy: 0.5938\n",
      "Epoch 00096: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.4515 - accuracy: 0.5937\n",
      "Epoch 97/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5274 - accuracy: 0.5752\n",
      "Epoch 00097: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.5278 - accuracy: 0.5750\n",
      "Epoch 98/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4397 - accuracy: 0.5968\n",
      "Epoch 00098: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.4394 - accuracy: 0.5968\n",
      "Epoch 99/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.7548 - accuracy: 0.4756\n",
      "Epoch 00099: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 2.7716 - accuracy: 0.4737\n",
      "Epoch 100/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 5.1222 - accuracy: 0.1281\n",
      "Epoch 00100: loss did not improve from 0.28177\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 5.1174 - accuracy: 0.1283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24294df3b88>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(inp, targets, steps_per_epoch=10, epochs=10)\n",
    "#model.get_weights().shape\n",
    "\n",
    "model.fit(X[:60000] , Y[:60000], epochs=100, batch_size=300, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 7s 110us/sample - loss: 4.6742 - accuracy: 0.2396\n",
      "[4.67418819476366, 0.2396]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# model.save('.\\model')\n",
    "score = model.evaluate(X[60000:120000], Y[60000:120000])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best weights recorded from training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename should reflect the name of the best weights available \n",
    "# in th local directory after training\n",
    "filename = \"best-weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" xtreme points belong to the upper hull\n",
      "    # all points to the right (below) the line joining the ex \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_10_input to have shape (30, 1) but got array with shape (100, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-20956aadac58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2410\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    580\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    583\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_10_input to have shape (30, 1) but got array with shape (100, 1)"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(datax)-1)\n",
    "pattern = []\n",
    "pattern = datax[start]\n",
    "print(\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_vocab[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_vocab[index]\n",
    "    seq_in = [int_to_vocab[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern = np.append(pattern, [index], axis=0) # TODO make so length of pattern is 101, as it should be\n",
    "    length = pattern.shape[0] \n",
    "    pattern = pattern[1:length] # issue with the length. no matter what, length of pattern become 99\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [LSTM: A search space odyssey](https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
