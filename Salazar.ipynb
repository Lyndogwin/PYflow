{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salazar\n",
    "*A Python-writting wizard*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Coding, while rewarding and essential to modernity, can be monotonous at times. However, with modern Neural Networks, the possibilities exist for a world where we can start the initial stages of a programming process of a software solution and then allow a machine to finish the work for us. This concept entails a future where programmers could eliminate the overhead of debugging and testing and allow them more time to focus on the planning stage of project management. \n",
    "\n",
    "Generative text has been around for some time now but generative coding is still a relatively new implementation of it’s paradigms. The difficulties with generative code could be akin to training a model to write stories with a dataset containing mostly Sci-Fi and then expecting that model to write a Nicholas Sparks’ novel. I.e. The problem here stems from the multitude of libraries as packages used to build upon programming languages to make them useful for specific tasks. Just because a model can produce C code doesn’t mean it can build an operating system. So what do we do if we want a swiss army knife for coding nearly every variation of code in a specific language? Well maybe we should use a method that employs a significant amount of data (string of code) in tandem with a method of effectively seeding the model. There are, thankfully, enough similarities between any two programs written in Python that some rule should be learnable by a network; combining that with the right amount of “starter code” should prove effective enough to get relatively useful outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "\"\"\"\n",
    "Numpy:             matrix manipulation and math\n",
    "Pandas:            csv parsing and various data structure tasks\n",
    "Mathpltlib.pyplot: data visualization\n",
    "set_trace:         debug breaks\n",
    "keras:             a machine learning library that is intuitive to read\n",
    "tensorflow:        backend for keras; also the most widely used machine learning library\n",
    "re:                regular expressions\n",
    "\"\"\"\n",
    "from copy import deepcopy as copy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sci\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "\n",
    "tf.config.optimizer.set_jit(True) # optimizes cpu usage\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate files as a singular string\n",
    "The bellow function uses a Python function knowns as ```walk``` to \"walk\" through a directory and read the files within that directory, storing them to the string ```content```\n",
    "\n",
    "Currently using [this repository](https://github.com/TheAlgorithms/Python) as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "concat_files\n",
    "----------\n",
    "\n",
    "Concatenate text files in a directory as a string\n",
    "\n",
    "dependent on 'os' Python module\n",
    "\n",
    "parameters\n",
    "----------\n",
    "directory: string; path of the target directory\n",
    "\n",
    "f_type:    tuple of strings; target file extentsions Ex: ('.py', '.cpp')\n",
    "\n",
    "return\n",
    "---------\n",
    "content:   concatenated string\n",
    "\n",
    "\"\"\"\n",
    "def concat_files(directory,f_type):\n",
    "    import os\n",
    "    # List all file in the dataset directory\n",
    "    # ------------------\n",
    "    all_file = []\n",
    "    content = \"\"\n",
    "\n",
    "    # walk through every directory and open every f_type file\n",
    "    # concatenate into var string \"content\"\n",
    "    for root, dirs, files in os.walk(directory): \n",
    "        for name in files:\n",
    "            if name.endswith(f_type): # we only care about .py\n",
    "                all_file.append(name)\n",
    "                with open(os.path.join(root,name), \"r\",encoding=\"utf8\") as f:\n",
    "                    content += f.read() + \"\\n\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = concat_files(\"dataset\",('.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a regular expression representation of ASCII characters\n",
    "This will useful in distiguishing the characters that are important in terms of writting Python code vs. characters that are exclusive to documentation such as emoji's and other non-latin characters. This will help to slim our data shape, increasing training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_ascii = \"[^\\x00-\\x7F]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellow are two fuctions with the same name\n",
    "[**The first**](#First-preprocessing-option) is for if you are *introducing new data to the dataset*, i.e. pulling new repositories into the \"dataset\" directory. This will perform the *encoding step*, converting every character in the concatenated string into an integer to be stored in a one dimensional array. \n",
    "\n",
    "[**The second**](#Second-preprocessing-option) is intended for loading an already encoded string and storing it into variable we will use going forward. This will save a signifigant amount of time, as it takes a while to encode a large string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First preprocessing option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "encode_string\n",
    "-----------\n",
    "Generate a dictionary representation of the characters found \n",
    "in a string keyed with integer representations\n",
    "\n",
    "Returns two dictionaries and an array. The two dictionaries are \n",
    "necessary to convert the string to integer representation\n",
    "and back again. The array is the string encoded as integer values.\n",
    "\n",
    "parameters\n",
    "----------\n",
    "content:      string; to be processed\n",
    "\n",
    "return\n",
    "----------\n",
    "vocab_to_int: dict; character to integer representation of unique characters in the string\n",
    "\n",
    "int_to_vocab: dict; integer to string representation\n",
    "\n",
    "encoded:      array; string encoded as integer values\n",
    "\"\"\"\n",
    "\n",
    "def pre_proc(content):   \n",
    "    # Convert the string \"content\" into a list of intergers\n",
    "#   -------------------------------------------------\n",
    "#     ### creates a set of the individual characters\n",
    "#     vocab = set(content)\n",
    "#     ### attempt to clean out non-ascii characters\n",
    "#     vocab_c = copy(vocab)\n",
    "#     for i, char in enumerate(vocab_c):\n",
    "#         if re.search(r_all_ascii,char):\n",
    "#             vocab.remove(char)\n",
    "#     print(vocab)\n",
    "#     print(len(vocab))\n",
    "#     ### use the set to sequentially generate a dictionary\n",
    "#     vocab_to_int = {c: i for i, c in enumerate(vocab)} \n",
    "#     # print(vocab_to_int)\n",
    "#     ### make keys the numerical values\n",
    "#     int_to_vocab = dict(enumerate(vocab)) \n",
    "    \n",
    "#     ### encode the \"content\" string using dict\n",
    "#     ### encoded = np.array([vocab_to_int[c] for c in content], dtype=np.int32)\n",
    "    \n",
    "#     encoded = np.array([],dtype=np.int16)\n",
    "#     for c in content:\n",
    "#         if c in vocab_to_int:\n",
    "#             encoded = np.append(encoded,vocab_to_int[c])\n",
    "#   -------------------------------------------------\n",
    "\n",
    "\n",
    "# use the bellow lines if you want a dictionary of all basic ASCII charcters.\n",
    "# otherwise, comment out.\n",
    "#   -------------------------------------------------\n",
    "    int_to_vocab = {i: chr(i) for i in range(127)}\n",
    "    vocab_to_int = {chr(i): i for i in range(127)}\n",
    "\n",
    "    encoded = np.array([],dtype=np.int16)\n",
    "    for c in content:\n",
    "        if c in vocab_to_int:\n",
    "            encoded = np.append(encoded,vocab_to_int[c])     \n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second preprocessing option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if you want to use data that is alread preprocessed \n",
    "def pre_proc(content):    \n",
    "    import json\n",
    "    \n",
    "    infile1 = \"./encoded.txt\"        # path to encoded string\n",
    "    infile2 = \"./vocab_to_int.json\"\n",
    "    infile3 = \"./int_to_vocab.json\"\n",
    "    \n",
    "    encoded = np.loadtxt(infile1, dtype=int) # load as an array of integers\n",
    "    \n",
    "#     with open(infile2, 'r') as fp:\n",
    "#         vocab_to_int = json.load(fp)\n",
    "    \n",
    "#     with open(infile3, 'r') as fp:\n",
    "#         int_to_vocab = json.load(fp)\n",
    "        \n",
    "    int_to_vocab = {i: chr(i) for i in range(127)}\n",
    "    vocab_to_int = {chr(i): i for i in range(127)}\n",
    "#   --------------------------------------------------    \n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the preprocessing funtion\n",
    "If you run the next cell, you will see the encoded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab, encoded = pre_proc(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~'}\n",
      "\n",
      "Encoded string: [105 109 112 ...  41  10  10]\n"
     ]
    }
   ],
   "source": [
    "#print(content)\n",
    "print(int_to_vocab)\n",
    "# this is all of the files concatenated. with each character encoded using the int_to_vocab\n",
    "print()\n",
    "print(\"Encoded string:\",encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\rightarrow$ Save encoded array to avoid heavy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile1 = \"./encoded.txt\"\n",
    "outfile2 = \"./vocab_to_int.json\"\n",
    "outfile3 = \"./int_to_vocab.json\"\n",
    "\n",
    "np.savetxt(outfile,encoded, fmt='%d')\n",
    "\n",
    "# with open(outfile2, 'w') as fp:\n",
    "#     json.dump(vocab_to_int, fp)\n",
    "\n",
    "# with open(outfile3, 'w') as fp:\n",
    "#     json.dump(int_to_vocab, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape data into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sequenc_gen\n",
    "---------------\n",
    "\n",
    "Partition an array of encoded characters into sequences.\n",
    "\n",
    "Parameters\n",
    "---------------\n",
    "encoded:         array of encoded characters; representation of a string\n",
    "vocab_to_int:    dictionary for conversion from character to integer\n",
    "int_to_vocab:    dictionary for conversion from integer to character\n",
    "\n",
    "Settings\n",
    "--------------\n",
    "sequence_length: Specify the desired length of the sequences\n",
    "\"\"\"\n",
    "def sequence_gen(encoded,vocab_to_int,int_to_vocab, **params):\n",
    "    global n_chars, n_vocab, n_patterns, datax, datay\n",
    "    n_chars = len(encoded)\n",
    "    n_vocab = len(vocab_to_int)\n",
    "    seq_len = params.pop(\"sequence_length\") # change from 50\n",
    "    datax = []\n",
    "    datay = []\n",
    "\n",
    "    # Loop through the encoded data and store \n",
    "    # sequences in datax and datay\n",
    "    for i in range(0, n_chars - seq_len, 1):\n",
    "        seq_in = encoded[i:i + seq_len] \n",
    "        seq_out = encoded[i + seq_len]\n",
    "        datax.append(seq_in)\n",
    "        datay.append(seq_out)\n",
    "    n_patterns = len(datax)\n",
    "    print(\"Total patterns: \", n_patterns)\n",
    "    print(\"Total unique characters: \", n_vocab)\n",
    "    print (\"\\\"\", ''.join([int_to_vocab[value] for value in datax[100]]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  1112800\n",
      "Total unique characters:  127\n",
      "\" ing bolzano\n",
      "\n",
      "    start = a\n",
      "    end = b\n",
      "    if function(a) == 0:  # one of the a or b is a root for t \"\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "sequence_gen(encoded,vocab_to_int, int_to_vocab, sequence_length=seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape the sequences in a format that is better suited to LSTM units\n",
    "We will retain the pattern x sequence shape while adding a additonal dimension to represent the number of features; in this case one, since the each value can only represent one ASCII character. In terms of the LSTM, the sequence length will function as time steps for our LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# reshape datax -- > [n_patterns, time steps, features]\n",
    "X = np.reshape(datax, (n_patterns,seq_len,1))\n",
    "X = X / float(n_vocab)\n",
    "Y = np_utils.to_categorical(datay)\n",
    "#Y = np.asarray(datay) # for sparse categorical cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "Here are the primary componets of the model\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(Y.shape[1], activation='softmax')\n",
    "```\n",
    "Two layers in the model are commonly found in many other machine learning applications, so we will focus on the most detrimental layer(s) to the effectiveness of this model: the LSTM layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](The_LSTM_cell.png)\n",
    "*From wikipedia*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above depicts what is commonly found in **LSTM cells**. A typical LSTM has an additional input over vanilla RNNs known as the **cell-state vector**. This vector along with the **hidden-state vector** and **input data** allow the LSTM cell to \"remember\" or \"forget\" certain sequences.\n",
    "\n",
    "There are 3 primary gates within a cell that utilize the sigmoid function:\n",
    "1. input gate $\\rightarrow$ controls whether the memory cell is updated; contributing to the cell-state\n",
    "2. forget gate $\\rightarrow$ controls if the memory cell is reset to zero; also contributing to the cell-state\n",
    "3. oupute gate $\\rightarrow$ controls if the information of the current cell state is made visable; directly contributing to the hidden-state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: breifly explain compilation choice**\n",
    "```python\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "\n",
    "# hidden vector size is still arbitrary at this point of testing,\n",
    "# so it is hard coded to 128\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y.shape[1], activation='softmax')) # output should be probabilities of character options\n",
    "\n",
    "#---------------------------------------\n",
    "# optimizer = RMSprop(learning_rate=0.05)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizer,\n",
    "#               metrics=['accuracy'])\n",
    "#---------------------------------------\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving trained weights\n",
    "Because this model takes a significant amount of time to train (about 10 minutes at 100 epochs on RTX 2070 super), we decided to save the weights that produce the lowest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = \"best-weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',verbose=1, save_best_only=True, mode='min')\n",
    "callback_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "**TODO: explain training method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120000 samples\n",
      "Epoch 1/250\n",
      "119500/120000 [============================>.] - ETA: 0s - loss: 3.3983 - accuracy: 0.2588\n",
      "Epoch 00001: loss did not improve from 1.25241\n",
      "120000/120000 [==============================] - 10s 87us/sample - loss: 3.3977 - accuracy: 0.2589\n",
      "Epoch 2/250\n",
      "119500/120000 [============================>.] - ETA: 0s - loss: 3.1574 - accuracy: 0.2552\n",
      "Epoch 00002: loss did not improve from 1.25241\n",
      "120000/120000 [==============================] - 6s 50us/sample - loss: 3.1574 - accuracy: 0.2551\n",
      "Epoch 3/250\n",
      " 83500/120000 [===================>..........] - ETA: 1s - loss: 3.0537 - accuracy: 0.2609\n",
      "Epoch 00003: loss did not improve from 1.25241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-01401da99294>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#model.get_weights().shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m120000\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m120000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m   \u001b[0mconstant_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[1;34m(tensor, partial)\u001b[0m\n\u001b[0;32m    820\u001b[0m   \"\"\"\n\u001b[0;32m    821\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \"\"\"\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.fit(inp, targets, steps_per_epoch=10, epochs=10)\n",
    "#model.get_weights().shape\n",
    "\n",
    "model.fit(X[:120000] , Y[:120000], epochs=250, batch_size=500, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 17s 291us/sample - loss: 1.0410 - accuracy: 0.7082\n",
      "[1.041020267889897, 0.70821667]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# model.save('.\\model')\n",
    "score = model.evaluate(X[60000:120000], Y[60000:120000])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best weights recorded from training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename should reflect the name of the best weights available \n",
    "# in th local directory after training\n",
    "filename = \"best-weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model's ability to generate code\n",
    "**TODO: explain generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" > geometric_series(4, -2, 2)\n",
      "    [-2, '-4.0', '-8.0', '-16.0']\n",
      "    >>> geometric_series(-4, 2, 2)\n",
      "   \"\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(datax)-1)\n",
    "pattern = []\n",
    "pattern = datax[start]\n",
    "print(\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_vocab[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_vocab[index]\n",
    "    seq_in = [int_to_vocab[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern = np.append(pattern, [index], axis=0) # TODO make so length of pattern is 101, as it should be\n",
    "    length = pattern.shape[0] \n",
    "    pattern = pattern[1:length] # issue with the length. no matter what, length of pattern become 99\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible TODO: Use autoencoder to translate natural language into code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [LSTM: A search space odyssey](https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
