{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\"\"\"\n",
    "Numpy: matrix manipulation and math\n",
    "Pandas: csv parsing and various data structure tasks\n",
    "Mathpltlib.pyplot: data visualization\n",
    "set_trace: debug breaks\n",
    "keras: a machine learning library that is intuitive to read\n",
    "tensorflow: backend for keras... also the most widely used machine learning library\n",
    "re: regular expressions\n",
    "\"\"\"\n",
    "from copy import deepcopy as copy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sci\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# not needed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "tf.config.optimizer.set_jit(True) # optimizes cpu usage\n",
    "\n",
    "import re\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "concat_files\n",
    "----------\n",
    "\n",
    "Concatenate text files in a directory as a string\n",
    "\n",
    "dependent on 'os' Python module\n",
    "\n",
    "parameters\n",
    "----------\n",
    "directory: string; path of the target directory\n",
    "\n",
    "f_type:    tuple of strings; target file extentsions Ex: ('.py', '.cpp')\n",
    "\n",
    "return\n",
    "---------\n",
    "content:   concatenated string\n",
    "\n",
    "\"\"\"\n",
    "def concat_files(directory,f_type):\n",
    "    import os\n",
    "    # List all file in the dataset directory\n",
    "    # ------------------\n",
    "    all_file = []\n",
    "    content = \"\"\n",
    "\n",
    "    # walk through every directory and open every f_type file\n",
    "    # concatenate into var string \"content\"\n",
    "    for root, dirs, files in os.walk(directory): \n",
    "        for name in files:\n",
    "            if name.endswith(f_type): # we only care about .py\n",
    "                all_file.append(name)\n",
    "                with open(os.path.join(root,name), \"r\") as f:\n",
    "                    content += f.read() + \"\\n\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = concat_files(\"dataset\",('.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_ascii = \"[^\\x00-\\x7F]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "encode_string\n",
    "-----------\n",
    "Generate a dictionary representation of the characters found \n",
    "in a string keyed with integer representations\n",
    "\n",
    "Returns two dictionaries and an array. The two dictionaries are \n",
    "necessary to convert the string to integer representation\n",
    "and back again. The array is the string encoded as integer values.\n",
    "\n",
    "parameters\n",
    "----------\n",
    "content:      string; to be processed\n",
    "\n",
    "return\n",
    "----------\n",
    "vocab_to_int: dict; character to integer representation of unique characters in the string\n",
    "\n",
    "int_to_vocab: dict; integer to string representation\n",
    "\n",
    "encoded:      array; string encoded as integer values\n",
    "\"\"\"\n",
    "\n",
    "def encode_string(content):   \n",
    "    # Convert the string \"content\" into a list of intergers\n",
    "    \n",
    "    ### creates a set of the individual characters\n",
    "    vocab = set(content)\n",
    "    ### attempt to clean out non-ascii characters\n",
    "    vocab_c = copy(vocab)\n",
    "    for i, char in enumerate(vocab_c):\n",
    "        if re.search(r_all_ascii,char):\n",
    "            vocab.remove(char)\n",
    "    print(vocab)\n",
    "    print(len(vocab))\n",
    "    ### use the set to sequentially generate a dictionary\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)} \n",
    "    print(vocab_to_int)\n",
    "    ### make keys the numerical values\n",
    "    int_to_vocab = dict(enumerate(vocab)) \n",
    "    \n",
    "    ### encode the \"content\" string using dict\n",
    "    ### encoded = np.array([vocab_to_int[c] for c in content], dtype=np.int32)\n",
    "    \n",
    "    # *** Uncomment the below lines if you haven't saved the encoded array\n",
    "    # Then rerun cell\n",
    "#   -------------------------------------------------\n",
    "#     encoded = np.array([],dtype=np.int16)\n",
    "#     for c in content:\n",
    "#         if c in vocab_to_int:\n",
    "#             encoded = np.append(encoded,vocab_to_int[c])\n",
    "#   -------------------------------------------------\n",
    "    encoded = np.load('./encoded.npy') # comment out if above lines are uncommented\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[', 'i', 'Y', 'V', '$', 'H', 't', 'm', '~', '6', 'a', 'd', 'Q', 'M', '\\t', '%', 'y', '_', '2', ']', 'Z', '{', '|', 'b', 'v', '.', 'w', '7', 'D', ',', '\\\\', '/', '`', 'j', '*', 'e', 'l', 'n', 'S', ';', 'B', 'E', '<', 'g', 'f', 'L', '8', '+', '#', '&', ' ', 'G', 'C', '\"', 'c', ')', '\\n', 'h', '5', '!', '^', 'K', '9', 'x', ':', 'o', 'k', '3', 'z', 'N', 'J', 'I', 'U', '-', '>', '=', 'A', '?', 'W', 'p', \"'\", '@', 'r', 'X', 'u', '4', 'F', 'P', 'T', '(', 'R', 's', 'O', '0', '}', 'q', '1'}\n",
      "97\n",
      "{'[': 0, 'i': 1, 'Y': 2, 'V': 3, '$': 4, 'H': 5, 't': 6, 'm': 7, '~': 8, '6': 9, 'a': 10, 'd': 11, 'Q': 12, 'M': 13, '\\t': 14, '%': 15, 'y': 16, '_': 17, '2': 18, ']': 19, 'Z': 20, '{': 21, '|': 22, 'b': 23, 'v': 24, '.': 25, 'w': 26, '7': 27, 'D': 28, ',': 29, '\\\\': 30, '/': 31, '`': 32, 'j': 33, '*': 34, 'e': 35, 'l': 36, 'n': 37, 'S': 38, ';': 39, 'B': 40, 'E': 41, '<': 42, 'g': 43, 'f': 44, 'L': 45, '8': 46, '+': 47, '#': 48, '&': 49, ' ': 50, 'G': 51, 'C': 52, '\"': 53, 'c': 54, ')': 55, '\\n': 56, 'h': 57, '5': 58, '!': 59, '^': 60, 'K': 61, '9': 62, 'x': 63, ':': 64, 'o': 65, 'k': 66, '3': 67, 'z': 68, 'N': 69, 'J': 70, 'I': 71, 'U': 72, '-': 73, '>': 74, '=': 75, 'A': 76, '?': 77, 'W': 78, 'p': 79, \"'\": 80, '@': 81, 'r': 82, 'X': 83, 'u': 84, '4': 85, 'F': 86, 'P': 87, 'T': 88, '(': 89, 'R': 90, 's': 91, 'O': 92, '0': 93, '}': 94, 'q': 95, '1': 96}\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab, encoded = encode_string(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\rightarrow$ Save encoded array to avoid heavy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tempfile import TemporaryFile as TF\n",
    "outfile = \"./encoded\"\n",
    "\n",
    "np.save(outfile,encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[', 1: 'i', 2: 'Y', 3: 'V', 4: '$', 5: 'H', 6: 't', 7: 'm', 8: '~', 9: '6', 10: 'a', 11: 'd', 12: 'Q', 13: 'M', 14: '\\t', 15: '%', 16: 'y', 17: '_', 18: '2', 19: ']', 20: 'Z', 21: '{', 22: '|', 23: 'b', 24: 'v', 25: '.', 26: 'w', 27: '7', 28: 'D', 29: ',', 30: '\\\\', 31: '/', 32: '`', 33: 'j', 34: '*', 35: 'e', 36: 'l', 37: 'n', 38: 'S', 39: ';', 40: 'B', 41: 'E', 42: '<', 43: 'g', 44: 'f', 45: 'L', 46: '8', 47: '+', 48: '#', 49: '&', 50: ' ', 51: 'G', 52: 'C', 53: '\"', 54: 'c', 55: ')', 56: '\\n', 57: 'h', 58: '5', 59: '!', 60: '^', 61: 'K', 62: '9', 63: 'x', 64: ':', 65: 'o', 66: 'k', 67: '3', 68: 'z', 69: 'N', 70: 'J', 71: 'I', 72: 'U', 73: '-', 74: '>', 75: '=', 76: 'A', 77: '?', 78: 'W', 79: 'p', 80: \"'\", 81: '@', 82: 'r', 83: 'X', 84: 'u', 85: '4', 86: 'F', 87: 'P', 88: 'T', 89: '(', 90: 'R', 91: 's', 92: 'O', 93: '0', 94: '}', 95: 'q', 96: '1'}\n",
      "[48 59 31 ... 55 56 56]\n"
     ]
    }
   ],
   "source": [
    "#print(content)\n",
    "print(int_to_vocab)\n",
    "# this is all of the files concatenated. with each character encoded using the int_to_vocab\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = [50,100,150,200]\n",
    "# for i in range(500): \n",
    "#     segment.append((int(len(encoded) / 500) * (i + 1)))\n",
    "batch = {\n",
    "    \"x\" : [\n",
    "        encoded[:segment[0]],\n",
    "        encoded[segment[0]:segment[1]]\n",
    "    ],\n",
    "    \"y\" : [\n",
    "        encoded[segment[1]:segment[2]],\n",
    "        encoded[segment[2]:segment[3]]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 97)\n"
     ]
    }
   ],
   "source": [
    "# inp = np.vstack((batch['x'][0],batch['x'][1]))\n",
    "# targets = np.vstack((batch['y'][0],batch['y'][1]))\n",
    "inp = batch['x'][0]\n",
    "targets = batch['y'][0]\n",
    "Xtest = batch['x'][1]\n",
    "Ttest = batch['y'][1]\n",
    "io_size = len(int_to_vocab)\n",
    "\n",
    "# inp = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(inp,io_size)))\n",
    "# targets = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(targets,io_size)))\n",
    "# Xtest = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(Xtest,io_size)))\n",
    "# Ttest = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(Ttest,io_size)))\n",
    "\n",
    "inp = tf.one_hot(inp,io_size)\n",
    "targets = tf.one_hot(targets,io_size)\n",
    "Xtest = tf.one_hot(Xtest,io_size)\n",
    "Ttest = tf.one_hot(Ttest,io_size)\n",
    "\n",
    "# encoded_as_i = np.hstack((np.ones((1,1)),tf.one_hot(encoded[:1],io_size)))\n",
    "encoded_as_i = tf.one_hot(encoded[:40],io_size)\n",
    "\n",
    "#tf.print(Xtest, summarize=50)\n",
    "print(inp.shape)\n",
    "\n",
    "# inp = tf.expand_dims(inp,2)\n",
    "# targets = tf.expand_dims(targets,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = []\n",
    "# temp = []\n",
    "# for i, char in enumerate(encoded):\n",
    "#     if int_to_vocab[char] == '\\n':\n",
    "#         lines.append(temp)\n",
    "#     else:\n",
    "#         temp.append(char)\n",
    "#         temp = []\n",
    "\n",
    "# x = np.zeros((len(lines), maxlen, len(int_to_vocab)), dtype=np.bool)\n",
    "# y = np.zeros((len(sentences), len(int_to_vocab)), dtype=np.bool)\n",
    "# for i, sentence in enumerate(sentences):\n",
    "#     for t, char in enumerate(sentence):\n",
    "#         x[i, t, char_indices[char]] = 1\n",
    "#     y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bran/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 18s 363ms/step - loss: 3.0160 - accuracy: 0.1752\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 16s 323ms/step - loss: 2.8579 - accuracy: 0.2060\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 2.2597 - accuracy: 0.2884\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 15s 308ms/step - loss: 1.8770 - accuracy: 0.4268\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 1.5137 - accuracy: 0.4972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f50b8df0f50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "\n",
    "# x_train = np.hstack((batch['x'][0],batch['x'][1]))\n",
    "# y_train = np.hstack((batch['y'][0],batch['y'][1]))\n",
    "# x_test = np.hstack((batch['x'][0],batch['x'][1]))\n",
    "# y_test = np.hstack((batch['y'][0],batch['y'][1]))\n",
    "\n",
    "max_features = 97\n",
    "#maxlen = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, output_dim=97))\n",
    "model.add(LSTM(97, input_shape=inp.shape))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(io_size, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(Xtest, Xtest, steps_per_epoch=50, epochs=5)\n",
    "#model.get_weights().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 4s 79ms/step\n",
      "[0.12511909484863282, 23.000000417232513]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model.save('./model')\n",
    "score = model.evaluate(inp, inp, steps=50)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(40, 97), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4.0382799e-02 3.6107301e-07 2.2326464e-07 ... 2.5392863e-05\n",
      "  2.9392481e-07 2.9682593e-07]\n",
      " [3.0115703e-02 9.5992255e-07 5.8833479e-07 ... 3.8130096e-05\n",
      "  7.7778634e-07 7.9772832e-07]\n",
      " [4.4122519e-04 1.1850213e-07 7.9293002e-08 ... 4.3012276e-02\n",
      "  1.0135348e-07 1.2478394e-07]\n",
      " ...\n",
      " [1.2823937e-03 1.7435547e-06 1.2205473e-06 ... 4.7490899e-05\n",
      "  1.3142378e-06 1.4809921e-06]\n",
      " [1.5982764e-02 4.5820761e-06 2.8151956e-06 ... 7.4550655e-05\n",
      "  3.8831436e-06 4.0619348e-06]\n",
      " [3.8662888e-02 4.1631239e-07 2.5747269e-07 ... 2.6746873e-05\n",
      "  3.3779594e-07 3.4244664e-07]], shape=(120, 97), dtype=float32)\n",
      "[ 0  8  8 34  8  8  0  0 34  8  8  8 34 34 34  8 34  8 34 34  0  0  0  1\n",
      "  1 34  8  8  8  9  8  8 34  8  8  8  8  8  8  8  8 34  8 34 34  8 34  8\n",
      " 34  8 34 13 34  8 11 34  8  8 34  8  8 34  8  8  8  8 29 34 37 34 34 34\n",
      " 14 34  8  8 34  8 34 34 34  7  8 34  8 34  8  8 34  8  8 34  8  8  3 34\n",
      " 34]\n",
      "0.99999994\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(tf.keras.backend.shape(Xtest),tf.keras.backend.shape(model.weights))\n",
    "# sci.softmax(Xtest @ encoded_to_i)\n",
    "print(encoded_as_i)\n",
    "print(tf.convert_to_tensor(model.predict(encoded_as_i, steps=3, verbose=0)))\n",
    "print(np.argmax(model.predict(encoded_as_i, steps=3, verbose=0),axis=0))\n",
    "print(np.sum(model.predict(encoded_as_i, steps=3, verbose=0)[0]))\n",
    "nextc = int_to_vocab[np.argmax(model.predict(encoded_as_i, steps=3, verbose=0),axis=0)[1]]\n",
    "print(nextc) # should give probabilities of next character\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iiiiiiiiiittiiiiyiiiiiiiiiyiiiiiiitittiittiiiiiiiitiiiiitiiiiiii\\\\\\iiiiiiiiiiiiiiititiitiiitiiiii\n"
     ]
    }
   ],
   "source": [
    "strout = \"\"\n",
    "data = Xtest\n",
    "#actual = \"\"\n",
    "predict = np.argmax(model.predict(data, steps=70, verbose=0),axis=0)\n",
    "for i in predict:\n",
    "    \n",
    "    nextc = int_to_vocab[predict[i]]\n",
    "    \n",
    "    #actual += int_to_vocab[data[i]]\n",
    "    strout += nextc\n",
    "print(strout)\n",
    "#print(actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [LSTM: A search space odyssey](https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
