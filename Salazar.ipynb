{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "\"\"\"\n",
    "Numpy: matrix manipulation and math\n",
    "Pandas: csv parsing and various data structure tasks\n",
    "Mathpltlib.pyplot: data visualization\n",
    "set_trace: debug breaks\n",
    "keras: a machine learning library that is intuitive to read\n",
    "tensorflow: backend for keras... also the most widely used machine learning library\n",
    "re: regular expressions\n",
    "\"\"\"\n",
    "from copy import deepcopy as copy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# not needed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "tf.config.optimizer.set_jit(True) # optimizes cpu usage\n",
    "\n",
    "import re\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "concat_files\n",
    "----------\n",
    "\n",
    "Concatenate text files in a directory as a string\n",
    "\n",
    "dependent on 'os' Python module\n",
    "\n",
    "parameters\n",
    "----------\n",
    "directory: string; path of the target directory\n",
    "\n",
    "f_type:    tuple of strings; target file extentsions Ex: ('.py', '.cpp')\n",
    "\n",
    "return\n",
    "---------\n",
    "content:   concatenated string\n",
    "\n",
    "\"\"\"\n",
    "def concat_files(directory,f_type):\n",
    "    import os\n",
    "    # List all file in the dataset directory\n",
    "    # ------------------\n",
    "    all_file = []\n",
    "    content = \"\"\n",
    "\n",
    "    # walk through every directory and open every f_type file\n",
    "    # concatenate into var string \"content\"\n",
    "    for root, dirs, files in os.walk(directory): \n",
    "        for name in files:\n",
    "            if name.endswith(f_type): # we only care about .py\n",
    "                all_file.append(name)\n",
    "                with open(os.path.join(root,name), \"r\") as f:\n",
    "                    content += f.read() + \"\\n\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = concat_files(\"dataset\",('.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_ascii = \"[^\\x00-\\x7F]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "encode_string\n",
    "-----------\n",
    "Generate a dictionary representation of the characters found \n",
    "in a string keyed with integer representations\n",
    "\n",
    "Returns two dictionaries and an array. The two dictionaries are \n",
    "necessary to convert the string to integer representation\n",
    "and back again. The array is the string encoded as integer values.\n",
    "\n",
    "parameters\n",
    "----------\n",
    "content:      string; to be processed\n",
    "\n",
    "return\n",
    "----------\n",
    "vocab_to_int: dict; character to integer representation of unique characters in the string\n",
    "\n",
    "int_to_vocab: dict; integer to string representation\n",
    "\n",
    "encoded:      array; string encoded as integer values\n",
    "\"\"\"\n",
    "\n",
    "def encode_string(content):   \n",
    "    # Convert the string \"content\" into a list of intergers\n",
    "    \n",
    "    # creates a set of the individual characters\n",
    "    vocab = set(content)\n",
    "    # attempt to clean out non-ascii characters\n",
    "#     vocab_c = vocab.copy()\n",
    "#     for i, char in enumerate(vocab_c):\n",
    "#         if re.search(r_all_ascii,char):\n",
    "#             vocab.remove(char)\n",
    "#     print(vocab)\n",
    "#     print(len(vocab))\n",
    "    # use the set to sequentially generate a dictionary\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)} \n",
    "    \n",
    "    # make keys the numerical values\n",
    "    int_to_vocab = dict(enumerate(vocab)) \n",
    "    \n",
    "    # encode the \"content\" string using dict\n",
    "    encoded = np.array([vocab_to_int[c] for c in content], dtype=np.int32)\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab, encoded = encode_string(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 's', 1: '|', 2: '<', 3: 'u', 4: '‚â•', 5: '>', 6: '?', 7: ',', 8: 'S', 9: 'Q', 10: '‚àö', 11: '_', 12: 'l', 13: 'Áª¥', 14: 'j', 15: 'K', 16: 'X', 17: 'ÔºÅ', 18: '5', 19: 'F', 20: 'T', 21: '‰∏ã', 22: 'Âà∞', 23: 'V', 24: '≈Ç', 25: '[', 26: \"'\", 27: '~', 28: 'e', 29: '9', 30: '‚Äò', 31: '¬Ω', 32: 'i', 33: 'W', 34: '@', 35: 'q', 36: 'Â∫¶', 37: 'o', 38: 'p', 39: 'A', 40: '‚Äì', 41: '‚â§', 42: 'r', 43: '√º', 44: 'b', 45: '√ó', 46: '`', 47: '\\t', 48: '!', 49: 'E', 50: '¬£', 51: 'üòÅ', 52: 'Z', 53: '}', 54: '$', 55: '‚Äô', 56: ')', 57: 'R', 58: '=', 59: '√£', 60: '1', 61: '#', 62: 'Ôºà', 63: 'O', 64: 'L', 65: '‚â†', 66: 'g', 67: 'h', 68: 'D', 69: '6', 70: '-', 71: 'Ôºö', 72: '3', 73: 'z', 74: '√Ö', 75: '‚àí', 76: ']', 77: '.', 78: '¬µ', 79: '4', 80: 'm', 81: '/', 82: 'Á≤æ', 83: 'f', 84: 'y', 85: 'Ôºå', 86: '*', 87: '\"', 88: ' ', 89: 'Ëææ', 90: 'k', 91: 'ü§ì', 92: 'c', 93: 'Âçà', 94: '^', 95: '¬≤', 96: '(', 97: 'C', 98: 'a', 99: '8', 100: '\\n', 101: ';', 102: 'n', 103: 'w', 104: '2', 105: 'I', 106: '‚Üí', 107: 'J', 108: 'd', 109: '+', 110: 'U', 111: '&', 112: 'B', 113: '7', 114: '0', 115: '%', 116: 'M', 117: 'êÄè', 118: '·êÉ', 119: 'P', 120: ':', 121: 'x', 122: 'v', 123: '\\\\', 124: '\\xa0', 125: '\\u202c', 126: 'N', 127: 'H', 128: 'Ôºâ', 129: 'G', 130: 'Y', 131: 't', 132: '{'}\n",
      "[ 61  48  81 ...  56 100 100]\n"
     ]
    }
   ],
   "source": [
    "#print(content)\n",
    "print(int_to_vocab)\n",
    "# this is all of the files concatenated. with each character encoded using the int_to_vocab\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = [10,20,30,40]\n",
    "# for i in range(500): \n",
    "#     segment.append((int(len(encoded) / 500) * (i + 1)))\n",
    "batch = {\n",
    "    \"x\" : [\n",
    "        encoded[:segment[0]],\n",
    "        encoded[segment[0]:segment[1]]\n",
    "    ],\n",
    "    \"y\" : [\n",
    "        encoded[segment[1]:segment[2]],\n",
    "        encoded[segment[2]:segment[3]]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "(10, 133)\n"
     ]
    }
   ],
   "source": [
    "# inp = np.vstack((batch['x'][0],batch['x'][1]))\n",
    "# targets = np.vstack((batch['y'][0],batch['y'][1]))\n",
    "inp = batch['x'][0]\n",
    "targets = batch['y'][0]\n",
    "io_size = len(int_to_vocab)\n",
    "\n",
    "inp = tf.one_hot(inp,io_size)\n",
    "targets = tf.one_hot(targets,io_size)\n",
    "tf.print(inp, summarize=50)\n",
    "print(inp.shape)\n",
    "\n",
    "# inp = tf.expand_dims(inp,2)\n",
    "# targets = tf.expand_dims(targets,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = []\n",
    "# temp = []\n",
    "# for i, char in enumerate(encoded):\n",
    "#     if int_to_vocab[char] == '\\n':\n",
    "#         lines.append(temp)\n",
    "#     else:\n",
    "#         temp.append(char)\n",
    "#         temp = []\n",
    "\n",
    "# x = np.zeros((len(lines), maxlen, len(int_to_vocab)), dtype=np.bool)\n",
    "# y = np.zeros((len(sentences), len(int_to_vocab)), dtype=np.bool)\n",
    "# for i, sentence in enumerate(sentences):\n",
    "#     for t, char in enumerate(sentence):\n",
    "#         x[i, t, char_indices[char]] = 1\n",
    "#     y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bran/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 41s 309ms/step - loss: 2.3270 - accuracy: 0.1729\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 39s 292ms/step - loss: 2.0558 - accuracy: 0.2045\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 38s 284ms/step - loss: 2.0519 - accuracy: 0.2023\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 39s 294ms/step - loss: 2.0448 - accuracy: 0.1940\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 38s 288ms/step - loss: 2.0352 - accuracy: 0.1917\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 39s 296ms/step - loss: 2.0367 - accuracy: 0.2113\n",
      "Epoch 7/10\n",
      " 17/133 [==>...........................] - ETA: 37s - loss: 2.0366 - accuracy: 0.1941"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "\n",
    "# x_train = np.hstack((batch['x'][0],batch['x'][1]))\n",
    "# y_train = np.hstack((batch['y'][0],batch['y'][1]))\n",
    "# x_test = np.hstack((batch['x'][0],batch['x'][1]))\n",
    "# y_test = np.hstack((batch['y'][0],batch['y'][1]))\n",
    "\n",
    "max_features = 1024\n",
    "#maxlen = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, output_dim=256))\n",
    "model.add(LSTM(10, input_shape=inp.shape))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(io_size, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(inp, targets, steps_per_epoch=133, epochs=10)\n",
    "score = model.evaluate(inp, targets, steps_per_epoch=133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
