{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\"\"\"\n",
    "Numpy: matrix manipulation and math\n",
    "Pandas: csv parsing and various data structure tasks\n",
    "Mathpltlib.pyplot: data visualization\n",
    "set_trace: debug breaks\n",
    "keras: a machine learning library that is intuitive to read\n",
    "tensorflow: backend for keras... also the most widely used machine learning library\n",
    "re: regular expressions\n",
    "\"\"\"\n",
    "from copy import deepcopy as copy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sci\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# not needed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "tf.config.optimizer.set_jit(True) # optimizes cpu usage\n",
    "\n",
    "import re\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "concat_files\n",
    "----------\n",
    "\n",
    "Concatenate text files in a directory as a string\n",
    "\n",
    "dependent on 'os' Python module\n",
    "\n",
    "parameters\n",
    "----------\n",
    "directory: string; path of the target directory\n",
    "\n",
    "f_type:    tuple of strings; target file extentsions Ex: ('.py', '.cpp')\n",
    "\n",
    "return\n",
    "---------\n",
    "content:   concatenated string\n",
    "\n",
    "\"\"\"\n",
    "def concat_files(directory,f_type):\n",
    "    import os\n",
    "    # List all file in the dataset directory\n",
    "    # ------------------\n",
    "    all_file = []\n",
    "    content = \"\"\n",
    "\n",
    "    # walk through every directory and open every f_type file\n",
    "    # concatenate into var string \"content\"\n",
    "    for root, dirs, files in os.walk(directory): \n",
    "        for name in files:\n",
    "            if name.endswith(f_type): # we only care about .py\n",
    "                all_file.append(name)\n",
    "                with open(os.path.join(root,name), \"r\",encoding=\"utf8\") as f:\n",
    "                    content += f.read() + \"\\n\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = concat_files(\"dataset\",('.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_ascii = \"[^\\x00-\\x7F]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "encode_string\n",
    "-----------\n",
    "Generate a dictionary representation of the characters found \n",
    "in a string keyed with integer representations\n",
    "\n",
    "Returns two dictionaries and an array. The two dictionaries are \n",
    "necessary to convert the string to integer representation\n",
    "and back again. The array is the string encoded as integer values.\n",
    "\n",
    "parameters\n",
    "----------\n",
    "content:      string; to be processed\n",
    "\n",
    "return\n",
    "----------\n",
    "vocab_to_int: dict; character to integer representation of unique characters in the string\n",
    "\n",
    "int_to_vocab: dict; integer to string representation\n",
    "\n",
    "encoded:      array; string encoded as integer values\n",
    "\"\"\"\n",
    "\n",
    "def encode_string(content):   \n",
    "    # Convert the string \"content\" into a list of intergers\n",
    "\n",
    "    ### creates a set of the individual characters\n",
    "    vocab = set(content)\n",
    "    ### attempt to clean out non-ascii characters\n",
    "    vocab_c = copy(vocab)\n",
    "    for i, char in enumerate(vocab_c):\n",
    "        if re.search(r_all_ascii,char):\n",
    "            vocab.remove(char)\n",
    "    print(vocab)\n",
    "    print(len(vocab))\n",
    "    ### use the set to sequentially generate a dictionary\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)} \n",
    "    print(vocab_to_int)\n",
    "    ### make keys the numerical values\n",
    "    int_to_vocab = dict(enumerate(vocab)) \n",
    "    \n",
    "    ### encode the \"content\" string using dict\n",
    "    ### encoded = np.array([vocab_to_int[c] for c in content], dtype=np.int32)\n",
    "    \n",
    "    # *** Uncomment the below lines if you haven't saved the encoded array\n",
    "    # Then rerun cell\n",
    "#   -------------------------------------------------\n",
    "#     encoded = np.array([],dtype=np.int16)\n",
    "#     for c in content:\n",
    "#         if c in vocab_to_int:\n",
    "#             encoded = np.append(encoded,vocab_to_int[c])\n",
    "#   -------------------------------------------------\n",
    "    encoded = np.load('./encoded.npy') # comment out if above lines are uncommented\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'%', 'P', '\\n', '`', '~', 'w', '0', 'u', 'N', '9', '&', 'd', 'F', 'R', '<', 'z', ',', 'M', ']', '/', 't', 'k', 'o', 'L', '8', ':', 'Q', 'W', 'g', '6', '$', '#', '2', 'v', '@', 'i', ' ', 'O', '[', 'I', '>', 'r', 'B', 'E', 'q', 'S', 'K', 'l', 'e', 'C', '^', '|', '*', 'f', 'b', 'p', 'a', ')', 'x', '+', '5', '(', '3', \"'\", '7', '!', 'j', 'm', 'T', '{', ';', '}', '\"', 'Y', 'c', 'U', 'D', '-', '?', '\\\\', 's', 'J', '1', 'h', 'H', 'G', '\\t', '_', 'A', '4', 'X', '=', 'Z', 'y', '.', 'n', 'V'}\n",
      "97\n",
      "{'%': 0, 'P': 1, '\\n': 2, '`': 3, '~': 4, 'w': 5, '0': 6, 'u': 7, 'N': 8, '9': 9, '&': 10, 'd': 11, 'F': 12, 'R': 13, '<': 14, 'z': 15, ',': 16, 'M': 17, ']': 18, '/': 19, 't': 20, 'k': 21, 'o': 22, 'L': 23, '8': 24, ':': 25, 'Q': 26, 'W': 27, 'g': 28, '6': 29, '$': 30, '#': 31, '2': 32, 'v': 33, '@': 34, 'i': 35, ' ': 36, 'O': 37, '[': 38, 'I': 39, '>': 40, 'r': 41, 'B': 42, 'E': 43, 'q': 44, 'S': 45, 'K': 46, 'l': 47, 'e': 48, 'C': 49, '^': 50, '|': 51, '*': 52, 'f': 53, 'b': 54, 'p': 55, 'a': 56, ')': 57, 'x': 58, '+': 59, '5': 60, '(': 61, '3': 62, \"'\": 63, '7': 64, '!': 65, 'j': 66, 'm': 67, 'T': 68, '{': 69, ';': 70, '}': 71, '\"': 72, 'Y': 73, 'c': 74, 'U': 75, 'D': 76, '-': 77, '?': 78, '\\\\': 79, 's': 80, 'J': 81, '1': 82, 'h': 83, 'H': 84, 'G': 85, '\\t': 86, '_': 87, 'A': 88, '4': 89, 'X': 90, '=': 91, 'Z': 92, 'y': 93, '.': 94, 'n': 95, 'V': 96}\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab, encoded = encode_string(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\rightarrow$ Save encoded array to avoid heavy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tempfile import TemporaryFile as TF\n",
    "outfile = \"./encoded\"\n",
    "\n",
    "np.save(outfile,encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '%', 1: 'P', 2: '\\n', 3: '`', 4: '~', 5: 'w', 6: '0', 7: 'u', 8: 'N', 9: '9', 10: '&', 11: 'd', 12: 'F', 13: 'R', 14: '<', 15: 'z', 16: ',', 17: 'M', 18: ']', 19: '/', 20: 't', 21: 'k', 22: 'o', 23: 'L', 24: '8', 25: ':', 26: 'Q', 27: 'W', 28: 'g', 29: '6', 30: '$', 31: '#', 32: '2', 33: 'v', 34: '@', 35: 'i', 36: ' ', 37: 'O', 38: '[', 39: 'I', 40: '>', 41: 'r', 42: 'B', 43: 'E', 44: 'q', 45: 'S', 46: 'K', 47: 'l', 48: 'e', 49: 'C', 50: '^', 51: '|', 52: '*', 53: 'f', 54: 'b', 55: 'p', 56: 'a', 57: ')', 58: 'x', 59: '+', 60: '5', 61: '(', 62: '3', 63: \"'\", 64: '7', 65: '!', 66: 'j', 67: 'm', 68: 'T', 69: '{', 70: ';', 71: '}', 72: '\"', 73: 'Y', 74: 'c', 75: 'U', 76: 'D', 77: '-', 78: '?', 79: '\\\\', 80: 's', 81: 'J', 82: '1', 83: 'h', 84: 'H', 85: 'G', 86: '\\t', 87: '_', 88: 'A', 89: '4', 90: 'X', 91: '=', 92: 'Z', 93: 'y', 94: '.', 95: 'n', 96: 'V'}\n",
      "[48 59 31 ... 55 56 56]\n"
     ]
    }
   ],
   "source": [
    "#print(content)\n",
    "print(int_to_vocab)\n",
    "# this is all of the files concatenated. with each character encoded using the int_to_vocab\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First data process attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "segment = [500,1000,1500,2000]\n",
    "# for i in range(500): \n",
    "#     segment.append((int(len(encoded) / 500) * (i + 1)))\n",
    "batch = {\n",
    "    \"x\" : [\n",
    "        encoded[:segment[0]],\n",
    "        encoded[segment[0]:segment[1]]\n",
    "    ],\n",
    "    \"y\" : [\n",
    "        encoded[1:segment[0]+1],\n",
    "        encoded[segment[0]+1:segment[1]+1]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 97)\n"
     ]
    }
   ],
   "source": [
    "# inp = np.vstack((batch['x'][0],batch['x'][1]))\n",
    "# targets = np.vstack((batch['y'][0],batch['y'][1]))\n",
    "inp = batch['x'][0]\n",
    "targets = batch['y'][0]\n",
    "Xtest = batch['x'][1]\n",
    "Ttest = batch['y'][1]\n",
    "io_size = len(int_to_vocab)\n",
    "\n",
    "# inp = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(inp,io_size)))\n",
    "# targets = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(targets,io_size)))\n",
    "# Xtest = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(Xtest,io_size)))\n",
    "# Ttest = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(Ttest,io_size)))\n",
    "\n",
    "inp = tf.one_hot(inp,io_size)\n",
    "targets = tf.one_hot(targets,io_size)\n",
    "Xtest = tf.one_hot(Xtest,io_size)\n",
    "Ttest = tf.one_hot(Ttest,io_size)\n",
    "\n",
    "# encoded_as_i = np.hstack((np.ones((1,1)),tf.one_hot(encoded[:1],io_size)))\n",
    "encoded_as_i = tf.one_hot(encoded[:500],io_size)\n",
    "\n",
    "#tf.print(Xtest, summarize=50)\n",
    "print(inp.shape)\n",
    "\n",
    "# inp = tf.expand_dims(inp,2)\n",
    "# targets = tf.expand_dims(targets,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = []\n",
    "# temp = []\n",
    "# for i, char in enumerate(encoded):\n",
    "#     if int_to_vocab[char] == '\\n':\n",
    "#         lines.append(temp)\n",
    "#     else:\n",
    "#         temp.append(char)\n",
    "#         temp = []\n",
    "\n",
    "# x = np.zeros((len(lines), maxlen, len(int_to_vocab)), dtype=np.bool)\n",
    "# y = np.zeros((len(sentences), len(int_to_vocab)), dtype=np.bool)\n",
    "# for i, sentence in enumerate(sentences):\n",
    "#     for t, char in enumerate(sentence):\n",
    "#         x[i, t, char_indices[char]] = 1\n",
    "#     y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second data processing attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  1114909\n",
      "Total unique characters:  97\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(encoded)\n",
    "n_vocab = len(vocab_to_int)\n",
    "seq_len = 100\n",
    "datax = []\n",
    "datay = []\n",
    "for i in range(0, n_chars - seq_len, 1):\n",
    "    seq_in = encoded[i:i + seq_len]\n",
    "    seq_out = encoded[i + seq_len]\n",
    "    datax.append(seq_in)\n",
    "    datay.append(seq_out)\n",
    "n_patterns = len(datax)\n",
    "print(\"Total patterns: \", n_patterns)\n",
    "print(\"Total unique characters: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "X = np.reshape(datax, (n_patterns,seq_len,1))\n",
    "X = X / float(n_vocab)\n",
    "Y = np_utils.to_categorical(datay)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "\n",
    "# x_train = np.hstack((batch['x'][0],batch['x'][1]))\n",
    "# y_train = np.hstack((batch['y'][0],batch['y'][1]))\n",
    "# x_test = np.hstack((batch['x'][0],batch['x'][1]))\n",
    "# y_test = np.hstack((batch['y'][0],batch['y'][1]))\n",
    "\n",
    "max_features = 97\n",
    "#maxlen = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "\n",
    "# optimizer = RMSprop(learning_rate=0.05)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizer,\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',verbose=1, save_best_only=True, mode='min')\n",
    "callback_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.6449 - accuracy: 0.5614\n",
      "Epoch 00001: loss improved from inf to 1.64610, saving model to weights-improvement-01-1.646096.hdf5\n",
      "60000/60000 [==============================] - 13s 220us/sample - loss: 1.6461 - accuracy: 0.5611\n",
      "Epoch 2/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.6216 - accuracy: 0.5684\n",
      "Epoch 00002: loss improved from 1.64610 to 1.62152, saving model to weights-improvement-02-1.621522.hdf5\n",
      "60000/60000 [==============================] - 12s 207us/sample - loss: 1.6215 - accuracy: 0.5684\n",
      "Epoch 3/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.5603 - accuracy: 0.5826\n",
      "Epoch 00003: loss improved from 1.62152 to 1.55995, saving model to weights-improvement-03-1.559948.hdf5\n",
      "60000/60000 [==============================] - 13s 211us/sample - loss: 1.5599 - accuracy: 0.5827\n",
      "Epoch 4/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.5323 - accuracy: 0.5888\n",
      "Epoch 00004: loss improved from 1.55995 to 1.53263, saving model to weights-improvement-04-1.532627.hdf5\n",
      "60000/60000 [==============================] - 12s 204us/sample - loss: 1.5326 - accuracy: 0.5886\n",
      "Epoch 5/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.5053 - accuracy: 0.5949\n",
      "Epoch 00005: loss improved from 1.53263 to 1.50491, saving model to weights-improvement-05-1.504908.hdf5\n",
      "60000/60000 [==============================] - 13s 210us/sample - loss: 1.5049 - accuracy: 0.5950\n",
      "Epoch 6/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.5048 - accuracy: 0.5946\n",
      "Epoch 00006: loss did not improve from 1.50491\n",
      "60000/60000 [==============================] - 13s 211us/sample - loss: 1.5050 - accuracy: 0.5945\n",
      "Epoch 7/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.4531 - accuracy: 0.6069\n",
      "Epoch 00007: loss improved from 1.50491 to 1.45317, saving model to weights-improvement-07-1.453165.hdf5\n",
      "60000/60000 [==============================] - 12s 207us/sample - loss: 1.4532 - accuracy: 0.6069\n",
      "Epoch 8/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.4175 - accuracy: 0.6145\n",
      "Epoch 00008: loss improved from 1.45317 to 1.41735, saving model to weights-improvement-08-1.417351.hdf5\n",
      "60000/60000 [==============================] - 13s 215us/sample - loss: 1.4174 - accuracy: 0.6146\n",
      "Epoch 9/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.3852 - accuracy: 0.6222\n",
      "Epoch 00009: loss improved from 1.41735 to 1.38524, saving model to weights-improvement-09-1.385239.hdf5\n",
      "60000/60000 [==============================] - 13s 213us/sample - loss: 1.3852 - accuracy: 0.6223\n",
      "Epoch 10/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.5882 - accuracy: 0.5755\n",
      "Epoch 00010: loss did not improve from 1.38524\n",
      "60000/60000 [==============================] - 13s 212us/sample - loss: 1.5875 - accuracy: 0.5757\n",
      "Epoch 11/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3663 - accuracy: 0.6259\n",
      "Epoch 00011: loss improved from 1.38524 to 1.36623, saving model to weights-improvement-11-1.366231.hdf5\n",
      "60000/60000 [==============================] - 12s 207us/sample - loss: 1.3662 - accuracy: 0.6259\n",
      "Epoch 12/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.3187 - accuracy: 0.6398\n",
      "Epoch 00012: loss improved from 1.36623 to 1.31822, saving model to weights-improvement-12-1.318219.hdf5\n",
      "60000/60000 [==============================] - 11s 177us/sample - loss: 1.3182 - accuracy: 0.6400\n",
      "Epoch 13/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2889 - accuracy: 0.6442\n",
      "Epoch 00013: loss improved from 1.31822 to 1.28903, saving model to weights-improvement-13-1.289028.hdf5\n",
      "60000/60000 [==============================] - 12s 202us/sample - loss: 1.2890 - accuracy: 0.6443\n",
      "Epoch 14/20\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 1.2724 - accuracy: 0.6492\n",
      "Epoch 00014: loss improved from 1.28903 to 1.27295, saving model to weights-improvement-14-1.272953.hdf5\n",
      "60000/60000 [==============================] - 13s 220us/sample - loss: 1.2730 - accuracy: 0.6492\n",
      "Epoch 15/20\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 1.2446 - accuracy: 0.6575\n",
      "Epoch 00015: loss improved from 1.27295 to 1.24577, saving model to weights-improvement-15-1.245769.hdf5\n",
      "60000/60000 [==============================] - 14s 231us/sample - loss: 1.2458 - accuracy: 0.6572\n",
      "Epoch 16/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.2457 - accuracy: 0.6565\n",
      "Epoch 00016: loss did not improve from 1.24577\n",
      "60000/60000 [==============================] - 14s 228us/sample - loss: 1.2460 - accuracy: 0.6565\n",
      "Epoch 17/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.2156 - accuracy: 0.6633\n",
      "Epoch 00017: loss improved from 1.24577 to 1.21504, saving model to weights-improvement-17-1.215045.hdf5\n",
      "60000/60000 [==============================] - 13s 219us/sample - loss: 1.2150 - accuracy: 0.6634\n",
      "Epoch 18/20\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 1.1922 - accuracy: 0.6682\n",
      "Epoch 00018: loss improved from 1.21504 to 1.19226, saving model to weights-improvement-18-1.192256.hdf5\n",
      "60000/60000 [==============================] - 14s 231us/sample - loss: 1.1923 - accuracy: 0.6683\n",
      "Epoch 19/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.1688 - accuracy: 0.6734\n",
      "Epoch 00019: loss improved from 1.19226 to 1.16898, saving model to weights-improvement-19-1.168981.hdf5\n",
      "60000/60000 [==============================] - 14s 226us/sample - loss: 1.1690 - accuracy: 0.6733\n",
      "Epoch 20/20\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 1.8137 - accuracy: 0.5605\n",
      "Epoch 00020: loss did not improve from 1.16898\n",
      "60000/60000 [==============================] - 13s 224us/sample - loss: 1.8221 - accuracy: 0.5586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2acdbde3108>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(inp, targets, steps_per_epoch=10, epochs=10)\n",
    "#model.get_weights().shape\n",
    "\n",
    "model.fit(X[:60000] , Y[:60000], epochs=20, batch_size=128, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 14s 242us/sample - loss: 3.1916 - accuracy: 0.2831\n",
      "[3.191573794937134, 0.28306666]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# model.save('.\\model')\n",
    "score = model.evaluate(X[60000:120000], Y[60000:120000])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt at testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(500, 97), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.6083467e-07 1.5905206e-01 1.6123180e-07 ... 6.8660093e-08\n",
      "  4.9035528e-08 3.6955536e-03]\n",
      " [1.6079790e-07 1.5846752e-01 1.6122462e-07 ... 6.8636282e-08\n",
      "  4.9024273e-08 3.7028429e-03]\n",
      " [1.6086379e-07 1.5974109e-01 1.6122148e-07 ... 6.8684187e-08\n",
      "  4.9045525e-08 3.6875154e-03]\n",
      " ...\n",
      " [1.6086251e-07 1.5962288e-01 1.6122836e-07 ... 6.8681082e-08\n",
      "  4.9044623e-08 3.6887384e-03]\n",
      " [1.6070277e-07 1.5707102e-01 1.6119995e-07 ... 6.8576917e-08\n",
      "  4.8995503e-08 3.7204761e-03]\n",
      " [1.6331968e-07 1.5637104e-01 1.6368018e-07 ... 6.9626587e-08\n",
      "  4.9744887e-08 3.7324438e-03]], shape=(500, 97), dtype=float32)\n",
      "[50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50]\n",
      "1.0000001\n",
      "j\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(tf.keras.backend.shape(Xtest),tf.keras.backend.shape(model.weights))\n",
    "# sci.softmax(Xtest @ encoded_to_i)\n",
    "print(encoded_as_i)\n",
    "print(tf.convert_to_tensor(model.predict(encoded_as_i, steps=1, verbose=0)))\n",
    "print(np.argmax(model.predict(encoded_as_i, steps=3, verbose=0),axis=1))\n",
    "print(np.sum(model.predict(encoded_as_i, steps=3, verbose=0)[0]))\n",
    "nextc = int_to_vocab[np.argmax(model.predict(encoded_as_i, steps=3, verbose=0),axis=1)[1]]\n",
    "print(nextc) # should give probabilities of next character\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50]\n",
      "jjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj\n"
     ]
    }
   ],
   "source": [
    "strout = \"\"\n",
    "data = Xtest\n",
    "#actual = \"\"\n",
    "predict = np.argmax(model.predict(data, steps=500, verbose=0),axis=1)\n",
    "print(predict)\n",
    "for i in predict:\n",
    "    \n",
    "    nextc = int_to_vocab[predict[i]]\n",
    "    \n",
    "    #actual += int_to_vocab[data[i]]\n",
    "    strout += nextc\n",
    "print(strout)\n",
    "#print(actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second attempt at testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best weights recorded from training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-19-1.168981.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" =)&\\ia^^^^PuEM1!Q=6^PuEMb! =^U^E1&,MPuE:=)&\\iaa^^^^e^=i0^dPqqi1iO0^\\!PO0=^0!^1!0&0i^Pu&Eia^^^^\\0=V^U \"\n",
      "7100\n",
      "length of:  100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_input to have shape (100, 1) but got array with shape (99, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-4b47a4af63e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2410\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    580\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    583\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_input to have shape (100, 1) but got array with shape (99, 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(datax)-1)\n",
    "pattern = []\n",
    "pattern = datax[start]\n",
    "print(\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_vocab[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_vocab[index]\n",
    "    seq_in = [int_to_vocab[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    np.hstack((pattern, [index])) # TODO make so length of pattern is 101, as it should be\n",
    "    length = len(pattern) \n",
    "    print(\"length of: \" ,len(pattern))\n",
    "    pattern = pattern[1:length + 1] # issue with the length. no matter what, length of pattern become 99\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [LSTM: A search space odyssey](https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
