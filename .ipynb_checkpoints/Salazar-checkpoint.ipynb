{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salazar\n",
    "*A Python-writting wizard*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Coding, while rewarding and essential to modernity, can be monotonous at times. However, with modern Neural Networks, the possibilities exist for a world where we can start the initial stages of a programming process of a software solution and then allow a machine to finish the work for us. This concept entails a future where programmers could eliminate the overhead of debugging and testing and allow them more time to focus on the planning stage of project management. \n",
    "\n",
    "Generative text has been around for some time now but generative coding is still a relatively new implementation of it’s paradigms. The difficulties with generative code could be akin to training a model to write stories with a dataset containing mostly Sci-Fi and then expecting that model to write a Nicholas Sparks’ novel. I.e. The problem here stems from the multitude of libraries as packages used to build upon programming languages to make them useful for specific tasks. Just because a model can produce C code doesn’t mean it can build an operating system. So what do we do if we want a swiss army knife for coding nearly every variation of code in a specific language? Well maybe we should use a method that employs a significant amount of data (string of code) in tandem with a method of effectively seeding the model. There are, thankfully, enough similarities between any two programs written in Python that some rule should be learnable by a network; combining that with the right amount of “starter code” should prove effective enough to get relatively useful outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "\"\"\"\n",
    "Numpy: matrix manipulation and math\n",
    "Pandas: csv parsing and various data structure tasks\n",
    "Mathpltlib.pyplot: data visualization\n",
    "set_trace: debug breaks\n",
    "keras: a machine learning library that is intuitive to read\n",
    "tensorflow: backend for keras... also the most widely used machine learning library\n",
    "re: regular expressions\n",
    "\"\"\"\n",
    "from copy import deepcopy as copy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sci\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "\n",
    "tf.config.optimizer.set_jit(True) # optimizes cpu usage\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate files as a singular string\n",
    "The bellow function uses a Python function knowns as ```walk``` to \"walk\" through a directory and read the files within that directory, storing them to the string ```content```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "concat_files\n",
    "----------\n",
    "\n",
    "Concatenate text files in a directory as a string\n",
    "\n",
    "dependent on 'os' Python module\n",
    "\n",
    "parameters\n",
    "----------\n",
    "directory: string; path of the target directory\n",
    "\n",
    "f_type:    tuple of strings; target file extentsions Ex: ('.py', '.cpp')\n",
    "\n",
    "return\n",
    "---------\n",
    "content:   concatenated string\n",
    "\n",
    "\"\"\"\n",
    "def concat_files(directory,f_type):\n",
    "    import os\n",
    "    # List all file in the dataset directory\n",
    "    # ------------------\n",
    "    all_file = []\n",
    "    content = \"\"\n",
    "\n",
    "    # walk through every directory and open every f_type file\n",
    "    # concatenate into var string \"content\"\n",
    "    for root, dirs, files in os.walk(directory): \n",
    "        for name in files:\n",
    "            if name.endswith(f_type): # we only care about .py\n",
    "                all_file.append(name)\n",
    "                with open(os.path.join(root,name), \"r\",encoding=\"utf8\") as f:\n",
    "                    content += f.read() + \"\\n\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = concat_files(\"dataset\",('.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a regular expression representation of ASCII characters\n",
    "This will useful in distiguishing the characters that are important in terms of writting Python code vs. characters that are exclusive to documentation such as emoji's and other non-latin characters. This will help to slim our data shape, increasing training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_ascii = \"[^\\x00-\\x7F]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "encode_string\n",
    "-----------\n",
    "Generate a dictionary representation of the characters found \n",
    "in a string keyed with integer representations\n",
    "\n",
    "Returns two dictionaries and an array. The two dictionaries are \n",
    "necessary to convert the string to integer representation\n",
    "and back again. The array is the string encoded as integer values.\n",
    "\n",
    "parameters\n",
    "----------\n",
    "content:      string; to be processed\n",
    "\n",
    "return\n",
    "----------\n",
    "vocab_to_int: dict; character to integer representation of unique characters in the string\n",
    "\n",
    "int_to_vocab: dict; integer to string representation\n",
    "\n",
    "encoded:      array; string encoded as integer values\n",
    "\"\"\"\n",
    "\n",
    "def encode_string(content):   \n",
    "    # Convert the string \"content\" into a list of intergers\n",
    "    # *** Uncomment the below lines if you haven't saved the encoded array\n",
    "    # Then rerun cell\n",
    "#   -------------------------------------------------\n",
    "#     ### creates a set of the individual characters\n",
    "#     vocab = set(content)\n",
    "#     ### attempt to clean out non-ascii characters\n",
    "#     vocab_c = copy(vocab)\n",
    "#     for i, char in enumerate(vocab_c):\n",
    "#         if re.search(r_all_ascii,char):\n",
    "#             vocab.remove(char)\n",
    "#     print(vocab)\n",
    "#     print(len(vocab))\n",
    "#     ### use the set to sequentially generate a dictionary\n",
    "#     vocab_to_int = {c: i for i, c in enumerate(vocab)} \n",
    "#     # print(vocab_to_int)\n",
    "#     ### make keys the numerical values\n",
    "#     int_to_vocab = dict(enumerate(vocab)) \n",
    "    \n",
    "#     ### encode the \"content\" string using dict\n",
    "#     ### encoded = np.array([vocab_to_int[c] for c in content], dtype=np.int32)\n",
    "    \n",
    "#     encoded = np.array([],dtype=np.int16)\n",
    "#     for c in content:\n",
    "#         if c in vocab_to_int:\n",
    "#             encoded = np.append(encoded,vocab_to_int[c])\n",
    "#   -------------------------------------------------\n",
    "\n",
    "\n",
    "# use the bellow lines if you want a dictionary of all basic ASCII charcters.\n",
    "# otherwise, comment out.\n",
    "#   -------------------------------------------------\n",
    "#     int_to_vocab = {i: chr(i) for i in range(127)}\n",
    "#     vocab_to_int = {chr(i): i for i in range(127)}\n",
    "\n",
    "#     encoded = np.array([],dtype=np.int16)\n",
    "#     for c in content:\n",
    "#         if c in vocab_to_int:\n",
    "#             encoded = np.append(encoded,vocab_to_int[c])    \n",
    "\n",
    "# Comment out if you are using this function for the first time \n",
    "# and do not have the required txt and json files\n",
    "#   -------------------------------------------------    \n",
    "    \n",
    "    infile1 = \"./encoded.txt\"\n",
    "    infile2 = \"./vocab_to_int.json\"\n",
    "    infile3 = \"./int_to_vocab.json\"\n",
    "    encoded = np.loadtxt(infile1, dtype=int) # comment out if above lines are uncommented\n",
    "    \n",
    "    with open(infile2, 'r') as fp:\n",
    "        vocab_to_int = json.load(fp)\n",
    "    \n",
    "    with open(infile3, 'r') as fp:\n",
    "        int_to_vocab = json.load(fp)\n",
    "#   --------------------------------------------------    \n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab, encoded = encode_string(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'c', '1': 'W', '2': '8', '3': '.', '4': 'r', '5': ';', '6': 's', '7': '\\t', '8': 'e', '9': ',', '10': 'Z', '11': '3', '12': '<', '13': '5', '14': 'u', '15': 'k', '16': 'G', '17': '&', '18': 'm', '19': '?', '20': 'n', '21': '^', '22': ')', '23': 'q', '24': 'l', '25': 'E', '26': 'x', '27': \"'\", '28': '@', '29': 'L', '30': 'P', '31': 'y', '32': 'I', '33': 'U', '34': 'M', '35': '{', '36': ':', '37': '6', '38': '0', '39': 'X', '40': '(', '41': 'F', '42': 'Q', '43': 'C', '44': '}', '45': '~', '46': '4', '47': '=', '48': '*', '49': '$', '50': ' ', '51': '1', '52': '/', '53': '+', '54': '7', '55': '\\n', '56': 'H', '57': 'A', '58': 'J', '59': '-', '60': '>', '61': 'a', '62': 'B', '63': 'd', '64': 'w', '65': '|', '66': 'f', '67': 'N', '68': '%', '69': 'z', '70': 'T', '71': 'b', '72': '2', '73': 'g', '74': '#', '75': '_', '76': 'O', '77': 'V', '78': 'o', '79': 'h', '80': '!', '81': ']', '82': '`', '83': 'i', '84': 'p', '85': 'R', '86': 'D', '87': '\"', '88': '[', '89': 'K', '90': 'v', '91': 'j', '92': 'S', '93': '\\\\', '94': '9', '95': 'Y', '96': 't'}\n",
      "[83 18 84 ... 22 55 55]\n"
     ]
    }
   ],
   "source": [
    "#print(content)\n",
    "print(int_to_vocab)\n",
    "# this is all of the files concatenated. with each character encoded using the int_to_vocab\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\rightarrow$ Save encoded array to avoid heavy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "outfile1 = \"./encoded.txt\"\n",
    "outfile2 = \"./vocab_to_int.json\"\n",
    "outfile3 = \"./int_to_vocab.json\"\n",
    "\n",
    "np.savetxt(outfile,encoded, fmt='%d')\n",
    "\n",
    "with open(outfile2, 'w') as fp:\n",
    "    json.dump(vocab_to_int, fp)\n",
    "\n",
    "with open(outfile3, 'w') as fp:\n",
    "    json.dump(int_to_vocab, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape data into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sequenc_gen\n",
    "---------------\n",
    "\n",
    "Partition an array of encoded characters into sequences.\n",
    "\n",
    "Parameters\n",
    "---------------\n",
    "encoded:         array of encoded characters; representation of a string\n",
    "vocab_to_int:    dictionary for conversion from character to integer\n",
    "int_to_vocab:    dictionary for conversion from integer to character\n",
    "\n",
    "Settings\n",
    "--------------\n",
    "sequence_length: Specify the desired length of the sequences\n",
    "\"\"\"\n",
    "def sequence_gen(encoded,vocab_to_int,int_to_vocab, **params):\n",
    "    global n_chars, n_vocab, n_patterns, datax, datay\n",
    "    n_chars = len(encoded)\n",
    "    n_vocab = len(vocab_to_int)\n",
    "    seq_len = params.pop(\"sequence_length\") # change from 50\n",
    "    datax = []\n",
    "    datay = []\n",
    "\n",
    "    # Loop through the encoded data and store \n",
    "    # sequences in datax and datay\n",
    "    for i in range(0, n_chars - seq_len, 1):\n",
    "        seq_in = encoded[i:i + seq_len] \n",
    "        seq_out = encoded[i + seq_len]\n",
    "        datax.append(seq_in)\n",
    "        datay.append(seq_out)\n",
    "    n_patterns = len(datax)\n",
    "    print(\"Total patterns: \", n_patterns)\n",
    "    print(\"Total unique characters: \", n_vocab)\n",
    "    print (\"\\\"\", ''.join([int_to_vocab[value] for value in datax[100]]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  1112800\n",
      "Total unique characters:  127\n",
      "\" ing bolzano\n",
      "\n",
      "    start = a\n",
      "    end = b\n",
      "    if function(a) == 0:  # one of the a or b is a root for t \"\n"
     ]
    }
   ],
   "source": [
    "sequence_gen(encoded,vocab_to_int, int_to_vocab, sequence_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape the sequences in a format that is better suited to LSTM units\n",
    "We will retain the pattern x sequence shape while adding a additonal dimension to represent the number of features; in this case one, since the each value can only represent one ASCII character. In terms of the LSTM, the sequence length will function as time steps for our LSTM units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# reshape datax -- > [n_patterns, time steps, features]\n",
    "X = np.reshape(datax, (n_patterns,seq_len,1))\n",
    "X = X / float(n_vocab)\n",
    "Y = np_utils.to_categorical(datay)\n",
    "#Y = np.asarray(datay) # for sparse categorical cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "Here are the primary componets of the model\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(Y.shape[1], activation='softmax')\n",
    "```\n",
    "Two layers in the model are commonly found in many other machine learning applications, so we will focus on the most detrimental layer(s) to the effectiveness of this model: the LSTM layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above depicts what is commonly found in LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "\n",
    "# optimizer = RMSprop(learning_rate=0.05)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizer,\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = \"best-weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',verbose=1, save_best_only=True, mode='min')\n",
    "callback_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.9179 - accuracy: 0.2642\n",
      "Epoch 00001: loss improved from 2.96475 to 2.91887, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.9189 - accuracy: 0.2640\n",
      "Epoch 2/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.8862 - accuracy: 0.2672\n",
      "Epoch 00002: loss improved from 2.91887 to 2.88659, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.8866 - accuracy: 0.2671\n",
      "Epoch 3/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.8552 - accuracy: 0.2701\n",
      "Epoch 00003: loss improved from 2.88659 to 2.85502, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.8550 - accuracy: 0.2700\n",
      "Epoch 4/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.8198 - accuracy: 0.2768\n",
      "Epoch 00004: loss improved from 2.85502 to 2.81978, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 2.8198 - accuracy: 0.2767\n",
      "Epoch 5/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.7828 - accuracy: 0.2830\n",
      "Epoch 00005: loss improved from 2.81978 to 2.78238, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.7824 - accuracy: 0.2832\n",
      "Epoch 6/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.7415 - accuracy: 0.2900\n",
      "Epoch 00006: loss improved from 2.78238 to 2.74183, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.7418 - accuracy: 0.2899\n",
      "Epoch 7/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.7003 - accuracy: 0.2982\n",
      "Epoch 00007: loss improved from 2.74183 to 2.69934, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.6993 - accuracy: 0.2984\n",
      "Epoch 8/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.6569 - accuracy: 0.3064\n",
      "Epoch 00008: loss improved from 2.69934 to 2.65714, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.6571 - accuracy: 0.3065\n",
      "Epoch 9/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.6153 - accuracy: 0.3153\n",
      "Epoch 00009: loss improved from 2.65714 to 2.61524, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.6152 - accuracy: 0.3153\n",
      "Epoch 10/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.5792 - accuracy: 0.3239\n",
      "Epoch 00010: loss improved from 2.61524 to 2.57876, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.5788 - accuracy: 0.3240\n",
      "Epoch 11/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.5392 - accuracy: 0.3322\n",
      "Epoch 00011: loss improved from 2.57876 to 2.53941, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.5394 - accuracy: 0.3321\n",
      "Epoch 12/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.5045 - accuracy: 0.3393\n",
      "Epoch 00012: loss improved from 2.53941 to 2.50414, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.5041 - accuracy: 0.3393\n",
      "Epoch 13/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.4653 - accuracy: 0.3482\n",
      "Epoch 00013: loss improved from 2.50414 to 2.46478, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.4648 - accuracy: 0.3483\n",
      "Epoch 14/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.4341 - accuracy: 0.3552\n",
      "Epoch 00014: loss improved from 2.46478 to 2.43433, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.4343 - accuracy: 0.3553\n",
      "Epoch 15/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.3957 - accuracy: 0.3627\n",
      "Epoch 00015: loss improved from 2.43433 to 2.39659, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.3966 - accuracy: 0.3625\n",
      "Epoch 16/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.3723 - accuracy: 0.3675\n",
      "Epoch 00016: loss improved from 2.39659 to 2.37179, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.3718 - accuracy: 0.3677\n",
      "Epoch 17/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.3377 - accuracy: 0.3769\n",
      "Epoch 00017: loss improved from 2.37179 to 2.33755, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.3376 - accuracy: 0.3771\n",
      "Epoch 18/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.3075 - accuracy: 0.3820\n",
      "Epoch 00018: loss improved from 2.33755 to 2.30754, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.3075 - accuracy: 0.3821\n",
      "Epoch 19/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.2830 - accuracy: 0.3886\n",
      "Epoch 00019: loss improved from 2.30754 to 2.28281, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.2828 - accuracy: 0.3886\n",
      "Epoch 20/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.2505 - accuracy: 0.3953\n",
      "Epoch 00020: loss improved from 2.28281 to 2.25034, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2503 - accuracy: 0.3954\n",
      "Epoch 21/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.2233 - accuracy: 0.4011\n",
      "Epoch 00021: loss improved from 2.25034 to 2.22321, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2232 - accuracy: 0.4012\n",
      "Epoch 22/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.1994 - accuracy: 0.4071\n",
      "Epoch 00022: loss improved from 2.22321 to 2.19970, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.1997 - accuracy: 0.4069\n",
      "Epoch 23/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.1695 - accuracy: 0.4131\n",
      "Epoch 00023: loss improved from 2.19970 to 2.17017, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.1702 - accuracy: 0.4130\n",
      "Epoch 24/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.1484 - accuracy: 0.4183\n",
      "Epoch 00024: loss improved from 2.17017 to 2.14821, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.1482 - accuracy: 0.4183\n",
      "Epoch 25/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.1243 - accuracy: 0.4245\n",
      "Epoch 00025: loss improved from 2.14821 to 2.12440, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.1244 - accuracy: 0.4243\n",
      "Epoch 26/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.0983 - accuracy: 0.4288\n",
      "Epoch 00026: loss improved from 2.12440 to 2.09866, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.0987 - accuracy: 0.4288\n",
      "Epoch 27/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.0719 - accuracy: 0.4348\n",
      "Epoch 00027: loss improved from 2.09866 to 2.07183, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.0718 - accuracy: 0.4348\n",
      "Epoch 28/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.0510 - accuracy: 0.4412\n",
      "Epoch 00028: loss improved from 2.07183 to 2.05086, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.0509 - accuracy: 0.4413\n",
      "Epoch 29/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.0322 - accuracy: 0.4438\n",
      "Epoch 00029: loss improved from 2.05086 to 2.03159, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.0316 - accuracy: 0.4440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 2.0038 - accuracy: 0.4511\n",
      "Epoch 00030: loss improved from 2.03159 to 2.00330, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.0033 - accuracy: 0.4512\n",
      "Epoch 31/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.9804 - accuracy: 0.4565\n",
      "Epoch 00031: loss improved from 2.00330 to 1.97982, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.9798 - accuracy: 0.4567\n",
      "Epoch 32/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.9571 - accuracy: 0.4640\n",
      "Epoch 00032: loss improved from 1.97982 to 1.95695, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.9570 - accuracy: 0.4639\n",
      "Epoch 33/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.9325 - accuracy: 0.4685\n",
      "Epoch 00033: loss improved from 1.95695 to 1.93213, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.9321 - accuracy: 0.4684\n",
      "Epoch 34/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.9161 - accuracy: 0.4720\n",
      "Epoch 00034: loss improved from 1.93213 to 1.91482, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.9148 - accuracy: 0.4724\n",
      "Epoch 35/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8916 - accuracy: 0.4805\n",
      "Epoch 00035: loss improved from 1.91482 to 1.89135, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.8914 - accuracy: 0.4805\n",
      "Epoch 36/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8723 - accuracy: 0.4836\n",
      "Epoch 00036: loss improved from 1.89135 to 1.87241, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.8724 - accuracy: 0.4835\n",
      "Epoch 37/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8516 - accuracy: 0.4884\n",
      "Epoch 00037: loss improved from 1.87241 to 1.85179, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.8518 - accuracy: 0.4884\n",
      "Epoch 38/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8372 - accuracy: 0.4919\n",
      "Epoch 00038: loss improved from 1.85179 to 1.83697, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.8370 - accuracy: 0.4919\n",
      "Epoch 39/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.8143 - accuracy: 0.4964\n",
      "Epoch 00039: loss improved from 1.83697 to 1.81377, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.8138 - accuracy: 0.4967\n",
      "Epoch 40/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7957 - accuracy: 0.5012\n",
      "Epoch 00040: loss improved from 1.81377 to 1.79523, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.7952 - accuracy: 0.5015\n",
      "Epoch 41/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7770 - accuracy: 0.5053\n",
      "Epoch 00041: loss improved from 1.79523 to 1.77573, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.7757 - accuracy: 0.5054\n",
      "Epoch 42/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7552 - accuracy: 0.5114\n",
      "Epoch 00042: loss improved from 1.77573 to 1.75545, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.7554 - accuracy: 0.5112\n",
      "Epoch 43/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7368 - accuracy: 0.5168\n",
      "Epoch 00043: loss improved from 1.75545 to 1.73716, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.7372 - accuracy: 0.5167\n",
      "Epoch 44/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7208 - accuracy: 0.5189\n",
      "Epoch 00044: loss improved from 1.73716 to 1.72133, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.7213 - accuracy: 0.5187\n",
      "Epoch 45/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.7113 - accuracy: 0.5230\n",
      "Epoch 00045: loss improved from 1.72133 to 1.71188, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.7119 - accuracy: 0.5228\n",
      "Epoch 46/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6846 - accuracy: 0.5294\n",
      "Epoch 00046: loss improved from 1.71188 to 1.68430, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.6843 - accuracy: 0.5296\n",
      "Epoch 47/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6746 - accuracy: 0.5298\n",
      "Epoch 00047: loss improved from 1.68430 to 1.67509, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.6751 - accuracy: 0.5298\n",
      "Epoch 48/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6554 - accuracy: 0.5364\n",
      "Epoch 00048: loss improved from 1.67509 to 1.65542, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.6554 - accuracy: 0.5364\n",
      "Epoch 49/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6346 - accuracy: 0.5402\n",
      "Epoch 00049: loss improved from 1.65542 to 1.63497, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.6350 - accuracy: 0.5399\n",
      "Epoch 50/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6222 - accuracy: 0.5442\n",
      "Epoch 00050: loss improved from 1.63497 to 1.62197, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.6220 - accuracy: 0.5443\n",
      "Epoch 51/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.6086 - accuracy: 0.5491\n",
      "Epoch 00051: loss improved from 1.62197 to 1.60825, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.6083 - accuracy: 0.5493\n",
      "Epoch 52/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5881 - accuracy: 0.5539\n",
      "Epoch 00052: loss improved from 1.60825 to 1.58772, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.5877 - accuracy: 0.5540\n",
      "Epoch 53/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5768 - accuracy: 0.5563\n",
      "Epoch 00053: loss improved from 1.58772 to 1.57736, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.5774 - accuracy: 0.5563\n",
      "Epoch 54/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5610 - accuracy: 0.5602\n",
      "Epoch 00054: loss improved from 1.57736 to 1.56037, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.5604 - accuracy: 0.5603\n",
      "Epoch 55/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5485 - accuracy: 0.5628\n",
      "Epoch 00055: loss improved from 1.56037 to 1.54836, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.5484 - accuracy: 0.5628\n",
      "Epoch 56/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5348 - accuracy: 0.5660\n",
      "Epoch 00056: loss improved from 1.54836 to 1.53472, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.5347 - accuracy: 0.5660\n",
      "Epoch 57/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5220 - accuracy: 0.5720\n",
      "Epoch 00057: loss improved from 1.53472 to 1.52188, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.5219 - accuracy: 0.5721\n",
      "Epoch 58/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.5045 - accuracy: 0.5723\n",
      "Epoch 00058: loss improved from 1.52188 to 1.50437, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.5044 - accuracy: 0.5724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4970 - accuracy: 0.5748\n",
      "Epoch 00059: loss improved from 1.50437 to 1.49669, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.4967 - accuracy: 0.5749\n",
      "Epoch 60/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4720 - accuracy: 0.5841\n",
      "Epoch 00060: loss improved from 1.49669 to 1.47254, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.4725 - accuracy: 0.5841\n",
      "Epoch 61/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4647 - accuracy: 0.5824\n",
      "Epoch 00061: loss improved from 1.47254 to 1.46522, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.4652 - accuracy: 0.5823\n",
      "Epoch 62/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4811 - accuracy: 0.5799\n",
      "Epoch 00062: loss did not improve from 1.46522\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.4825 - accuracy: 0.5796\n",
      "Epoch 63/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4657 - accuracy: 0.5827\n",
      "Epoch 00063: loss did not improve from 1.46522\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.4660 - accuracy: 0.5825\n",
      "Epoch 64/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4339 - accuracy: 0.5926\n",
      "Epoch 00064: loss improved from 1.46522 to 1.43351, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.4335 - accuracy: 0.5927\n",
      "Epoch 65/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4216 - accuracy: 0.5962\n",
      "Epoch 00065: loss improved from 1.43351 to 1.42197, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.4220 - accuracy: 0.5961\n",
      "Epoch 66/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4073 - accuracy: 0.5984\n",
      "Epoch 00066: loss improved from 1.42197 to 1.40707, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.4071 - accuracy: 0.5985\n",
      "Epoch 67/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.4014 - accuracy: 0.6010\n",
      "Epoch 00067: loss improved from 1.40707 to 1.40207, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.4021 - accuracy: 0.6008\n",
      "Epoch 68/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.3843 - accuracy: 0.6036\n",
      "Epoch 00068: loss improved from 1.40207 to 1.38436, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.3844 - accuracy: 0.6037\n",
      "Epoch 69/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.3800 - accuracy: 0.6059\n",
      "Epoch 00069: loss improved from 1.38436 to 1.38005, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.3801 - accuracy: 0.6058\n",
      "Epoch 70/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.3661 - accuracy: 0.6089\n",
      "Epoch 00070: loss improved from 1.38005 to 1.36660, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.3666 - accuracy: 0.6087\n",
      "Epoch 71/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.3536 - accuracy: 0.6143\n",
      "Epoch 00071: loss improved from 1.36660 to 1.35393, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.3539 - accuracy: 0.6141\n",
      "Epoch 72/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.3399 - accuracy: 0.6153\n",
      "Epoch 00072: loss improved from 1.35393 to 1.34045, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.3405 - accuracy: 0.6152\n",
      "Epoch 73/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.3340 - accuracy: 0.6178\n",
      "Epoch 00073: loss improved from 1.34045 to 1.33422, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.3342 - accuracy: 0.6179\n",
      "Epoch 74/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.3248 - accuracy: 0.6207\n",
      "Epoch 00074: loss improved from 1.33422 to 1.32511, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.3251 - accuracy: 0.6206\n",
      "Epoch 75/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.3156 - accuracy: 0.6235\n",
      "Epoch 00075: loss improved from 1.32511 to 1.31672, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.3167 - accuracy: 0.6233\n",
      "Epoch 76/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.3076 - accuracy: 0.6224\n",
      "Epoch 00076: loss improved from 1.31672 to 1.30800, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.3080 - accuracy: 0.6224\n",
      "Epoch 77/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2941 - accuracy: 0.6279\n",
      "Epoch 00077: loss improved from 1.30800 to 1.29508, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.2951 - accuracy: 0.6276\n",
      "Epoch 78/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2829 - accuracy: 0.6314\n",
      "Epoch 00078: loss improved from 1.29508 to 1.28264, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.2826 - accuracy: 0.6314\n",
      "Epoch 79/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2740 - accuracy: 0.6339\n",
      "Epoch 00079: loss improved from 1.28264 to 1.27376, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.2738 - accuracy: 0.6340\n",
      "Epoch 80/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2676 - accuracy: 0.6342\n",
      "Epoch 00080: loss improved from 1.27376 to 1.26747, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.2675 - accuracy: 0.6343\n",
      "Epoch 81/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2561 - accuracy: 0.6385\n",
      "Epoch 00081: loss improved from 1.26747 to 1.25622, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.2562 - accuracy: 0.6384\n",
      "Epoch 82/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2451 - accuracy: 0.6413\n",
      "Epoch 00082: loss improved from 1.25622 to 1.24524, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.2452 - accuracy: 0.6411\n",
      "Epoch 83/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2507 - accuracy: 0.6396\n",
      "Epoch 00083: loss did not improve from 1.24524\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.2510 - accuracy: 0.6395\n",
      "Epoch 84/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2428 - accuracy: 0.6408\n",
      "Epoch 00084: loss improved from 1.24524 to 1.24265, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.2427 - accuracy: 0.6409\n",
      "Epoch 85/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2229 - accuracy: 0.6461\n",
      "Epoch 00085: loss improved from 1.24265 to 1.22357, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.2236 - accuracy: 0.6460\n",
      "Epoch 86/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2169 - accuracy: 0.6489\n",
      "Epoch 00086: loss improved from 1.22357 to 1.21736, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.2174 - accuracy: 0.6486\n",
      "Epoch 87/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2087 - accuracy: 0.6504\n",
      "Epoch 00087: loss improved from 1.21736 to 1.20879, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.2088 - accuracy: 0.6504\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.2063 - accuracy: 0.6492\n",
      "Epoch 00088: loss improved from 1.20879 to 1.20608, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.2061 - accuracy: 0.6493\n",
      "Epoch 89/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.1895 - accuracy: 0.6549\n",
      "Epoch 00089: loss improved from 1.20608 to 1.18957, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.1896 - accuracy: 0.6549\n",
      "Epoch 90/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.1877 - accuracy: 0.6556\n",
      "Epoch 00090: loss improved from 1.18957 to 1.18788, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.1879 - accuracy: 0.6555\n",
      "Epoch 91/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.1717 - accuracy: 0.6598\n",
      "Epoch 00091: loss improved from 1.18788 to 1.17159, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.1716 - accuracy: 0.6598\n",
      "Epoch 92/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.1665 - accuracy: 0.6602\n",
      "Epoch 00092: loss improved from 1.17159 to 1.16665, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.1666 - accuracy: 0.6602\n",
      "Epoch 93/100\n",
      "59100/60000 [============================>.] - ETA: 0s - loss: 1.1620 - accuracy: 0.6625\n",
      "Epoch 00093: loss improved from 1.16665 to 1.16263, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 1.1626 - accuracy: 0.6625\n",
      "Epoch 94/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.1511 - accuracy: 0.6622\n",
      "Epoch 00094: loss improved from 1.16263 to 1.15154, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 1.1515 - accuracy: 0.6618\n",
      "Epoch 95/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.1555 - accuracy: 0.6639\n",
      "Epoch 00095: loss did not improve from 1.15154\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.1554 - accuracy: 0.6640\n",
      "Epoch 96/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.1502 - accuracy: 0.6636\n",
      "Epoch 00096: loss improved from 1.15154 to 1.15120, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.1512 - accuracy: 0.6635\n",
      "Epoch 97/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: 1.1349 - accuracy: 0.6685\n",
      "Epoch 00097: loss improved from 1.15120 to 1.13450, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.1345 - accuracy: 0.6687\n",
      "Epoch 98/100\n",
      "59400/60000 [============================>.] - ETA: 0s - loss: 1.1237 - accuracy: 0.6720\n",
      "Epoch 00098: loss improved from 1.13450 to 1.12426, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.1243 - accuracy: 0.6717\n",
      "Epoch 99/100\n",
      "59100/60000 [============================>.] - ETA: 0s - loss: 1.1183 - accuracy: 0.6728\n",
      "Epoch 00099: loss improved from 1.12426 to 1.12071, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.1207 - accuracy: 0.6721\n",
      "Epoch 100/100\n",
      "59400/60000 [============================>.] - ETA: 0s - loss: 1.1090 - accuracy: 0.6752\n",
      "Epoch 00100: loss improved from 1.12071 to 1.10877, saving model to best-weights.hdf5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.1088 - accuracy: 0.6751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f80df63088>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(inp, targets, steps_per_epoch=10, epochs=10)\n",
    "#model.get_weights().shape\n",
    "\n",
    "model.fit(X[:60000] , Y[:60000], epochs=100, batch_size=300, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 13s 211us/sample - loss: 3.7226 - accuracy: 0.3041\n",
      "[3.72255146261851, 0.3041]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# model.save('.\\model')\n",
    "score = model.evaluate(X[60000:120000], Y[60000:120000])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best weights recorded from training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename should reflect the name of the best weights available \n",
    "# in th local directory after training\n",
    "filename = \"best-weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" lection\n",
      "    for number in collection:\n",
      "        counting_arr[number - coll_min] += 1\n",
      "\n",
      "    # sum each p \"\n",
      "omuens that ao alcce teet the andrypine tesuenreeern crlcharcsesnsereser))\n",
      "                Co                         [ocrypted_d = 0\n",
      "            reyu = [[\n",
      "            rotut(\"\"\"  \"\n",
      "            cteruint__am] = iumut(\"Plert nnete tet se teet in ret bntereten keye andetetedede.\n",
      "\n",
      "        )\n",
      "\n",
      "    batert = \"       x, = lowar__t_dxisin(b,  b % b  %  * (*sxir()) lel(sey( - sel) %  0\n",
      "        print_n = andey(+ 1)\n",
      "        return False\n",
      "\n",
      "\n",
      "def ms_necellet(aryent_iceey, potat_sesu):\n",
      "    pet = 0\n",
      "    for = iumat(inp( - l_n1    2  * inn ruinte ko seteng:\n",
      "            reiuet = 0\n",
      "            deccy_xem = 0\n",
      "\n",
      "    ror i in range(len(sentete)):\n",
      "        if nomrt(ionc) : comceiitex,\n",
      "            denuinteted( = comuenit.\n",
      "            rumntnnndddm(= 0, comution\n",
      "        rf sow(x_wue(seluence, ==):\n",
      "        re boart[n][j] == 1:\n",
      "            return False\n",
      "    for i in range(len(board)):\n",
      "        if board[i][c]l]l] == 1:\n",
      "            return False\n",
      "    for i in range(len(bonrt)):\n",
      "        if coard[i][cel]l]l = 1:\n",
      "            retu\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(datax)-1)\n",
    "pattern = []\n",
    "pattern = datax[start]\n",
    "print(\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_vocab[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_vocab[index]\n",
    "    seq_in = [int_to_vocab[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern = np.append(pattern, [index], axis=0) # TODO make so length of pattern is 101, as it should be\n",
    "    length = pattern.shape[0] \n",
    "    pattern = pattern[1:length] # issue with the length. no matter what, length of pattern become 99\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [LSTM: A search space odyssey](https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
