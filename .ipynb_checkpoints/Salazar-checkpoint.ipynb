{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "\"\"\"\n",
    "Numpy: matrix manipulation and math\n",
    "Pandas: csv parsing and various data structure tasks\n",
    "Mathpltlib.pyplot: data visualization\n",
    "set_trace: debug breaks\n",
    "keras: a machine learning library that is intuitive to read\n",
    "tensorflow: backend for keras... also the most widely used machine learning library\n",
    "re: regular expressions\n",
    "\"\"\"\n",
    "from copy import deepcopy as copy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sci\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# not needed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "tf.config.optimizer.set_jit(True) # optimizes cpu usage\n",
    "\n",
    "import re\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "concat_files\n",
    "----------\n",
    "\n",
    "Concatenate text files in a directory as a string\n",
    "\n",
    "dependent on 'os' Python module\n",
    "\n",
    "parameters\n",
    "----------\n",
    "directory: string; path of the target directory\n",
    "\n",
    "f_type:    tuple of strings; target file extentsions Ex: ('.py', '.cpp')\n",
    "\n",
    "return\n",
    "---------\n",
    "content:   concatenated string\n",
    "\n",
    "\"\"\"\n",
    "def concat_files(directory,f_type):\n",
    "    import os\n",
    "    # List all file in the dataset directory\n",
    "    # ------------------\n",
    "    all_file = []\n",
    "    content = \"\"\n",
    "\n",
    "    # walk through every directory and open every f_type file\n",
    "    # concatenate into var string \"content\"\n",
    "    for root, dirs, files in os.walk(directory): \n",
    "        for name in files:\n",
    "            if name.endswith(f_type): # we only care about .py\n",
    "                all_file.append(name)\n",
    "                with open(os.path.join(root,name), \"r\") as f:\n",
    "                    content += f.read() + \"\\n\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = concat_files(\"dataset\",('.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_ascii = \"[^\\x00-\\x7F]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "encode_string\n",
    "-----------\n",
    "Generate a dictionary representation of the characters found \n",
    "in a string keyed with integer representations\n",
    "\n",
    "Returns two dictionaries and an array. The two dictionaries are \n",
    "necessary to convert the string to integer representation\n",
    "and back again. The array is the string encoded as integer values.\n",
    "\n",
    "parameters\n",
    "----------\n",
    "content:      string; to be processed\n",
    "\n",
    "return\n",
    "----------\n",
    "vocab_to_int: dict; character to integer representation of unique characters in the string\n",
    "\n",
    "int_to_vocab: dict; integer to string representation\n",
    "\n",
    "encoded:      array; string encoded as integer values\n",
    "\"\"\"\n",
    "\n",
    "def encode_string(content):   \n",
    "    # Convert the string \"content\" into a list of intergers\n",
    "    \n",
    "    ### creates a set of the individual characters\n",
    "    vocab = set(content)\n",
    "    ### attempt to clean out non-ascii characters\n",
    "    vocab_c = copy(vocab)\n",
    "    for i, char in enumerate(vocab_c):\n",
    "        if re.search(r_all_ascii,char):\n",
    "            vocab.remove(char)\n",
    "    print(vocab)\n",
    "    print(len(vocab))\n",
    "    ### use the set to sequentially generate a dictionary\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)} \n",
    "    print(vocab_to_int)\n",
    "    ### make keys the numerical values\n",
    "    int_to_vocab = dict(enumerate(vocab)) \n",
    "    \n",
    "    ### encode the \"content\" string using dict\n",
    "    ### encoded = np.array([vocab_to_int[c] for c in content], dtype=np.int32)\n",
    "    \n",
    "    # *** Uncomment the below lines if you haven't saved the encoded array\n",
    "    # Then rerun cell\n",
    "#   -------------------------------------------------\n",
    "#     encoded = np.array([],dtype=np.int16)\n",
    "#     for c in content:\n",
    "#         if c in vocab_to_int:\n",
    "#             encoded = np.append(encoded,vocab_to_int[c])\n",
    "#   -------------------------------------------------\n",
    "    encoded = np.load('./encoded.npy') # comment out if above lines are uncommented\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I', 'V', ':', '8', '^', 'm', 'a', 'h', 'L', 'Z', '-', 'l', 'i', 'W', 'k', 'q', '7', '|', ' ', '{', 'B', 'S', '*', 'C', ']', \"'\", '@', ';', 'v', '5', '>', 'F', '&', 'o', 'O', 'K', 'E', 's', 'p', '(', '\\t', '~', 'J', 'n', '!', '3', 'c', '$', 'y', '4', '\\n', '.', '`', ',', '=', '6', 'R', 'e', 'g', '\"', ')', 'w', '\\\\', 'U', 'j', '+', 'u', '9', 'G', 'X', 'N', 'f', '<', 'Y', 'T', '[', 'P', 't', '?', 'z', 'Q', 'H', 'A', '1', 'M', '_', '0', 'x', '%', 'D', 'b', '2', '#', 'd', '/', '}', 'r'}\n",
      "97\n",
      "{'I': 0, 'V': 1, ':': 2, '8': 3, '^': 4, 'm': 5, 'a': 6, 'h': 7, 'L': 8, 'Z': 9, '-': 10, 'l': 11, 'i': 12, 'W': 13, 'k': 14, 'q': 15, '7': 16, '|': 17, ' ': 18, '{': 19, 'B': 20, 'S': 21, '*': 22, 'C': 23, ']': 24, \"'\": 25, '@': 26, ';': 27, 'v': 28, '5': 29, '>': 30, 'F': 31, '&': 32, 'o': 33, 'O': 34, 'K': 35, 'E': 36, 's': 37, 'p': 38, '(': 39, '\\t': 40, '~': 41, 'J': 42, 'n': 43, '!': 44, '3': 45, 'c': 46, '$': 47, 'y': 48, '4': 49, '\\n': 50, '.': 51, '`': 52, ',': 53, '=': 54, '6': 55, 'R': 56, 'e': 57, 'g': 58, '\"': 59, ')': 60, 'w': 61, '\\\\': 62, 'U': 63, 'j': 64, '+': 65, 'u': 66, '9': 67, 'G': 68, 'X': 69, 'N': 70, 'f': 71, '<': 72, 'Y': 73, 'T': 74, '[': 75, 'P': 76, 't': 77, '?': 78, 'z': 79, 'Q': 80, 'H': 81, 'A': 82, '1': 83, 'M': 84, '_': 85, '0': 86, 'x': 87, '%': 88, 'D': 89, 'b': 90, '2': 91, '#': 92, 'd': 93, '/': 94, '}': 95, 'r': 96}\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab, encoded = encode_string(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\rightarrow$ Save encoded array to avoid heavy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tempfile import TemporaryFile as TF\n",
    "outfile = \"./encoded\"\n",
    "\n",
    "np.save(outfile,encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'I', 1: 'V', 2: ':', 3: '8', 4: '^', 5: 'm', 6: 'a', 7: 'h', 8: 'L', 9: 'Z', 10: '-', 11: 'l', 12: 'i', 13: 'W', 14: 'k', 15: 'q', 16: '7', 17: '|', 18: ' ', 19: '{', 20: 'B', 21: 'S', 22: '*', 23: 'C', 24: ']', 25: \"'\", 26: '@', 27: ';', 28: 'v', 29: '5', 30: '>', 31: 'F', 32: '&', 33: 'o', 34: 'O', 35: 'K', 36: 'E', 37: 's', 38: 'p', 39: '(', 40: '\\t', 41: '~', 42: 'J', 43: 'n', 44: '!', 45: '3', 46: 'c', 47: '$', 48: 'y', 49: '4', 50: '\\n', 51: '.', 52: '`', 53: ',', 54: '=', 55: '6', 56: 'R', 57: 'e', 58: 'g', 59: '\"', 60: ')', 61: 'w', 62: '\\\\', 63: 'U', 64: 'j', 65: '+', 66: 'u', 67: '9', 68: 'G', 69: 'X', 70: 'N', 71: 'f', 72: '<', 73: 'Y', 74: 'T', 75: '[', 76: 'P', 77: 't', 78: '?', 79: 'z', 80: 'Q', 81: 'H', 82: 'A', 83: '1', 84: 'M', 85: '_', 86: '0', 87: 'x', 88: '%', 89: 'D', 90: 'b', 91: '2', 92: '#', 93: 'd', 94: '/', 95: '}', 96: 'r'}\n",
      "[48 59 31 ... 55 56 56]\n"
     ]
    }
   ],
   "source": [
    "#print(content)\n",
    "print(int_to_vocab)\n",
    "# this is all of the files concatenated. with each character encoded using the int_to_vocab\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = [300,600,900,1200]\n",
    "# for i in range(500): \n",
    "#     segment.append((int(len(encoded) / 500) * (i + 1)))\n",
    "batch = {\n",
    "    \"x\" : [\n",
    "        encoded[:segment[0]],\n",
    "        encoded[segment[0]:segment[1]]\n",
    "    ],\n",
    "    \"y\" : [\n",
    "        encoded[1:segment[0]+1],\n",
    "        encoded[segment[0]+1:segment[1]+1]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 97)\n"
     ]
    }
   ],
   "source": [
    "# inp = np.vstack((batch['x'][0],batch['x'][1]))\n",
    "# targets = np.vstack((batch['y'][0],batch['y'][1]))\n",
    "inp = batch['x'][0]\n",
    "targets = batch['y'][0]\n",
    "Xtest = batch['x'][1]\n",
    "Ttest = batch['y'][1]\n",
    "io_size = len(int_to_vocab)\n",
    "\n",
    "# inp = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(inp,io_size)))\n",
    "# targets = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(targets,io_size)))\n",
    "# Xtest = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(Xtest,io_size)))\n",
    "# Ttest = np.hstack((np.ones((Xtest.shape[0],1)),tf.one_hot(Ttest,io_size)))\n",
    "\n",
    "inp = tf.one_hot(inp,io_size)\n",
    "targets = tf.one_hot(targets,io_size)\n",
    "Xtest = tf.one_hot(Xtest,io_size)\n",
    "Ttest = tf.one_hot(Ttest,io_size)\n",
    "\n",
    "# encoded_as_i = np.hstack((np.ones((1,1)),tf.one_hot(encoded[:1],io_size)))\n",
    "encoded_as_i = tf.one_hot(encoded[:40],io_size)\n",
    "\n",
    "#tf.print(Xtest, summarize=50)\n",
    "print(inp.shape)\n",
    "\n",
    "# inp = tf.expand_dims(inp,2)\n",
    "# targets = tf.expand_dims(targets,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = []\n",
    "# temp = []\n",
    "# for i, char in enumerate(encoded):\n",
    "#     if int_to_vocab[char] == '\\n':\n",
    "#         lines.append(temp)\n",
    "#     else:\n",
    "#         temp.append(char)\n",
    "#         temp = []\n",
    "\n",
    "# x = np.zeros((len(lines), maxlen, len(int_to_vocab)), dtype=np.bool)\n",
    "# y = np.zeros((len(sentences), len(int_to_vocab)), dtype=np.bool)\n",
    "# for i, sentence in enumerate(sentences):\n",
    "#     for t, char in enumerate(sentence):\n",
    "#         x[i, t, char_indices[char]] = 1\n",
    "#     y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bran/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "97/97 [==============================] - 93s 964ms/step - loss: 3.3607 - accuracy: 0.1746\n",
      "Epoch 2/5\n",
      "97/97 [==============================] - 92s 948ms/step - loss: 3.0469 - accuracy: 0.1955\n",
      "Epoch 3/5\n",
      "97/97 [==============================] - 93s 959ms/step - loss: 3.2144 - accuracy: 0.1953\n",
      "Epoch 4/5\n",
      "97/97 [==============================] - 93s 963ms/step - loss: 3.0073 - accuracy: 0.2015\n",
      "Epoch 5/5\n",
      "97/97 [==============================] - 93s 958ms/step - loss: 2.6702 - accuracy: 0.2258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb37e0fdd10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "\n",
    "# x_train = np.hstack((batch['x'][0],batch['x'][1]))\n",
    "# y_train = np.hstack((batch['y'][0],batch['y'][1]))\n",
    "# x_test = np.hstack((batch['x'][0],batch['x'][1]))\n",
    "# y_test = np.hstack((batch['y'][0],batch['y'][1]))\n",
    "\n",
    "max_features = 97\n",
    "#maxlen = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, output_dim=97))\n",
    "model.add(LSTM(97, input_shape=inp.shape))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(io_size, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(Xtest, Ttest, steps_per_epoch=97, epochs=5)\n",
    "#model.get_weights().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 26s 270ms/step\n",
      "[0.04106758304477967, 15.519999653100967]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model.save('./model')\n",
    "score = model.evaluate(inp, targets, steps=97)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(40, 97), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4.0382799e-02 3.6107301e-07 2.2326464e-07 ... 2.5392863e-05\n",
      "  2.9392481e-07 2.9682593e-07]\n",
      " [3.0115703e-02 9.5992255e-07 5.8833479e-07 ... 3.8130096e-05\n",
      "  7.7778634e-07 7.9772832e-07]\n",
      " [4.4122519e-04 1.1850213e-07 7.9293002e-08 ... 4.3012276e-02\n",
      "  1.0135348e-07 1.2478394e-07]\n",
      " ...\n",
      " [1.2823937e-03 1.7435547e-06 1.2205473e-06 ... 4.7490899e-05\n",
      "  1.3142378e-06 1.4809921e-06]\n",
      " [1.5982764e-02 4.5820761e-06 2.8151956e-06 ... 7.4550655e-05\n",
      "  3.8831436e-06 4.0619348e-06]\n",
      " [3.8662888e-02 4.1631239e-07 2.5747269e-07 ... 2.6746873e-05\n",
      "  3.3779594e-07 3.4244664e-07]], shape=(120, 97), dtype=float32)\n",
      "[ 0  8  8 34  8  8  0  0 34  8  8  8 34 34 34  8 34  8 34 34  0  0  0  1\n",
      "  1 34  8  8  8  9  8  8 34  8  8  8  8  8  8  8  8 34  8 34 34  8 34  8\n",
      " 34  8 34 13 34  8 11 34  8  8 34  8  8 34  8  8  8  8 29 34 37 34 34 34\n",
      " 14 34  8  8 34  8 34 34 34  7  8 34  8 34  8  8 34  8  8 34  8  8  3 34\n",
      " 34]\n",
      "0.99999994\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(tf.keras.backend.shape(Xtest),tf.keras.backend.shape(model.weights))\n",
    "# sci.softmax(Xtest @ encoded_to_i)\n",
    "print(encoded_as_i)\n",
    "print(tf.convert_to_tensor(model.predict(encoded_as_i, steps=3, verbose=0)))\n",
    "print(np.argmax(model.predict(encoded_as_i, steps=3, verbose=0),axis=0))\n",
    "print(np.sum(model.predict(encoded_as_i, steps=3, verbose=0)[0]))\n",
    "nextc = int_to_vocab[np.argmax(model.predict(encoded_as_i, steps=3, verbose=0),axis=0)[1]]\n",
    "print(nextc) # should give probabilities of next character\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30  24  30  30  30  30  61   0  30  30  61   1  30  30  30  30   8  14\n",
      "  34  30  30  30  30   0   1  42  14  42  33   9  30   3  30  30   2   9\n",
      "   8   0  30  30  30  30  30 217  87  87  30   2  30  30 205  30  30  20\n",
      "  43 122   5  14  30  30  30  30  30 121   9 281  61 274  30 280  30  30\n",
      "  30 121  30  64  30  30  30   6  30  30   0  30  31  30  30  13  30   8\n",
      " 156  61  30 274  30  30  41]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 217 is out of bounds for axis 0 with size 97",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-8f41bb083955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnextc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#actual += int_to_vocab[data[i]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 217 is out of bounds for axis 0 with size 97"
     ]
    }
   ],
   "source": [
    "strout = \"\"\n",
    "data = Xtest\n",
    "#actual = \"\"\n",
    "predict = np.argmax(model.predict(data, steps=97, verbose=0),axis=0)\n",
    "print(predict)\n",
    "for i in predict:\n",
    "    \n",
    "    nextc = int_to_vocab[predict[i]]\n",
    "    \n",
    "    #actual += int_to_vocab[data[i]]\n",
    "    strout += nextc\n",
    "print(strout)\n",
    "#print(actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [LSTM: A search space odyssey](https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
