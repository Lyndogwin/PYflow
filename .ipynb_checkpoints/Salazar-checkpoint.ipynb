{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salazar\n",
    "*A Python-writting wizard*\n",
    "\n",
    "# I. Introduction\n",
    "\n",
    "Coding, while rewarding and essential to modernity, can be monotonous at times. However, with modern Neural Networks, the possibilities exist for a world where we can start the initial stages of a programming process of a software solution and then allow a machine to finish the work for us. This concept entails a future where programmers could eliminate the overhead of debugging and testing and allow them more time to focus on the planning stage of project management. \n",
    "\n",
    "Generative text has been around for some time now but generative coding is still a relatively new implementation of it’s paradigms. The difficulties with generative code could be akin to training a model to write stories with a dataset containing mostly Sci-Fi and then expecting that model to write a Nicholas Sparks’ novel. I.e. The problem here stems from the multitude of libraries as packages used to build upon programming languages to make them useful for specific tasks. Just because a model can produce C code doesn’t mean it can build an operating system. So what do we do if we want a swiss army knife for coding nearly every variation of code in a specific language? Well maybe we should use a method that employs a significant amount of data (string of code) in tandem with a method of effectively seeding the model. There are, thankfully, enough similarities between any two programs written in Python that some rule should be learnable by a network; combining that with the right amount of “starter code” should prove effective enough to get relatively useful outputs.\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal is simple, albeit ambitious: develop a preprocessing method and LSTM model capable of generating source code from a supplied seed.\n",
    "\n",
    "### Possible expansions in the future\n",
    "\n",
    "Use an autencoder to generate a source code seed from English, from which the LSTM model can generate trailing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Background\n",
    "In the research paper \"A deep language model for software code\" researchers Hoa Khanh Dam, Truyen Tran, and Trang Pham; the minds behind Deepsoft cover their efforts in determining the validity of LSTMs in learning Java code [[1]](#References). Their argument for pursuing this model is based on the failure of models such as **n-grams** in capturing long context where dependent code elements scatter far apart. Dam and other also suggested that the unique capability of LSTMs in capturing \"useful\" patterns across a specified time step. In their research, the LSTM model performed well in generation Java code, so we decided this would be a good place to begin constructing our own model.\n",
    "\n",
    "[To references](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Method\n",
    "**Todo: add a breif decription to each list item**\n",
    "### 1. Concatenate and Encode data\n",
    "### 2. Reshape into sequneces\n",
    "### 3. Training \n",
    "### 4. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "\"\"\"\n",
    "Numpy:             matrix manipulation and math\n",
    "Pandas:            csv parsing and various data structure tasks\n",
    "Mathpltlib.pyplot: data visualization\n",
    "set_trace:         debug breaks\n",
    "keras:             a machine learning library that is intuitive to read\n",
    "tensorflow:        backend for keras; also the most widely used machine learning library\n",
    "re:                regular expressions\n",
    "pickle:            save objects to a file\n",
    "\"\"\"\n",
    "from copy import deepcopy as copy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sci\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "tf.config.optimizer.set_jit(True) # optimizes cpu usage\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate files as a singular string\n",
    "The bellow function uses a Python function knowns as ```walk``` to \"walk\" through a directory and read the files within that directory, storing them to the string ```content```. For our purposes, we have pulled from a github repository consisting of a large amount of Python code into a directory called `dataset` \n",
    "\n",
    "Currently using [this repository](https://github.com/TheAlgorithms/Python) as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "concat_files\n",
    "----------\n",
    "\n",
    "Concatenate text files in a directory as a string\n",
    "\n",
    "dependent on 'os' Python module\n",
    "\n",
    "parameters\n",
    "----------\n",
    "directory: string; path of the target directory\n",
    "\n",
    "f_type:    tuple of strings; target file extentsions Ex: ('.py', '.cpp')\n",
    "\n",
    "return\n",
    "---------\n",
    "content:   concatenated string\n",
    "\n",
    "\"\"\"\n",
    "def concat_files(directory,f_type,excluded = \"\"):\n",
    "    import os\n",
    "    # List all file in the dataset directory\n",
    "    # ------------------\n",
    "    all_file = []\n",
    "    content = \"\"\n",
    "\n",
    "    # walk through every directory and open every f_type file\n",
    "    # concatenate into var string \"content\"\n",
    "    for root, dirs, files in os.walk(directory): \n",
    "        for name in files:\n",
    "            if name.endswith(f_type) and not name.find(excluded): # we only care about .py\n",
    "                all_file.append(name)\n",
    "                with open(os.path.join(root,name), \"r\",encoding=\"utf8\") as f:\n",
    "                    content += f.read() + \"\\n\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "\n",
      "\n",
      "def bisection(\n",
      "    function, a, b\n",
      "):  # finds where the function becomes 0 in [a,b] us\n"
     ]
    }
   ],
   "source": [
    "content = concat_files(\"dataset\",('.py'))\n",
    "print(content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a regular expression representation of ASCII characters\n",
    "This will useful in distiguishing the characters that are important in terms of writting Python code vs. characters that are exclusive to documentation such as emoji's and other non-latin characters. This will help to slim our data shape, increasing training speed.\n",
    "\n",
    "We learned later that there is a builtin python function, `chr()`, that can handle this with a bit more ease and is included in the final version of the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_ascii = \"[^\\x00-\\x7F]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellow are two fuctions with the same name\n",
    "[**The first**](#First-preprocessing-option) is for if you are *introducing new data to the dataset*, i.e. pulling new repositories into the \"dataset\" directory. This will perform the *encoding step*, converting every character in the concatenated string into an integer to be stored in a one dimensional array. \n",
    "\n",
    "[**The second**](#Second-preprocessing-option) is intended for loading an already encoded string and storing it into variable we will use going forward. This will save a signifigant amount of time, as it takes a while to encode a large string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First preprocessing option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "encode_string\n",
    "-----------\n",
    "Generate a dictionary representation of the characters found \n",
    "in a string keyed with integer representations\n",
    "\n",
    "Returns two dictionaries and an array. The two dictionaries are \n",
    "necessary to convert the string to integer representation\n",
    "and back again. The array is the string encoded as integer values.\n",
    "\n",
    "parameters\n",
    "----------\n",
    "content:      string; to be processed\n",
    "\n",
    "return\n",
    "----------\n",
    "vocab_to_int: dict; character to integer representation of unique characters in the string\n",
    "\n",
    "int_to_vocab: dict; integer to string representation\n",
    "\n",
    "encoded:      array; string encoded as integer values\n",
    "\"\"\"\n",
    "\n",
    "def pre_proc(content):   \n",
    "    # Convert the string \"content\" into a list of intergers\n",
    "#   -------------------------------------------------\n",
    "#     ### creates a set of the individual characters\n",
    "#     vocab = set(content)\n",
    "#     ### attempt to clean out non-ascii characters\n",
    "#     vocab_c = copy(vocab)\n",
    "#     for i, char in enumerate(vocab_c):\n",
    "#         if re.search(r_all_ascii,char):\n",
    "#             vocab.remove(char)\n",
    "#     print(vocab)\n",
    "#     print(len(vocab))\n",
    "#     ### use the set to sequentially generate a dictionary\n",
    "#     vocab_to_int = {c: i for i, c in enumerate(vocab)} \n",
    "#     # print(vocab_to_int)\n",
    "#     ### make keys the numerical values\n",
    "#     int_to_vocab = dict(enumerate(vocab)) \n",
    "    \n",
    "#     ### encode the \"content\" string using dict\n",
    "#     ### encoded = np.array([vocab_to_int[c] for c in content], dtype=np.int32)\n",
    "    \n",
    "#     encoded = np.array([],dtype=np.int16)\n",
    "#     for c in content:\n",
    "#         if c in vocab_to_int:\n",
    "#             encoded = np.append(encoded,vocab_to_int[c])\n",
    "#   -------------------------------------------------\n",
    "\n",
    "\n",
    "# use the bellow lines if you want a dictionary of all basic ASCII charcters.\n",
    "# otherwise, comment out.\n",
    "#   -------------------------------------------------\n",
    "    comment = False\n",
    "    quote_total = 0\n",
    "    quote_comment = False\n",
    "    string = False\n",
    "    ap_string = False\n",
    "    \n",
    "    int_to_vocab = {i: chr(i) for i in range(127)}\n",
    "    vocab_to_int = {chr(i): i for i in range(127)}\n",
    "\n",
    "    encoded = np.array([],dtype=np.int16)\n",
    "    for c in content:\n",
    "        if c in vocab_to_int:\n",
    "            if c == '\"' and not (comment or ap_string):\n",
    "                quote_total += 1\n",
    "                if quote_total == 3:\n",
    "                    quote_comment = not quote_comment\n",
    "                if (quote_total == 1) and (not quote_comment):\n",
    "                    string = not string\n",
    "            elif c == \"'\" and not (comment or quote_comment or string):\n",
    "                ap_string = not ap_string\n",
    "            elif c == '#' and not (comment or quote_comment or string or ap_string):\n",
    "                comment = True\n",
    "            elif comment and c == '\\n':\n",
    "                comment = False\n",
    "            if (not comment) and (not quote_comment):\n",
    "                encoded = np.append(encoded,vocab_to_int[c]) \n",
    "            if c != '\"':\n",
    "                quote_total = 0\n",
    "#             encoded = np.append(encoded,vocab_to_int[c])\n",
    "                \n",
    "                \n",
    "    return vocab_to_int, int_to_vocab, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second preprocessing option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if you want to use data that is alread preprocessed \n",
    "def pre_proc(content):    \n",
    "    import pickle\n",
    "    \n",
    "    infile1 = \"./encodings/encoded2.txt\"        # path to encoded string\n",
    "    infile2 = \"./encodings/vocab_to_int\"\n",
    "    infile3 = \"./encodings/int_to_vocab\"\n",
    "    \n",
    "    encoded = np.loadtxt(infile1, dtype=int) # load as an array of integers\n",
    "    \n",
    "#     with open(infile2, 'r') as fp:\n",
    "#         vocab_to_int = pickle.load(fp)\n",
    "    \n",
    "#     with open(infile3, 'r') as fp:\n",
    "#         int_to_vocab = pickle.load(fp)\n",
    "        \n",
    "    int_to_vocab = {i: chr(i) for i in range(127)}\n",
    "    vocab_to_int = {chr(i): i for i in range(127)}\n",
    "#   --------------------------------------------------    \n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the preprocessing funtion\n",
    "If you run the next cell, you will see the encoded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \n",
      "for i in range(5):\n",
      "   print(\"#this is source code#\")\n",
      "\"\"\"\n",
      "for i in range(5):\n",
      "   print(\"this is more source code\") \"\n"
     ]
    }
   ],
   "source": [
    "string = '# this is a test function \\nfor i in range(5):\\n   print(\"#this is source code#\")\\n\"\"\"\\n   this is a \\n   comment\\n\"\"\"\\nfor i in range(5):\\n   print(\"this is more source code\")'\n",
    "vocab_to_int, int_to_vocab, test_encoded = pre_proc(string)\n",
    "print (\"\\\"\", ''.join([int_to_vocab[value] for value in test_encoded]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab, encoded = pre_proc(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~'}\n",
      "\n",
      "Encoded string: [105 109 112 ...  41  10  10]\n"
     ]
    }
   ],
   "source": [
    "#print(content)\n",
    "print(int_to_vocab)\n",
    "# this is all of the files concatenated. with each character encoded using the int_to_vocab\n",
    "print()\n",
    "print(\"Encoded string:\",encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\rightarrow$ Save encoded array to avoid heavy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile1 = \"./encodings/encoded2.txt\"\n",
    "outfile2 = \"./encodings/vocab_to_int\"\n",
    "outfile3 = \"./encodings/int_to_vocab\"\n",
    "\n",
    "np.savetxt(outfile1,encoded, fmt='%d')\n",
    "\n",
    "with open(outfile2, 'wb') as fp:\n",
    "    pickle.dump(vocab_to_int, fp)\n",
    "\n",
    "with open(outfile3, 'wb') as fp:\n",
    "    pickle.dump(int_to_vocab, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape data into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sequenc_gen\n",
    "---------------\n",
    "\n",
    "Partition an array of encoded characters into sequences.\n",
    "\n",
    "Parameters\n",
    "---------------\n",
    "encoded:         array of encoded characters; representation of a string\n",
    "vocab_to_int:    dictionary for conversion from character to integer\n",
    "int_to_vocab:    dictionary for conversion from integer to character\n",
    "\n",
    "Settings\n",
    "--------------\n",
    "sequence_length: Specify the desired length of the sequences\n",
    "\"\"\"\n",
    "class Sequencer:\n",
    "    def __init__(self,seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        pass\n",
    "    \n",
    "    def sequence_gen(self, encoded, vocab_to_int, int_to_vocab):\n",
    "        #return n_chars, n_vocab, n_patterns, datax, datay\n",
    "        self.n_chars = len(encoded)\n",
    "        self.n_vocab = len(vocab_to_int)\n",
    "        self.datax = []\n",
    "        self.datay = []\n",
    "\n",
    "        # Loop through the encoded data and store \n",
    "        # sequences in datax and datay\n",
    "        for i in range(0, self.n_chars - self.seq_len, 1):\n",
    "            seq_in = encoded[i:i + self.seq_len] \n",
    "            seq_out = encoded[i + self.seq_len]\n",
    "            self.datax.append(seq_in)\n",
    "            self.datay.append(seq_out)\n",
    "        self.n_patterns = len(self.datax)\n",
    "        print(\"Total patterns: \", self.n_patterns)\n",
    "        print(\"Total unique characters: \", self.n_vocab)\n",
    "        print (\"\\\"\", ''.join([int_to_vocab[value] for value in self.datax[100]]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  1022743\n",
      "Total unique characters:  127\n",
      "\" = 0:  \n",
      "        return a\n",
      "    elif function(b) == 0:\n",
      "        return b\n",
      "    elif (\n",
      "        function(a) * \"\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "sequencer = Sequencer(100)\n",
    "sequencer.sequence_gen(encoded,vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape the sequences in a format that is better suited to LSTM cells\n",
    "We will retain the pattern x sequence shape while adding a additonal dimension to represent the number of features; in this case one, since the each value can only represent one ASCII character. In terms of the LSTM, the sequence length will function as time steps for our LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(seq): \n",
    "    global X, Y\n",
    "    from keras.utils import np_utils\n",
    "\n",
    "    # reshape datax -- > [n_patterns, time steps, features]\n",
    "    X = np.reshape(seq.datax, (seq.n_patterns,seq.seq_len,1))\n",
    "    X = X / float(seq.n_vocab)\n",
    "    Y = np_utils.to_categorical(seq.datay)\n",
    "    #Y = np.asarray(datay) # for sparse categorical cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_data(sequencer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "Here are the primary componets of the model built with Keras:\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(Y.shape[1], activation='softmax')\n",
    "```\n",
    "Two layers in the model are commonly found in many other machine learning applications, so we will focus on the most detrimental layer(s) to the effectiveness of this model: the LSTM layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](The_LSTM_cell.png)\n",
    "*From wikipedia*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above depicts what is commonly found in **LSTM cells**. A typical LSTM has an additional input over vanilla RNNs known as the **cell-state vector**. This vector along with the **hidden-state vector** and **input data** allow the LSTM cell to \"remember\" or \"forget\" certain sequences [[1]](#References).\n",
    "\n",
    "There are 3 primary gates within a cell that utilize the sigmoid function:\n",
    "1. input gate $\\rightarrow$ controls whether the memory cell is updated; contributing to the cell-state\n",
    "2. forget gate $\\rightarrow$ controls if the memory cell is reset to zero; also contributing to the cell-state\n",
    "3. oupute gate $\\rightarrow$ controls if the information of the current cell state is made visable; directly contributing to the hidden-state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: breifly explain compilation choice**\n",
    "Because we are interested in generating source code based on a supplied seed, we want to predict the next character in a string based on the probably the next character being a specific choice in from our vocab, or dictonary of possible ASCII characters. Hence, the loss function we chose for our model is **categorical crossentropy**.\n",
    "```python\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "We tried serveral different optimization options, but **adam** seemed to perform the best as far as producing a more accurate model. It also seems to the be the most commonly used optimizer in terms of text generation, as its adatptive learning rate excels in many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model\n",
    "**TODO: maybe try different nn layers** More research needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def model_compile(**params):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "    from tensorflow.keras.layers import Embedding\n",
    "    from tensorflow.keras.layers import LSTM\n",
    "    from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "    global model\n",
    "    \n",
    "    dropout = params.pop(\"dropout\",0.5)\n",
    "    lstm_size = params.pop(\"lstm_size\",128)\n",
    "    secondL_scalar = params.pop(\"second_layer_scalar\",1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_size, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(int(lstm_size * secondL_scalar),recurrent_dropout = dropout))\n",
    "    #model.add(Dropout(dropout))\n",
    "    model.add(Dense(Y.shape[1], activation='softmax')) # output should be probabilities of character options\n",
    "\n",
    "    #---------------------------------------\n",
    "    # optimizer = RMSprop(learning_rate=0.05)\n",
    "\n",
    "    # model.compile(loss='categorical_crossentropy',\n",
    "    #               optimizer=optimizer,\n",
    "    #               metrics=['accuracy'])\n",
    "    #---------------------------------------\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving trained weights\n",
    "Because this model takes a significant amount of time to train (about 10 minutes at 100 epochs on RTX 2070 super), we decided to save the weights that produce the lowest loss. The cell bellow will utillize keras' callback feature to save the current weights on each epoch that returns the new, lowest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "    \n",
    "modelpath = \"./models/best-weights.hdf5\"\n",
    "seqpath = \"./models/sequencer\"\n",
    "\n",
    "# modelpath = \"dummypath\"\n",
    "# seqpath = \"dummypath2\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(modelpath, monitor='loss',verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "class SaveBest(Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "       Format is based on tensorflow documentation \n",
    "    Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq, lowest_loss, patience=0):\n",
    "        super(SaveBest, self).__init__()\n",
    "\n",
    "        self.patience = patience\n",
    "        self.seq = seq\n",
    "        self.lowest_loss = lowest_loss\n",
    "\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global best_seq, lowest_loss\n",
    "        \n",
    "        current = logs.get('loss')\n",
    "        if np.less(current, self.lowest_loss):\n",
    "            best_seq = self.seq ## Need to determine best sequence length\n",
    "            self.best = current\n",
    "            self.lowest_loss = current\n",
    "            lowest_loss = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.model.save(modelpath)\n",
    "\n",
    "            with open(seqpath, 'wb') as fp:\n",
    "                pickle.dump(best_seq, fp)\n",
    "            \n",
    "            print(\"\\nnew best weights found\")\n",
    "#         else:\n",
    "#             self.wait += 1\n",
    "#             if self.wait >= self.patience:\n",
    "# #                 self.stopped_epoch = epoch\n",
    "#                 self.model.stop_training = True\n",
    "#                 print('Shifting to next parameter')\n",
    "# #                 self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_seq = np.Inf\n",
    "lowest_loss = np.Inf\n",
    "def parameter_search():\n",
    "    sequence_lens = [50]\n",
    "    dropouts = [0.4,0.5]\n",
    "    lstm_sizes = [128]\n",
    "    lstm_scalars = [1]\n",
    "    epochs = [200] \n",
    "    for i,v in enumerate(lstm_sizes):\n",
    "        for j,h in enumerate(lstm_scalars):\n",
    "            for k,r in enumerate(dropouts):\n",
    "                for p,e in enumerate(epochs):\n",
    "                    for z,l in enumerate(sequence_lens):\n",
    "                        sequencer = Sequencer(l)\n",
    "                        sequencer.sequence_gen(encoded,vocab_to_int, int_to_vocab)\n",
    "                        reshape_data(sequencer)\n",
    "                        model_compile(dropout=r,lstm_size=v,second_layer_scalar=h)\n",
    "                        print(\"fitting model with parameters:\")\n",
    "                        print(\"lstm size:\",v)\n",
    "                        print(\"lstm scalar:\",h)\n",
    "                        print(\"dropout:\",r)\n",
    "                        print(\"sequence length:\",l)\n",
    "                        try:\n",
    "                            model.fit(X[:200000] , Y[:200000], epochs=e, batch_size=500, callbacks=[SaveBest(sequencer,lowest_loss,15)])\n",
    "                        except:\n",
    "                            # TODO: write to error log\n",
    "                            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "**TODO: explain training method once we get the best parameters**\n",
    "\n",
    "We decided to take a parameter searching approach, as not much research is available to draw from when building a model that can write source code. So, based on text generation research, we started with a few basics parameters and kept the network somewhat shallow in order to stay within the RTX 2070's 8 GB of VRAM. We stepped away from **early stopping**, as was implemented by **Dam** and others [[2]](#References). we decided to compare all parameters with others immediatly by cross validating based on a global loss variable that stores the current loss value whenever a new best loss is discovered during training. Also, once a new-best-loss was discoverd, we saved both the model and the sequencer object to a separate files in order to use freely after training was complete.\n",
    "\n",
    "### Parameter searching\n",
    "Here we wrote a function to test several parameters. This function ran over the course of 3 days and proved to be sucessful in finding the best network configuration. Some of the values picked for the validation process were derived from Dam and others work, most noteably the mention that shorter sequence length seem to prevail in the case of generative code [[2]](#References).\n",
    "\n",
    "- LSTM layer size $\\rightarrow$ number of LSTM cells in the layer\n",
    "- LSTM scalar $\\rightarrow$ size of second LSTM layer based on the size of the first.\n",
    "- Sequence length $\\rightarrow$ number of characters to pass as input to the model\n",
    "- Dropout rate (layer) $\\rightarrow$ percentage of weights that are randomly ignored; combats overfitting\n",
    "- Epochs $\\rightarrow$ number of time data is passed into the model during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  665158\n",
      "Total unique characters:  127\n",
      "\" = 0:  \n",
      "        return a\n",
      "    elif function(b) == 0: \"\n",
      "fitting model with parameters:\n",
      "lstm size: 128\n",
      "lstm scalar: 1\n",
      "dropout: 0.4\n",
      "sequence length: 50\n",
      "Train on 200000 samples\n",
      "Epoch 1/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 3.1255 - accuracy: 0.2879\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 55s 276us/sample - loss: 3.1251 - accuracy: 0.2880\n",
      "Epoch 2/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.8588 - accuracy: 0.3007\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 244us/sample - loss: 2.8585 - accuracy: 0.3008\n",
      "Epoch 3/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.7577 - accuracy: 0.3106\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 2.7577 - accuracy: 0.3106\n",
      "Epoch 4/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.6950 - accuracy: 0.3172\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 2.6948 - accuracy: 0.3172\n",
      "Epoch 5/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.6352 - accuracy: 0.3256\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 244us/sample - loss: 2.6349 - accuracy: 0.3257\n",
      "Epoch 6/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.5760 - accuracy: 0.3340\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 249us/sample - loss: 2.5758 - accuracy: 0.3341\n",
      "Epoch 7/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.5123 - accuracy: 0.3469\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 2.5124 - accuracy: 0.3468\n",
      "Epoch 8/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.4436 - accuracy: 0.3627\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 2.4439 - accuracy: 0.3627\n",
      "Epoch 9/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.3832 - accuracy: 0.3772\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 2.3836 - accuracy: 0.3771\n",
      "Epoch 10/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.3299 - accuracy: 0.3896\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 2.3299 - accuracy: 0.3896\n",
      "Epoch 11/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.2849 - accuracy: 0.4001\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 240us/sample - loss: 2.2851 - accuracy: 0.4001\n",
      "Epoch 12/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.2442 - accuracy: 0.4093\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 2.2443 - accuracy: 0.4093\n",
      "Epoch 13/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.2085 - accuracy: 0.4167\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 2.2085 - accuracy: 0.4167\n",
      "Epoch 14/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.1757 - accuracy: 0.4245\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 2.1756 - accuracy: 0.4245\n",
      "Epoch 15/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.1465 - accuracy: 0.4308\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 242us/sample - loss: 2.1462 - accuracy: 0.4309\n",
      "Epoch 16/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.1188 - accuracy: 0.4374\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 2.1187 - accuracy: 0.4374\n",
      "Epoch 17/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.0951 - accuracy: 0.4417\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 257us/sample - loss: 2.0950 - accuracy: 0.4417\n",
      "Epoch 18/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.0716 - accuracy: 0.4473\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 2.0717 - accuracy: 0.4472\n",
      "Epoch 19/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.0501 - accuracy: 0.4527\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 2.0500 - accuracy: 0.4527\n",
      "Epoch 20/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.0288 - accuracy: 0.4576\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 240us/sample - loss: 2.0289 - accuracy: 0.4576\n",
      "Epoch 21/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 2.0080 - accuracy: 0.4627\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 240us/sample - loss: 2.0080 - accuracy: 0.4627\n",
      "Epoch 22/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.9893 - accuracy: 0.4664\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.9893 - accuracy: 0.4663\n",
      "Epoch 23/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.9720 - accuracy: 0.4707\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 244us/sample - loss: 1.9721 - accuracy: 0.4707\n",
      "Epoch 24/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.9532 - accuracy: 0.4745\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.9532 - accuracy: 0.4746\n",
      "Epoch 25/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.9382 - accuracy: 0.4785\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 243us/sample - loss: 1.9383 - accuracy: 0.4785\n",
      "Epoch 26/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.9204 - accuracy: 0.4827\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 246us/sample - loss: 1.9204 - accuracy: 0.4827\n",
      "Epoch 27/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.9050 - accuracy: 0.4861\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 250us/sample - loss: 1.9049 - accuracy: 0.4861\n",
      "Epoch 28/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.8894 - accuracy: 0.4907\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.8896 - accuracy: 0.4906\n",
      "Epoch 29/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.8766 - accuracy: 0.4939\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 256us/sample - loss: 1.8766 - accuracy: 0.4939\n",
      "Epoch 30/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.8608 - accuracy: 0.4982\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.8609 - accuracy: 0.4982\n",
      "Epoch 31/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.8511 - accuracy: 0.5003\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 257us/sample - loss: 1.8508 - accuracy: 0.5004\n",
      "Epoch 32/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.8334 - accuracy: 0.5041\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.8334 - accuracy: 0.5041\n",
      "Epoch 33/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.8224 - accuracy: 0.5063\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.8226 - accuracy: 0.5062\n",
      "Epoch 34/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.8123 - accuracy: 0.5100\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 242us/sample - loss: 1.8125 - accuracy: 0.5099\n",
      "Epoch 35/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.7985 - accuracy: 0.5126\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.7987 - accuracy: 0.5125\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.7882 - accuracy: 0.5154\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 247us/sample - loss: 1.7884 - accuracy: 0.5153\n",
      "Epoch 37/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.7748 - accuracy: 0.5184\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.7746 - accuracy: 0.5184\n",
      "Epoch 38/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.7625 - accuracy: 0.5213\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 1.7628 - accuracy: 0.5213\n",
      "Epoch 39/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.7531 - accuracy: 0.5237\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.7532 - accuracy: 0.5236\n",
      "Epoch 40/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.7418 - accuracy: 0.5268\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.7421 - accuracy: 0.5267\n",
      "Epoch 41/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.7332 - accuracy: 0.5293\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 241us/sample - loss: 1.7333 - accuracy: 0.5293\n",
      "Epoch 42/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.7226 - accuracy: 0.5315\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 1.7226 - accuracy: 0.5315\n",
      "Epoch 43/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.7123 - accuracy: 0.5341\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.7125 - accuracy: 0.5340\n",
      "Epoch 44/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.7041 - accuracy: 0.5362\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 250us/sample - loss: 1.7040 - accuracy: 0.5362\n",
      "Epoch 45/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6956 - accuracy: 0.5389\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.6959 - accuracy: 0.5388\n",
      "Epoch 46/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6835 - accuracy: 0.5423\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.6832 - accuracy: 0.5424\n",
      "Epoch 47/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6747 - accuracy: 0.5435\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.6745 - accuracy: 0.5436\n",
      "Epoch 48/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6660 - accuracy: 0.5459\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 249us/sample - loss: 1.6660 - accuracy: 0.5459\n",
      "Epoch 49/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6587 - accuracy: 0.5478\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.6585 - accuracy: 0.5478\n",
      "Epoch 50/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6489 - accuracy: 0.5502\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 1.6492 - accuracy: 0.5501\n",
      "Epoch 51/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6412 - accuracy: 0.5518\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.6413 - accuracy: 0.5518\n",
      "Epoch 52/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6338 - accuracy: 0.5536\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.6337 - accuracy: 0.5536\n",
      "Epoch 53/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6248 - accuracy: 0.5563\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.6249 - accuracy: 0.5563\n",
      "Epoch 54/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6155 - accuracy: 0.5581\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 1.6157 - accuracy: 0.5580\n",
      "Epoch 55/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6101 - accuracy: 0.5601\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.6099 - accuracy: 0.5601\n",
      "Epoch 56/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.6001 - accuracy: 0.5634\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 249us/sample - loss: 1.6002 - accuracy: 0.5632\n",
      "Epoch 57/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5946 - accuracy: 0.5640\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.5945 - accuracy: 0.5640\n",
      "Epoch 58/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5843 - accuracy: 0.5668\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.5843 - accuracy: 0.5668\n",
      "Epoch 59/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5772 - accuracy: 0.5677\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.5774 - accuracy: 0.5676\n",
      "Epoch 60/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5702 - accuracy: 0.5690\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 246us/sample - loss: 1.5703 - accuracy: 0.5690\n",
      "Epoch 61/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5631 - accuracy: 0.5709\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.5629 - accuracy: 0.5709\n",
      "Epoch 62/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5601 - accuracy: 0.5719\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 257us/sample - loss: 1.5602 - accuracy: 0.5719\n",
      "Epoch 63/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5503 - accuracy: 0.5748\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.5506 - accuracy: 0.5747\n",
      "Epoch 64/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5420 - accuracy: 0.5772\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.5421 - accuracy: 0.5771\n",
      "Epoch 65/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5359 - accuracy: 0.5781\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.5356 - accuracy: 0.5782\n",
      "Epoch 66/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5311 - accuracy: 0.5794\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.5312 - accuracy: 0.5794\n",
      "Epoch 67/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5222 - accuracy: 0.5816\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 256us/sample - loss: 1.5221 - accuracy: 0.5816\n",
      "Epoch 68/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5162 - accuracy: 0.5831\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.5159 - accuracy: 0.5832\n",
      "Epoch 69/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5087 - accuracy: 0.5849\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 250us/sample - loss: 1.5089 - accuracy: 0.5849\n",
      "Epoch 70/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.5049 - accuracy: 0.5856\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.5051 - accuracy: 0.5855\n",
      "Epoch 71/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4941 - accuracy: 0.5887\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 1.4941 - accuracy: 0.5887\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4895 - accuracy: 0.5902\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 245us/sample - loss: 1.4898 - accuracy: 0.5902\n",
      "Epoch 73/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4863 - accuracy: 0.5911\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.4864 - accuracy: 0.5911\n",
      "Epoch 74/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4782 - accuracy: 0.5932\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 244us/sample - loss: 1.4780 - accuracy: 0.5932\n",
      "Epoch 75/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4709 - accuracy: 0.5939\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 256us/sample - loss: 1.4710 - accuracy: 0.5939\n",
      "Epoch 76/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4650 - accuracy: 0.5957\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.4648 - accuracy: 0.5957\n",
      "Epoch 77/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4606 - accuracy: 0.5969\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.4606 - accuracy: 0.5969\n",
      "Epoch 78/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4533 - accuracy: 0.6001\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 256us/sample - loss: 1.4534 - accuracy: 0.6001\n",
      "Epoch 79/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4516 - accuracy: 0.5994\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.4520 - accuracy: 0.5993\n",
      "Epoch 80/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4424 - accuracy: 0.6027\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 1.4425 - accuracy: 0.6027\n",
      "Epoch 81/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4339 - accuracy: 0.6035\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.4342 - accuracy: 0.6035\n",
      "Epoch 82/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4307 - accuracy: 0.6051\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.4305 - accuracy: 0.6051\n",
      "Epoch 83/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4268 - accuracy: 0.6069\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.4264 - accuracy: 0.6070\n",
      "Epoch 84/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4220 - accuracy: 0.6084\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 246us/sample - loss: 1.4222 - accuracy: 0.6083\n",
      "Epoch 85/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4169 - accuracy: 0.6085\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.4169 - accuracy: 0.6086\n",
      "Epoch 86/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4117 - accuracy: 0.6088\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 235us/sample - loss: 1.4117 - accuracy: 0.6088\n",
      "Epoch 87/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4040 - accuracy: 0.6117\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.4042 - accuracy: 0.6117\n",
      "Epoch 88/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.4003 - accuracy: 0.6129\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 234us/sample - loss: 1.4004 - accuracy: 0.6130\n",
      "Epoch 89/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3958 - accuracy: 0.6143\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.3958 - accuracy: 0.6143\n",
      "Epoch 90/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3894 - accuracy: 0.6154\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.3896 - accuracy: 0.6153\n",
      "Epoch 91/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3848 - accuracy: 0.6165\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.3850 - accuracy: 0.6165\n",
      "Epoch 92/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3821 - accuracy: 0.6170\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.3820 - accuracy: 0.6170\n",
      "Epoch 93/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3765 - accuracy: 0.6191\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.3767 - accuracy: 0.6192\n",
      "Epoch 94/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3701 - accuracy: 0.6203\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 236us/sample - loss: 1.3703 - accuracy: 0.6203\n",
      "Epoch 95/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3686 - accuracy: 0.6207\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.3688 - accuracy: 0.6206\n",
      "Epoch 96/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3626 - accuracy: 0.6219\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.3625 - accuracy: 0.6220\n",
      "Epoch 97/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3593 - accuracy: 0.6230\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.3592 - accuracy: 0.6231\n",
      "Epoch 98/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3533 - accuracy: 0.6243\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 236us/sample - loss: 1.3532 - accuracy: 0.6244\n",
      "Epoch 99/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3458 - accuracy: 0.6265\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.3458 - accuracy: 0.6265\n",
      "Epoch 100/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3448 - accuracy: 0.6271\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 241us/sample - loss: 1.3451 - accuracy: 0.6270\n",
      "Epoch 101/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3384 - accuracy: 0.6289\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.3384 - accuracy: 0.6289\n",
      "Epoch 102/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3351 - accuracy: 0.6296\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.3352 - accuracy: 0.6296\n",
      "Epoch 103/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3321 - accuracy: 0.6300\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.3323 - accuracy: 0.6299\n",
      "Epoch 104/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3271 - accuracy: 0.6309\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 241us/sample - loss: 1.3271 - accuracy: 0.6310\n",
      "Epoch 105/200\n",
      "200000/200000 [==============================] - 40s 198us/sample - loss: 1.3307 - accuracy: 0.6306\n",
      "Epoch 106/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3164 - accuracy: 0.6343\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.3159 - accuracy: 0.6344\n",
      "Epoch 107/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3128 - accuracy: 0.6358\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.3127 - accuracy: 0.6358\n",
      "Epoch 108/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3089 - accuracy: 0.6359\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.3096 - accuracy: 0.6358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/200\n",
      "200000/200000 [==============================] - 40s 198us/sample - loss: 1.3242 - accuracy: 0.6316\n",
      "Epoch 110/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.3017 - accuracy: 0.6380\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 241us/sample - loss: 1.3017 - accuracy: 0.6380\n",
      "Epoch 111/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2957 - accuracy: 0.6400\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.2958 - accuracy: 0.6400\n",
      "Epoch 112/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2937 - accuracy: 0.6396\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 240us/sample - loss: 1.2933 - accuracy: 0.6397\n",
      "Epoch 113/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2909 - accuracy: 0.6412\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.2911 - accuracy: 0.6411\n",
      "Epoch 114/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2844 - accuracy: 0.6421\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.2841 - accuracy: 0.6422\n",
      "Epoch 115/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2805 - accuracy: 0.6431\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.2806 - accuracy: 0.6431\n",
      "Epoch 116/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 1.2808 - accuracy: 0.6433\n",
      "Epoch 117/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 1.2807 - accuracy: 0.6435\n",
      "Epoch 118/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2749 - accuracy: 0.6455\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.2748 - accuracy: 0.6455\n",
      "Epoch 119/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2679 - accuracy: 0.6459\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.2676 - accuracy: 0.6460\n",
      "Epoch 120/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2645 - accuracy: 0.6479\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.2644 - accuracy: 0.6479\n",
      "Epoch 121/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2610 - accuracy: 0.6473\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.2609 - accuracy: 0.6474\n",
      "Epoch 122/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2575 - accuracy: 0.6491\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.2575 - accuracy: 0.6490\n",
      "Epoch 123/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2566 - accuracy: 0.6499\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.2564 - accuracy: 0.6500\n",
      "Epoch 124/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2493 - accuracy: 0.6518\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 241us/sample - loss: 1.2493 - accuracy: 0.6518\n",
      "Epoch 125/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2466 - accuracy: 0.6514\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.2468 - accuracy: 0.6514\n",
      "Epoch 126/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2430 - accuracy: 0.6534\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.2428 - accuracy: 0.6534\n",
      "Epoch 127/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2403 - accuracy: 0.6539\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.2404 - accuracy: 0.6539\n",
      "Epoch 128/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2349 - accuracy: 0.6542\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.2348 - accuracy: 0.6543\n",
      "Epoch 129/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2324 - accuracy: 0.6550\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.2327 - accuracy: 0.6549\n",
      "Epoch 130/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2312 - accuracy: 0.6554\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.2315 - accuracy: 0.6553\n",
      "Epoch 131/200\n",
      "200000/200000 [==============================] - 40s 198us/sample - loss: 1.3085 - accuracy: 0.6357\n",
      "Epoch 132/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2249 - accuracy: 0.6582\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 239us/sample - loss: 1.2248 - accuracy: 0.6582\n",
      "Epoch 133/200\n",
      "200000/200000 [==============================] - 40s 200us/sample - loss: 1.2263 - accuracy: 0.6569\n",
      "Epoch 134/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2142 - accuracy: 0.6597\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 240us/sample - loss: 1.2140 - accuracy: 0.6598\n",
      "Epoch 135/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2122 - accuracy: 0.6604\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.2126 - accuracy: 0.6604\n",
      "Epoch 136/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2084 - accuracy: 0.6626\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.2088 - accuracy: 0.6626\n",
      "Epoch 137/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2072 - accuracy: 0.6621\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 238us/sample - loss: 1.2070 - accuracy: 0.6622\n",
      "Epoch 138/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2070 - accuracy: 0.6615\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.2067 - accuracy: 0.6616\n",
      "Epoch 139/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.2049 - accuracy: 0.6620\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 244us/sample - loss: 1.2048 - accuracy: 0.6620\n",
      "Epoch 140/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1986 - accuracy: 0.6636\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 250us/sample - loss: 1.1985 - accuracy: 0.6637\n",
      "Epoch 141/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1977 - accuracy: 0.6647\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 246us/sample - loss: 1.1981 - accuracy: 0.6646\n",
      "Epoch 142/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1937 - accuracy: 0.6661\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.1936 - accuracy: 0.6661\n",
      "Epoch 143/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1896 - accuracy: 0.6665\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.1896 - accuracy: 0.6665\n",
      "Epoch 144/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1867 - accuracy: 0.6670\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 243us/sample - loss: 1.1865 - accuracy: 0.6671\n",
      "Epoch 145/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1852 - accuracy: 0.6675\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 247us/sample - loss: 1.1854 - accuracy: 0.6675\n",
      "Epoch 146/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.1881 - accuracy: 0.6671\n",
      "Epoch 147/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1799 - accuracy: 0.6688\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 1.1799 - accuracy: 0.6688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.1848 - accuracy: 0.6668\n",
      "Epoch 149/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1744 - accuracy: 0.6711\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 257us/sample - loss: 1.1743 - accuracy: 0.6711\n",
      "Epoch 150/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1687 - accuracy: 0.6721\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.1686 - accuracy: 0.6721\n",
      "Epoch 151/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.1693 - accuracy: 0.6719\n",
      "Epoch 152/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1667 - accuracy: 0.6727\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.1666 - accuracy: 0.6728\n",
      "Epoch 153/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1608 - accuracy: 0.6735\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 250us/sample - loss: 1.1608 - accuracy: 0.6734\n",
      "Epoch 154/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.1613 - accuracy: 0.6732\n",
      "Epoch 155/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1572 - accuracy: 0.6747\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 52s 259us/sample - loss: 1.1572 - accuracy: 0.6747\n",
      "Epoch 156/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.1606 - accuracy: 0.6738\n",
      "Epoch 157/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1527 - accuracy: 0.6756\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.1526 - accuracy: 0.6757\n",
      "Epoch 158/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1515 - accuracy: 0.6753\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.1515 - accuracy: 0.6753\n",
      "Epoch 159/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1484 - accuracy: 0.6770\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.1485 - accuracy: 0.6770\n",
      "Epoch 160/200\n",
      "200000/200000 [==============================] - 41s 207us/sample - loss: 1.1762 - accuracy: 0.6701\n",
      "Epoch 161/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1417 - accuracy: 0.6785\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 257us/sample - loss: 1.1419 - accuracy: 0.6784\n",
      "Epoch 162/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.1451 - accuracy: 0.6780\n",
      "Epoch 163/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1372 - accuracy: 0.6799\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 249us/sample - loss: 1.1373 - accuracy: 0.6799\n",
      "Epoch 164/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1293 - accuracy: 0.6817\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.1293 - accuracy: 0.6817\n",
      "Epoch 165/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 1.1315 - accuracy: 0.6809\n",
      "Epoch 166/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.1298 - accuracy: 0.6813\n",
      "Epoch 167/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1263 - accuracy: 0.6820\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.1264 - accuracy: 0.6819\n",
      "Epoch 168/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.1269 - accuracy: 0.6821\n",
      "Epoch 169/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1225 - accuracy: 0.6839\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 1.1226 - accuracy: 0.6838\n",
      "Epoch 170/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1192 - accuracy: 0.6841\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 245us/sample - loss: 1.1192 - accuracy: 0.6840\n",
      "Epoch 171/200\n",
      "200000/200000 [==============================] - 42s 209us/sample - loss: 1.1199 - accuracy: 0.6838\n",
      "Epoch 172/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1141 - accuracy: 0.6864\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.1139 - accuracy: 0.6865\n",
      "Epoch 173/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1112 - accuracy: 0.6857\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.1113 - accuracy: 0.6857\n",
      "Epoch 174/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.1122 - accuracy: 0.6860\n",
      "Epoch 175/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.1131 - accuracy: 0.6857\n",
      "Epoch 176/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1097 - accuracy: 0.6869\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 250us/sample - loss: 1.1096 - accuracy: 0.6868\n",
      "Epoch 177/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1058 - accuracy: 0.6877\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.1061 - accuracy: 0.6877\n",
      "Epoch 178/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.1013 - accuracy: 0.6896\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 254us/sample - loss: 1.1012 - accuracy: 0.6897\n",
      "Epoch 179/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.1207 - accuracy: 0.6838\n",
      "Epoch 180/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0961 - accuracy: 0.6898\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.0963 - accuracy: 0.6898\n",
      "Epoch 181/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0949 - accuracy: 0.6904\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 257us/sample - loss: 1.0949 - accuracy: 0.6904\n",
      "Epoch 182/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.1048 - accuracy: 0.6876\n",
      "Epoch 183/200\n",
      "200000/200000 [==============================] - 42s 208us/sample - loss: 1.0976 - accuracy: 0.6899\n",
      "Epoch 184/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0901 - accuracy: 0.6918\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 250us/sample - loss: 1.0901 - accuracy: 0.6917\n",
      "Epoch 185/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0865 - accuracy: 0.6934\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.0869 - accuracy: 0.6934\n",
      "Epoch 186/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0851 - accuracy: 0.6931\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 1.0850 - accuracy: 0.6931\n",
      "Epoch 187/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0830 - accuracy: 0.6934\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.0830 - accuracy: 0.6934\n",
      "Epoch 188/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.0968 - accuracy: 0.6900\n",
      "Epoch 189/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0765 - accuracy: 0.6953\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.0766 - accuracy: 0.6953\n",
      "Epoch 190/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0748 - accuracy: 0.6950\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 253us/sample - loss: 1.0749 - accuracy: 0.6950\n",
      "Epoch 191/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0740 - accuracy: 0.6956\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 48s 241us/sample - loss: 1.0742 - accuracy: 0.6956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0727 - accuracy: 0.6955\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 47s 237us/sample - loss: 1.0729 - accuracy: 0.6955\n",
      "Epoch 193/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0686 - accuracy: 0.6973\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 252us/sample - loss: 1.0684 - accuracy: 0.6974\n",
      "Epoch 194/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.0687 - accuracy: 0.6968\n",
      "Epoch 195/200\n",
      "200000/200000 [==============================] - 40s 201us/sample - loss: 1.0831 - accuracy: 0.6921\n",
      "Epoch 196/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.0688 - accuracy: 0.6972\n",
      "Epoch 197/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0629 - accuracy: 0.6985\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 50s 251us/sample - loss: 1.0629 - accuracy: 0.6985\n",
      "Epoch 198/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0610 - accuracy: 0.6995\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 246us/sample - loss: 1.0613 - accuracy: 0.6994\n",
      "Epoch 199/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0596 - accuracy: 0.6988\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 49s 243us/sample - loss: 1.0597 - accuracy: 0.6988\n",
      "Epoch 200/200\n",
      "199500/200000 [============================>.] - ETA: 0s - loss: 1.0555 - accuracy: 0.7003\n",
      "new best weights found\n",
      "200000/200000 [==============================] - 51s 255us/sample - loss: 1.0555 - accuracy: 0.7003\n",
      "Total patterns:  665158\n",
      "Total unique characters:  127\n",
      "\" = 0:  \n",
      "        return a\n",
      "    elif function(b) == 0: \"\n",
      "fitting model with parameters:\n",
      "lstm size: 128\n",
      "lstm scalar: 1\n",
      "dropout: 0.5\n",
      "sequence length: 50\n",
      "Train on 200000 samples\n",
      "Epoch 1/200\n",
      "200000/200000 [==============================] - 46s 229us/sample - loss: 3.1319 - accuracy: 0.2877\n",
      "Epoch 2/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 2.8854 - accuracy: 0.2993\n",
      "Epoch 3/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 2.7972 - accuracy: 0.3069\n",
      "Epoch 4/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 2.7391 - accuracy: 0.3130\n",
      "Epoch 5/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 2.6946 - accuracy: 0.3193\n",
      "Epoch 6/200\n",
      "200000/200000 [==============================] - 42s 209us/sample - loss: 2.6502 - accuracy: 0.3253\n",
      "Epoch 7/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 2.5993 - accuracy: 0.3320\n",
      "Epoch 8/200\n",
      "200000/200000 [==============================] - 41s 207us/sample - loss: 2.5437 - accuracy: 0.3414\n",
      "Epoch 9/200\n",
      "200000/200000 [==============================] - 42s 209us/sample - loss: 2.4906 - accuracy: 0.3520\n",
      "Epoch 10/200\n",
      "200000/200000 [==============================] - 41s 207us/sample - loss: 2.4447 - accuracy: 0.3613\n",
      "Epoch 11/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 2.4010 - accuracy: 0.3706\n",
      "Epoch 12/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 2.3635 - accuracy: 0.3795\n",
      "Epoch 13/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 2.3264 - accuracy: 0.3884\n",
      "Epoch 14/200\n",
      "200000/200000 [==============================] - 41s 207us/sample - loss: 2.2946 - accuracy: 0.3959\n",
      "Epoch 15/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 2.2656 - accuracy: 0.4035\n",
      "Epoch 16/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 2.2371 - accuracy: 0.4092\n",
      "Epoch 17/200\n",
      "200000/200000 [==============================] - 41s 207us/sample - loss: 2.2128 - accuracy: 0.4139\n",
      "Epoch 18/200\n",
      "200000/200000 [==============================] - 40s 200us/sample - loss: 2.1886 - accuracy: 0.4193\n",
      "Epoch 19/200\n",
      "200000/200000 [==============================] - 41s 204us/sample - loss: 2.1683 - accuracy: 0.4243\n",
      "Epoch 20/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 2.1461 - accuracy: 0.4293\n",
      "Epoch 21/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 2.1266 - accuracy: 0.4342\n",
      "Epoch 22/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 2.1073 - accuracy: 0.4379\n",
      "Epoch 23/200\n",
      "200000/200000 [==============================] - 40s 201us/sample - loss: 2.0907 - accuracy: 0.4422\n",
      "Epoch 24/200\n",
      "200000/200000 [==============================] - 40s 198us/sample - loss: 2.0748 - accuracy: 0.4458\n",
      "Epoch 25/200\n",
      "200000/200000 [==============================] - 40s 200us/sample - loss: 2.0578 - accuracy: 0.4497\n",
      "Epoch 26/200\n",
      "200000/200000 [==============================] - 40s 198us/sample - loss: 2.0433 - accuracy: 0.4527\n",
      "Epoch 27/200\n",
      "200000/200000 [==============================] - 40s 201us/sample - loss: 2.0271 - accuracy: 0.4577\n",
      "Epoch 28/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 2.0140 - accuracy: 0.4604\n",
      "Epoch 29/200\n",
      "200000/200000 [==============================] - 40s 200us/sample - loss: 1.9999 - accuracy: 0.4637\n",
      "Epoch 30/200\n",
      "200000/200000 [==============================] - 40s 200us/sample - loss: 1.9868 - accuracy: 0.4661\n",
      "Epoch 31/200\n",
      "200000/200000 [==============================] - 40s 198us/sample - loss: 1.9739 - accuracy: 0.4701\n",
      "Epoch 32/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 1.9627 - accuracy: 0.4728\n",
      "Epoch 33/200\n",
      "200000/200000 [==============================] - 40s 198us/sample - loss: 1.9499 - accuracy: 0.4749\n",
      "Epoch 34/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 1.9352 - accuracy: 0.4785\n",
      "Epoch 35/200\n",
      "200000/200000 [==============================] - 40s 198us/sample - loss: 1.9244 - accuracy: 0.4809\n",
      "Epoch 36/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 1.9134 - accuracy: 0.4843\n",
      "Epoch 37/200\n",
      "200000/200000 [==============================] - 40s 198us/sample - loss: 1.9032 - accuracy: 0.4859\n",
      "Epoch 38/200\n",
      "200000/200000 [==============================] - 40s 201us/sample - loss: 1.8917 - accuracy: 0.4884\n",
      "Epoch 39/200\n",
      "200000/200000 [==============================] - 40s 201us/sample - loss: 1.8823 - accuracy: 0.4903\n",
      "Epoch 40/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 1.8691 - accuracy: 0.4941\n",
      "Epoch 41/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 1.8602 - accuracy: 0.4963\n",
      "Epoch 42/200\n",
      "200000/200000 [==============================] - 40s 200us/sample - loss: 1.8503 - accuracy: 0.4993\n",
      "Epoch 43/200\n",
      "200000/200000 [==============================] - 40s 200us/sample - loss: 1.8399 - accuracy: 0.5012\n",
      "Epoch 44/200\n",
      "200000/200000 [==============================] - 40s 200us/sample - loss: 1.8313 - accuracy: 0.5033\n",
      "Epoch 45/200\n",
      "200000/200000 [==============================] - 40s 202us/sample - loss: 1.8202 - accuracy: 0.5065\n",
      "Epoch 46/200\n",
      "200000/200000 [==============================] - 40s 198us/sample - loss: 1.8115 - accuracy: 0.5089\n",
      "Epoch 47/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 1.8000 - accuracy: 0.5106\n",
      "Epoch 48/200\n",
      "200000/200000 [==============================] - 40s 200us/sample - loss: 1.7928 - accuracy: 0.5129\n",
      "Epoch 49/200\n",
      "200000/200000 [==============================] - 40s 201us/sample - loss: 1.7838 - accuracy: 0.5150\n",
      "Epoch 50/200\n",
      "200000/200000 [==============================] - 40s 202us/sample - loss: 1.7753 - accuracy: 0.5179\n",
      "Epoch 51/200\n",
      "200000/200000 [==============================] - 42s 208us/sample - loss: 1.7654 - accuracy: 0.5194\n",
      "Epoch 52/200\n",
      "200000/200000 [==============================] - 40s 202us/sample - loss: 1.7577 - accuracy: 0.5211\n",
      "Epoch 53/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.7496 - accuracy: 0.5239\n",
      "Epoch 54/200\n",
      "200000/200000 [==============================] - 43s 216us/sample - loss: 1.7414 - accuracy: 0.5258\n",
      "Epoch 55/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.7327 - accuracy: 0.5285\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.7247 - accuracy: 0.5298\n",
      "Epoch 57/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.7163 - accuracy: 0.5317\n",
      "Epoch 58/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.7075 - accuracy: 0.5344\n",
      "Epoch 59/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.7015 - accuracy: 0.5364\n",
      "Epoch 60/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 1.6917 - accuracy: 0.5382\n",
      "Epoch 61/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.6908 - accuracy: 0.5390\n",
      "Epoch 62/200\n",
      "200000/200000 [==============================] - 42s 208us/sample - loss: 1.6775 - accuracy: 0.5426\n",
      "Epoch 63/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.6718 - accuracy: 0.5436\n",
      "Epoch 64/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.6610 - accuracy: 0.5462\n",
      "Epoch 65/200\n",
      "200000/200000 [==============================] - 40s 199us/sample - loss: 1.6579 - accuracy: 0.5475\n",
      "Epoch 66/200\n",
      "200000/200000 [==============================] - 41s 207us/sample - loss: 1.6493 - accuracy: 0.5496\n",
      "Epoch 67/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.6439 - accuracy: 0.5508\n",
      "Epoch 68/200\n",
      "200000/200000 [==============================] - 41s 204us/sample - loss: 1.6355 - accuracy: 0.5540\n",
      "Epoch 69/200\n",
      "200000/200000 [==============================] - 40s 201us/sample - loss: 1.6295 - accuracy: 0.5538\n",
      "Epoch 70/200\n",
      "200000/200000 [==============================] - 42s 209us/sample - loss: 1.6217 - accuracy: 0.5577\n",
      "Epoch 71/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.6172 - accuracy: 0.5585\n",
      "Epoch 72/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.6111 - accuracy: 0.5596\n",
      "Epoch 73/200\n",
      "200000/200000 [==============================] - 41s 205us/sample - loss: 1.6026 - accuracy: 0.5618\n",
      "Epoch 74/200\n",
      "200000/200000 [==============================] - 41s 204us/sample - loss: 1.5966 - accuracy: 0.5637\n",
      "Epoch 75/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 1.5948 - accuracy: 0.5641\n",
      "Epoch 76/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.5868 - accuracy: 0.5660\n",
      "Epoch 77/200\n",
      "200000/200000 [==============================] - 41s 207us/sample - loss: 1.5793 - accuracy: 0.5681\n",
      "Epoch 78/200\n",
      "200000/200000 [==============================] - 40s 202us/sample - loss: 1.5720 - accuracy: 0.5702\n",
      "Epoch 79/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 1.5675 - accuracy: 0.5717\n",
      "Epoch 80/200\n",
      "200000/200000 [==============================] - 43s 216us/sample - loss: 1.5626 - accuracy: 0.5721\n",
      "Epoch 81/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.5559 - accuracy: 0.5737\n",
      "Epoch 82/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.5514 - accuracy: 0.5761\n",
      "Epoch 83/200\n",
      "200000/200000 [==============================] - 43s 216us/sample - loss: 1.5441 - accuracy: 0.5773\n",
      "Epoch 84/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.5395 - accuracy: 0.5776\n",
      "Epoch 85/200\n",
      "200000/200000 [==============================] - 41s 203us/sample - loss: 1.5344 - accuracy: 0.5800\n",
      "Epoch 86/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.5290 - accuracy: 0.5813\n",
      "Epoch 87/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.5236 - accuracy: 0.5830\n",
      "Epoch 88/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.5161 - accuracy: 0.5847\n",
      "Epoch 89/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.5122 - accuracy: 0.5849\n",
      "Epoch 90/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.5078 - accuracy: 0.5862\n",
      "Epoch 91/200\n",
      "200000/200000 [==============================] - 41s 203us/sample - loss: 1.5033 - accuracy: 0.5879\n",
      "Epoch 92/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.4981 - accuracy: 0.5892\n",
      "Epoch 93/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 1.4939 - accuracy: 0.5904\n",
      "Epoch 94/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.4898 - accuracy: 0.5916\n",
      "Epoch 95/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.4825 - accuracy: 0.5939\n",
      "Epoch 96/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.4778 - accuracy: 0.5940\n",
      "Epoch 97/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.4735 - accuracy: 0.5963\n",
      "Epoch 98/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 1.4690 - accuracy: 0.5968\n",
      "Epoch 99/200\n",
      "200000/200000 [==============================] - 40s 202us/sample - loss: 1.4631 - accuracy: 0.5981\n",
      "Epoch 100/200\n",
      "200000/200000 [==============================] - 41s 205us/sample - loss: 1.4560 - accuracy: 0.6003\n",
      "Epoch 101/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.4538 - accuracy: 0.6013\n",
      "Epoch 102/200\n",
      "200000/200000 [==============================] - 42s 208us/sample - loss: 1.4481 - accuracy: 0.6021\n",
      "Epoch 103/200\n",
      "200000/200000 [==============================] - 40s 202us/sample - loss: 1.4448 - accuracy: 0.6030\n",
      "Epoch 104/200\n",
      "200000/200000 [==============================] - 42s 208us/sample - loss: 1.4414 - accuracy: 0.6042\n",
      "Epoch 105/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 1.4364 - accuracy: 0.6057\n",
      "Epoch 106/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.4318 - accuracy: 0.6065\n",
      "Epoch 107/200\n",
      "200000/200000 [==============================] - 41s 207us/sample - loss: 1.4266 - accuracy: 0.6086\n",
      "Epoch 108/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.4231 - accuracy: 0.6091\n",
      "Epoch 109/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.4169 - accuracy: 0.6115\n",
      "Epoch 110/200\n",
      "200000/200000 [==============================] - 43s 216us/sample - loss: 1.4155 - accuracy: 0.6104\n",
      "Epoch 111/200\n",
      "200000/200000 [==============================] - 42s 208us/sample - loss: 1.4105 - accuracy: 0.6125\n",
      "Epoch 112/200\n",
      "200000/200000 [==============================] - 40s 200us/sample - loss: 1.4055 - accuracy: 0.6140\n",
      "Epoch 113/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.4052 - accuracy: 0.6135\n",
      "Epoch 114/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.3985 - accuracy: 0.6158\n",
      "Epoch 115/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.3921 - accuracy: 0.6168\n",
      "Epoch 116/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 1.3895 - accuracy: 0.6166\n",
      "Epoch 117/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.3847 - accuracy: 0.6190\n",
      "Epoch 118/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.3821 - accuracy: 0.6188\n",
      "Epoch 119/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.3761 - accuracy: 0.6212\n",
      "Epoch 120/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.3723 - accuracy: 0.6225\n",
      "Epoch 121/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.3665 - accuracy: 0.6237\n",
      "Epoch 122/200\n",
      "200000/200000 [==============================] - 43s 216us/sample - loss: 1.3648 - accuracy: 0.6247\n",
      "Epoch 123/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.3637 - accuracy: 0.6238\n",
      "Epoch 124/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.3544 - accuracy: 0.6263\n",
      "Epoch 125/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.3524 - accuracy: 0.6280\n",
      "Epoch 126/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.3515 - accuracy: 0.6277\n",
      "Epoch 127/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.3481 - accuracy: 0.6284\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.3433 - accuracy: 0.6300\n",
      "Epoch 129/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.3381 - accuracy: 0.6304\n",
      "Epoch 130/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.3364 - accuracy: 0.6317\n",
      "Epoch 131/200\n",
      "200000/200000 [==============================] - 42s 212us/sample - loss: 1.3292 - accuracy: 0.6333\n",
      "Epoch 132/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.3286 - accuracy: 0.6327\n",
      "Epoch 133/200\n",
      "200000/200000 [==============================] - 42s 208us/sample - loss: 1.3257 - accuracy: 0.6342\n",
      "Epoch 134/200\n",
      "200000/200000 [==============================] - 41s 207us/sample - loss: 1.3206 - accuracy: 0.6351\n",
      "Epoch 135/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.3200 - accuracy: 0.6360\n",
      "Epoch 136/200\n",
      "200000/200000 [==============================] - 43s 216us/sample - loss: 1.3139 - accuracy: 0.6367\n",
      "Epoch 137/200\n",
      "200000/200000 [==============================] - 43s 217us/sample - loss: 1.3385 - accuracy: 0.6301\n",
      "Epoch 138/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.3108 - accuracy: 0.6389\n",
      "Epoch 139/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.3085 - accuracy: 0.6389\n",
      "Epoch 140/200\n",
      "200000/200000 [==============================] - 42s 208us/sample - loss: 1.3018 - accuracy: 0.6402\n",
      "Epoch 141/200\n",
      "200000/200000 [==============================] - 43s 216us/sample - loss: 1.2981 - accuracy: 0.6406\n",
      "Epoch 142/200\n",
      "200000/200000 [==============================] - 42s 209us/sample - loss: 1.2989 - accuracy: 0.6416\n",
      "Epoch 143/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.2898 - accuracy: 0.6431\n",
      "Epoch 144/200\n",
      "200000/200000 [==============================] - 44s 222us/sample - loss: 1.2858 - accuracy: 0.6446\n",
      "Epoch 145/200\n",
      "200000/200000 [==============================] - 44s 221us/sample - loss: 1.2857 - accuracy: 0.6444\n",
      "Epoch 146/200\n",
      "200000/200000 [==============================] - 44s 219us/sample - loss: 1.2798 - accuracy: 0.6455\n",
      "Epoch 147/200\n",
      "200000/200000 [==============================] - 45s 223us/sample - loss: 1.2788 - accuracy: 0.6458\n",
      "Epoch 148/200\n",
      "200000/200000 [==============================] - 44s 222us/sample - loss: 1.2755 - accuracy: 0.6474\n",
      "Epoch 149/200\n",
      "200000/200000 [==============================] - 44s 221us/sample - loss: 1.2739 - accuracy: 0.6468\n",
      "Epoch 150/200\n",
      "200000/200000 [==============================] - 44s 221us/sample - loss: 1.2706 - accuracy: 0.6485\n",
      "Epoch 151/200\n",
      "200000/200000 [==============================] - 45s 223us/sample - loss: 1.2680 - accuracy: 0.6493\n",
      "Epoch 152/200\n",
      "200000/200000 [==============================] - 44s 221us/sample - loss: 1.2617 - accuracy: 0.6511\n",
      "Epoch 153/200\n",
      "200000/200000 [==============================] - 44s 218us/sample - loss: 1.2630 - accuracy: 0.6516\n",
      "Epoch 154/200\n",
      "200000/200000 [==============================] - 44s 222us/sample - loss: 1.2565 - accuracy: 0.6524\n",
      "Epoch 155/200\n",
      "200000/200000 [==============================] - 45s 223us/sample - loss: 1.2537 - accuracy: 0.6528\n",
      "Epoch 156/200\n",
      "200000/200000 [==============================] - 43s 217us/sample - loss: 1.2507 - accuracy: 0.6539\n",
      "Epoch 157/200\n",
      "200000/200000 [==============================] - 44s 220us/sample - loss: 1.2497 - accuracy: 0.6541\n",
      "Epoch 158/200\n",
      "200000/200000 [==============================] - 44s 221us/sample - loss: 1.2556 - accuracy: 0.6521\n",
      "Epoch 159/200\n",
      "200000/200000 [==============================] - 44s 220us/sample - loss: 1.2444 - accuracy: 0.6552\n",
      "Epoch 160/200\n",
      "200000/200000 [==============================] - 44s 221us/sample - loss: 1.2418 - accuracy: 0.6560\n",
      "Epoch 161/200\n",
      "200000/200000 [==============================] - 44s 220us/sample - loss: 1.2385 - accuracy: 0.6558\n",
      "Epoch 162/200\n",
      "200000/200000 [==============================] - 42s 210us/sample - loss: 1.2335 - accuracy: 0.6577\n",
      "Epoch 163/200\n",
      "200000/200000 [==============================] - 45s 223us/sample - loss: 1.2320 - accuracy: 0.6593\n",
      "Epoch 164/200\n",
      "200000/200000 [==============================] - 44s 218us/sample - loss: 1.2302 - accuracy: 0.6596\n",
      "Epoch 165/200\n",
      "200000/200000 [==============================] - 44s 220us/sample - loss: 1.2244 - accuracy: 0.6604\n",
      "Epoch 166/200\n",
      "200000/200000 [==============================] - 44s 220us/sample - loss: 1.2250 - accuracy: 0.6607\n",
      "Epoch 167/200\n",
      "200000/200000 [==============================] - 44s 219us/sample - loss: 1.2224 - accuracy: 0.6608\n",
      "Epoch 168/200\n",
      "200000/200000 [==============================] - 44s 220us/sample - loss: 1.2192 - accuracy: 0.6608\n",
      "Epoch 169/200\n",
      "200000/200000 [==============================] - 44s 222us/sample - loss: 1.2161 - accuracy: 0.6625\n",
      "Epoch 170/200\n",
      "200000/200000 [==============================] - 45s 223us/sample - loss: 1.2132 - accuracy: 0.6621\n",
      "Epoch 171/200\n",
      "200000/200000 [==============================] - 45s 223us/sample - loss: 1.2099 - accuracy: 0.6638\n",
      "Epoch 172/200\n",
      "200000/200000 [==============================] - 44s 221us/sample - loss: 1.2092 - accuracy: 0.6648\n",
      "Epoch 173/200\n",
      "200000/200000 [==============================] - 44s 222us/sample - loss: 1.2044 - accuracy: 0.6661\n",
      "Epoch 174/200\n",
      "200000/200000 [==============================] - 43s 216us/sample - loss: 1.2009 - accuracy: 0.6665\n",
      "Epoch 175/200\n",
      "200000/200000 [==============================] - 44s 219us/sample - loss: 1.1983 - accuracy: 0.6675\n",
      "Epoch 176/200\n",
      "200000/200000 [==============================] - 42s 209us/sample - loss: 1.1958 - accuracy: 0.6681\n",
      "Epoch 177/200\n",
      "200000/200000 [==============================] - 44s 221us/sample - loss: 1.1917 - accuracy: 0.6690\n",
      "Epoch 178/200\n",
      "200000/200000 [==============================] - 44s 220us/sample - loss: 1.1913 - accuracy: 0.6689\n",
      "Epoch 179/200\n",
      "200000/200000 [==============================] - 44s 218us/sample - loss: 1.1919 - accuracy: 0.6689\n",
      "Epoch 180/200\n",
      "200000/200000 [==============================] - 44s 222us/sample - loss: 1.1894 - accuracy: 0.6695\n",
      "Epoch 181/200\n",
      "200000/200000 [==============================] - 44s 219us/sample - loss: 1.1832 - accuracy: 0.6711\n",
      "Epoch 182/200\n",
      "200000/200000 [==============================] - 44s 221us/sample - loss: 1.1817 - accuracy: 0.6717\n",
      "Epoch 183/200\n",
      "200000/200000 [==============================] - 44s 220us/sample - loss: 1.1772 - accuracy: 0.6722\n",
      "Epoch 184/200\n",
      "200000/200000 [==============================] - 45s 225us/sample - loss: 1.1757 - accuracy: 0.6738\n",
      "Epoch 185/200\n",
      "200000/200000 [==============================] - 43s 215us/sample - loss: 1.1746 - accuracy: 0.6731\n",
      "Epoch 186/200\n",
      "200000/200000 [==============================] - 44s 219us/sample - loss: 1.1704 - accuracy: 0.6731\n",
      "Epoch 187/200\n",
      "200000/200000 [==============================] - 44s 218us/sample - loss: 1.1703 - accuracy: 0.6737\n",
      "Epoch 188/200\n",
      "200000/200000 [==============================] - 43s 213us/sample - loss: 1.1692 - accuracy: 0.6748\n",
      "Epoch 189/200\n",
      "200000/200000 [==============================] - 42s 211us/sample - loss: 1.1672 - accuracy: 0.6748\n",
      "Epoch 190/200\n",
      "200000/200000 [==============================] - 43s 214us/sample - loss: 1.1613 - accuracy: 0.6764\n",
      "Epoch 191/200\n",
      "200000/200000 [==============================] - 45s 224us/sample - loss: 1.1589 - accuracy: 0.6770\n",
      "Epoch 192/200\n",
      "200000/200000 [==============================] - 45s 224us/sample - loss: 1.1596 - accuracy: 0.6775\n",
      "Epoch 193/200\n",
      "200000/200000 [==============================] - 45s 226us/sample - loss: 1.1547 - accuracy: 0.6785\n",
      "Epoch 194/200\n",
      "200000/200000 [==============================] - 44s 220us/sample - loss: 1.1530 - accuracy: 0.6793\n",
      "Epoch 195/200\n",
      "200000/200000 [==============================] - 44s 220us/sample - loss: 1.1573 - accuracy: 0.6774\n",
      "Epoch 196/200\n",
      "200000/200000 [==============================] - 41s 207us/sample - loss: 1.1526 - accuracy: 0.6784\n",
      "Epoch 197/200\n",
      "200000/200000 [==============================] - 41s 206us/sample - loss: 1.1449 - accuracy: 0.6809\n",
      "Epoch 198/200\n",
      "200000/200000 [==============================] - 43s 216us/sample - loss: 1.1491 - accuracy: 0.6809\n",
      "Epoch 199/200\n",
      "200000/200000 [==============================] - 45s 225us/sample - loss: 1.1404 - accuracy: 0.6821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200\n",
      "200000/200000 [==============================] - 45s 224us/sample - loss: 1.1388 - accuracy: 0.6826\n"
     ]
    }
   ],
   "source": [
    "parameter_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option1: Use documented best results (hardcoded from training results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# testpath = \"best-weights-test.hdf5\"\n",
    "# modelpath = testpath\n",
    "# testpath2 = \"sequencer-test\"\n",
    "# seqpath = testpath2\n",
    "\n",
    "# best_seq = Sequencer(60)\n",
    "# best_seq.sequence_gen(encoded,vocab_to_int, int_to_vocab)\n",
    "# reshape_data(best_seq)\n",
    "\n",
    "# # save sequencer object\n",
    "# print(best_seq.seq_len)\n",
    "# with open(seqpath, 'wb') as fp:\n",
    "#     pickle.dump(best_seq, fp)\n",
    "    \n",
    "# model_compile(dropout=0.2,lstm_size=100,second_layer_scalar=1)\n",
    "# # save model\n",
    "# model.save(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option2: or Load best weights recorded from training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# # filename should reflect the name of the best weights available \n",
    "# # in th local directory after training\n",
    "# model.load_weights(modelpath)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "modelpath = \"./models/best-weights.hdf5\"\n",
    "seqpath = \"./models/sequencer\"\n",
    "\n",
    "with open(seqpath, 'rb') as fp:\n",
    "    best_seq = pickle.load(fp)\n",
    "\n",
    "reshape_data(best_seq)\n",
    "\n",
    "model = load_model(modelpath)\n",
    "\n",
    "opt = Adam(learning_rate = 0.001, beta_1 = 0.7875, beta_2 = 0.999, epsilon = pow(10,-8))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests require pytest, pytest not installed\n",
      "20\n",
      "{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential_4\", \"layers\": [{\"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm_8\", \"trainable\": true, \"batch_input_shape\": [null, 20, 1], \"dtype\": \"float32\", \"return_sequences\": true, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"time_major\": false, \"units\": 100, \"activation\": \"tanh\", \"recurrent_activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"recurrent_initializer\": {\"class_name\": \"Orthogonal\", \"config\": {\"gain\": 1.0, \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.0, \"implementation\": 2}}, {\"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm_9\", \"trainable\": true, \"dtype\": \"float32\", \"return_sequences\": false, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"time_major\": false, \"units\": 100, \"activation\": \"tanh\", \"recurrent_activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"recurrent_initializer\": {\"class_name\": \"Orthogonal\", \"config\": {\"gain\": 1.0, \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.5, \"implementation\": 1}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_4\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 127, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import h5py\n",
    "h5py.run_tests()\n",
    "\n",
    "# print(model.get_weights())\n",
    "\n",
    "json_string = model.to_json()\n",
    "print(best_seq.seq_len)\n",
    "\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *(Optional)* train the model once again with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples\n",
      "Epoch 1/200\n",
      "200000/200000 [==============================] - 55s 277us/sample - loss: 1.0581 - accuracy: 0.6997\n",
      "Epoch 2/200\n",
      "200000/200000 [==============================] - 52s 258us/sample - loss: 1.0560 - accuracy: 0.7001\n",
      "Epoch 3/200\n",
      " 87000/200000 [============>.................] - ETA: 29s - loss: 1.0407 - accuracy: 0.7053"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.fit(X[:200000] , Y[:200000], epochs=200, batch_size=500)\n",
    "except:\n",
    "    # TODO: write to error log\n",
    "    pass\n",
    "\n",
    "timestamp = datetime.datetime.now()\n",
    "\n",
    "model.save(\"fmodels/refit-{timestamp}.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model's ability to generate code\n",
    "**TODO: try new prediction method**\n",
    "This next cell uses a random sequence from the dataset as a seed to allow the model to predict the next characters. This will test the model's abiltiy to genrate code.\n",
    "```python\n",
    "prediction = model.predict(x, verbose=0)\n",
    "index = sample(prediction)\n",
    "result = int_to_vocab[index]\n",
    "```\n",
    "This portion of the code will use the seed to predict the next character. This is a basic operation in terms of classification so we will not go too deep into explaining this. The line, `index = sample(prediction)`, simply takes each vector of probabilities and indicates the index with the highest value, essential providing the predicted next-character. The next portion will append the predicted charcter to the seed pattern, repeating the process to predict the following character. `sample()` essentially reweights the probablity distrubution based on a certain temperature. This allows us to prevent any repetitive sequences from generating by introducing random character periodically.\n",
    "\n",
    "The sampling fucntions is as detailed bellow and is based on a generative text inquiry [[3]](#References).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=0.5):\n",
    "    # helper function to sample an index from a probability array\n",
    "    # the higher the temperature the more likely the function\n",
    "    # will pick a random character during the prediction process\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  else make a new vertex\n",
      "            self.vertex[fr \"\n",
      "tett = sorfn\n",
      "        return srden\n",
      "\n",
      "\n",
      "ief dilntaaenaooeoe(key: cnt, miy) -> Lose:\n",
      "    \"\"\"\n",
      "    rettln = []\n",
      "    tpintTiit = (\" * xaxaime * 1, Potste  seatar_x]reds\"+ Fonemrsestmaniqa\n",
      "      \"     cacrar = arort_ni.pop(oodet, wad=) miget itnyestae inpeta ar aicoket nsrinenn\"oseut oelue\n",
      "\n",
      "        #               bjtefj += tollt_sidts[sow,8, llr]\n",
      "\n",
      "    return bmcme ail:brre , lenidm\n",
      "    crapypted_rtatiapr(proes, \n",
      "    irs = ipppt(\"Etter kessagens: \"\",\"sppnt()\n",
      "    teturn cyue\n",
      "\n",
      "\n",
      "df __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "dmport Rodi:\n",
      "    M\"T \" \"A\n",
      "  \"\"\" \"\n",
      "\n",
      "    dec e_ita__r(self, bava)\"fdltistrnn))y  iiaci_g wrin to tr reek if pudnricknec]sibi tspdkt\n",
      "    eor c in range(len(c + 1)):\n",
      "        if v = 0:\n",
      "            tumpt(kpdntp(\n",
      "T\": b + lelf.trr[i]s])\n",
      "        forneri_at2 \"       +:ptmt do eopntnl vrtts oe bipey inpme rp fesc oi she knttedlna DsrtyisdAn,..\n",
      "        eepcer_asrayeqspnng(todK) aooemn__, autertaredue:,\n",
      "    \n",
      "    krint(\"\". couutt_ml + [ , \" \n",
      "    hou i in range(1,,sen()\n",
      "        fom j in range(0, ken(\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(best_seq.datax)-1)\n",
    "pattern = []\n",
    "pattern = best_seq.datax[start]\n",
    "print(\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_vocab[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(best_seq.n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    #index = np.argmax(prediction)\n",
    "    index = sample(prediction[0])\n",
    "    #index = np.random.choice(Y.shape[1], 1, p=prediction[0])[0]\n",
    "    result = int_to_vocab[index]\n",
    "    seq_in = [int_to_vocab[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern = np.append(pattern, [index], axis=0) \n",
    "    length = pattern.shape[0] \n",
    "    pattern = pattern[1:length] \n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Greff, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., & Schmidhuber, J. (2016). LSTM: A search space odyssey. IEEE transactions on neural networks and learning systems, 28(10), 2222-2232.](https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer)\n",
    "2. [Dam, H. K., Tran, T., & Pham, T. (2016). A deep language model for software code. arXiv preprint arXiv:1608.02715.](https://arxiv.org/pdf/1608.02715.pdf)\n",
    "3. [Temperature based sampling to prevent repetitive generation](https://stackoverflow.com/questions/47125723/keras-lstm-for-text-generation-keeps-repeating-a-line-or-a-sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#I.-Introduction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
